# From app.py

import concurrent.futures
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
import anthropic
import json
import re
from urllib.parse import urlparse
from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import requests
from tavily import TavilyClient
import json as _json
import math
from urllib.parse import urljoin, urlparse

from selenium import webdriver
from selenium.webdriver.common.by import By
import time
import random
import pandas as pd

# FROM AND IMPORT CODES
import queue
import threading
import math
import time
import traceback
from pydub import AudioSegment

from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

import speech_recognition as sr

import json
from flask import jsonify, request
import json
from flask import Flask, render_template, redirect, session, request, jsonify

import os
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail
import os
import pprint
import re
import sys
import json
import time
import string
import secrets
import tempfile
import subprocess
import select
from datetime import datetime
from functools import wraps
import traceback
import requests

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

import logging
from logging import StreamHandler

from psycopg2 import pool

from threading import Event
from concurrent.futures import ThreadPoolExecutor, as_completed

from flask import (
    Flask, render_template, request, jsonify, redirect, url_for, 
    send_from_directory, session, Response, stream_with_context, flash
)

from flask_sqlalchemy import SQLAlchemy

from werkzeug.security import generate_password_hash, check_password_hash
from werkzeug.utils import secure_filename

from difflib import SequenceMatcher, get_close_matches
from markupsafe import Markup
from docx import Document
from pydub import AudioSegment
import vosk
import wave
import pdfplumber
import uuid
import hashlib
from datetime import datetime, timedelta
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

import pty
import spacy
import psycopg2
import stripe
import openai

from transformers import BertTokenizer, BertForSequenceClassification
import torch
from torch.nn.functional import softmax

import sqlite3
import threading
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import difflib
import schedule
from datetime import datetime

import hashlib
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time

from selenium.common.exceptions import TimeoutException


import random
from urllib.parse import urljoin, urlparse
import os
import hashlib
import secrets
import psycopg2
from functools import wraps
from flask import request, jsonify, session

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import re, requests, hashlib
from xml.etree import ElementTree as ET
from urllib.parse import urlparse

import requests

from psycopg2.extras import Json

import requests, re
from urllib.parse import urlparse

# TEMP: hardcoded dev key (replace in prod)
YOUTUBE_API_KEY = "AIzaSyB4bDEu7BzJY6_pMx3GmQgC-AZgxIDU4jQ"

# --- YOUTUBE PUBSUB CONFIG ---------------------------------------------------
import os
PUBLIC_BASE_URL     = os.environ.get("PUBLIC_BASE_URL", "https://advisorcheck.onrender.com")
YT_CALLBACK_PATH    = "/youtube/callback"
YT_CALLBACK_URL     = f"{PUBLIC_BASE_URL}{YT_CALLBACK_PATH}"

# Used by the hub to confirm you own the callback URL (sent back on GET challenge)
YTHUB_VERIFY_TOKEN  = os.environ.get("YTHUB_VERIFY_TOKEN", "dev-verify-token")

# Used to sign POST deliveries (You‚Äôll verify X-Hub-Signature)
YTHUB_SECRET        = os.environ.get("YTHUB_SECRET", "dev-signing-secret")

# Lease length (YouTube defaults ~4‚Äì7 days; 5 days is safe)
YTHUB_LEASE_SECONDS = int(os.environ.get("YTHUB_LEASE_SECONDS", "432000"))  # 5 days
# ----------------------------------------------------------------------------


# ‚ö†Ô∏è Replace these with your real values
APP_ID = "1718598702202188"
APP_SECRET = "ba989ffd85f6244abb11e80f4bcd5064"
ACCESS_TOKEN = f"{APP_ID}|{APP_SECRET}"   # ‚úÖ App token for any public page

# Add this near the top of your Python file with your other imports and configs
PROXYMESH_USERNAME = "riley.r.giauque@gmail.com"  # Replace with your actual username
PROXYMESH_PASSWORD = "Legacy$77"  # Replace with your actual password

# Available ProxyMesh endpoints
PROXY_ENDPOINTS = [
    'us-ca.proxymesh.com:31280',
    'us-wa.proxymesh.com:31280',
    'fr.proxymesh.com:31280',
    'jp.proxymesh.com:31280',
    'au.proxymesh.com:31280',
    'open.proxymesh.com:31280'
    # Add more as they become authorized in your dashboard
]

# Global proxy rotation
current_proxy_index = 0

def get_next_proxy():
    """Get the next proxy in rotation"""
    global current_proxy_index
    proxy = PROXY_ENDPOINTS[current_proxy_index]
    current_proxy_index = (current_proxy_index + 1) % len(PROXY_ENDPOINTS)
    return proxy


SKIP_UI_RE = re.compile(r'\b(Like|Share|Comment|Follow|Boost post|Manage Page)\b', re.I)


login_request_data = {'show': False}

import time
# Add these module-level variables for better performance
seen_locations = set()
existing_post_texts = set()  # Move this outside the function



facebook_refresh_in_progress = {}

# Database lock
db_lock = threading.Lock()

flask_logger = logging.getLogger('werkzeug')
flask_logger.setLevel(logging.WARNING)  # Change to INFO if you want to capture more details


# Add this near the top of your file with other global variables
session_token_usage = {
    'prompt_tokens': 0, 
    'completion_tokens': 0,
    'total_tokens': 0,
    'total_cost': 0.0,
    'api_calls': 0,
    'standard_pricing_calls': 0,
    'discount_pricing_calls': 0,
    'start_time': time.time()
}

# Add this near the top with other global variables
SKEPTICISM_WORDS = []


os.makedirs('uploads', exist_ok=True)

# Global driver management
_facebook_driver = None
_facebook_logged_in = False
twitter_driver = None
twitter_logged_in = False

processed_post_hashes = set()

import psutil
import os
import gc


# Add this counter at the top with your globals
_posts_processed_this_session = 0


def get_db():
    return get_db_connection()

def hash_password(password):
    salt = secrets.token_hex(16)
    pwd_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)
    return salt + pwd_hash.hex()

def verify_password(password, hash_with_salt):
    salt = hash_with_salt[:32]
    stored_hash = hash_with_salt[32:]
    pwd_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)
    return pwd_hash.hex() == stored_hash

def login_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        if 'user_id' not in session:
            return jsonify({'error': 'Login required'}), 401
        return f(*args, **kwargs)
    return decorated

def request_login_confirmation(platform, message):
    """Request login confirmation via the web interface"""
    import json
    import os
    import time
    
    print(f"üî• CREATING LOGIN REQUEST: {platform} - {message}")
    
    # Create a simple flag file that the web app can check
    flag_data = {
        'show_modal': True,
        'platform': platform,
        'message': message,
        'timestamp': time.time()
    }
    
    # Write to a temporary file that the web app monitors
    flag_file = 'login_request.json'
    with open(flag_file, 'w') as f:
        json.dump(flag_data, f)
    
    print(f"üî• Created {flag_file} with data: {flag_data}")
    
    # Wait for confirmation
    print("üî• Waiting for confirmation...")
    while True:
        try:
            if os.path.exists('login_confirmed.json'):
                print("üî• Found login_confirmed.json!")
                with open('login_confirmed.json', 'r') as f:
                    result = json.load(f)
                
                print(f"üî• Confirmation result: {result}")
                
                # Clean up
                os.remove('login_confirmed.json')
                os.remove(flag_file)
                
                if not result.get('confirmed', False):
                    raise Exception("Login cancelled by user")
                break
        except FileNotFoundError:
            pass
        except Exception as e:
            print(f"üî• Error in confirmation: {e}")
            break
        
        time.sleep(1)
    
    print("üî• Login confirmation completed!")
    
def get_chrome_options():
    """Chrome options that prevent memory crashes"""
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--memory-pressure-off')
    #options.add_argument('--max_old_space_size=8192')  # Limit to 8GB
    options.add_argument('--disable-background-timer-throttling')
    return options

def clear_memory_every_n_posts(n=10):
    """Clear memory every N posts during long scrapes"""
    global _posts_processed_this_session
    
    _posts_processed_this_session += 1
    
    if _posts_processed_this_session % n == 0:
        logger.info(f"üßπ Clearing memory after {_posts_processed_this_session} posts")
        gc.collect()  # Force garbage collection
        time.sleep(0.5)  # Brief pause to let memory clear

def log_memory_usage(label=""):
    """Log current memory usage"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    memory_percent = process.memory_percent()
    
    # System memory
    system_memory = psutil.virtual_memory()
    
    logger.info(f"üß† MEMORY {label}:")
    logger.info(f"  Process Memory: {memory_info.rss / 1024 / 1024:.1f} MB ({memory_percent:.1f}%)")
    logger.info(f"  System Memory: {system_memory.percent:.1f}% used ({system_memory.available / 1024 / 1024 / 1024:.1f} GB available)")
    
    return memory_info.rss / 1024 / 1024  # Return MB used

# Add this to your LinkedIn scraping function
def scrape_linkedin_with_monitoring(url):
    log_memory_usage("BEFORE_START")
    
    driver = None
    try:
        driver = webdriver.Chrome(options=get_chrome_options())
        log_memory_usage("AFTER_DRIVER_CREATE")
        
        driver.get(url)
        log_memory_usage("AFTER_PAGE_LOAD")
        
        # Your scraping logic here
        posts = extract_posts(driver)
        log_memory_usage("AFTER_POST_EXTRACTION")
        
        return posts
        
    except Exception as e:
        log_memory_usage("ON_ERROR")
        logger.error(f"Error: {str(e)}")
        raise
    finally:
        if driver:
            driver.quit()
            log_memory_usage("AFTER_CLEANUP")
            gc.collect()  # Force garbage collection
            log_memory_usage("AFTER_GC")

def monitor_chrome_processes():
    """Monitor all Chrome processes"""
    chrome_memory = 0
    chrome_processes = []
    
    for proc in psutil.process_iter(['pid', 'name', 'memory_info']):
        try:
            if 'chrome' in proc.info['name'].lower():
                memory_mb = proc.info['memory_info'].rss / 1024 / 1024
                chrome_memory += memory_mb
                chrome_processes.append({
                    'pid': proc.info['pid'],
                    'name': proc.info['name'],
                    'memory_mb': memory_mb
                })
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    
    logger.info(f"üåê CHROME PROCESSES: {len(chrome_processes)} processes using {chrome_memory:.1f} MB total")
    for proc in chrome_processes:
        logger.info(f"  {proc['name']} (PID: {proc['pid']}): {proc['memory_mb']:.1f} MB")
    
    return chrome_memory

def check_memory_limits():
    """Check if we're approaching memory limits"""
    process = psutil.Process(os.getpid())
    system_memory = psutil.virtual_memory()
    
    # Get current usage
    process_memory_mb = process.memory_info().rss / 1024 / 1024
    system_available_gb = system_memory.available / 1024 / 1024 / 1024
    
    # Define limits
    PROCESS_LIMIT_MB = 1000  # 1GB per process
    SYSTEM_LIMIT_GB = 2      # Stop if less than 2GB available
    
    if process_memory_mb > PROCESS_LIMIT_MB:
        logger.warning(f"‚ö†Ô∏è Process memory high: {process_memory_mb:.1f} MB (limit: {PROCESS_LIMIT_MB} MB)")
        return False
        
    if system_available_gb < SYSTEM_LIMIT_GB:
        logger.warning(f"‚ö†Ô∏è System memory low: {system_available_gb:.1f} GB available (limit: {SYSTEM_LIMIT_GB} GB)")
        return False
        
    return True

# --- YOUTUBE HELPERS ---------------------------------------------------------
import re, requests, xml.etree.ElementTree as ET
from urllib.parse import urlparse
import os

YTD_NS = {
    'atom': 'http://www.w3.org/2005/Atom',
    'yt': 'http://www.youtube.com/xml/schemas/2015',
    'media': 'http://search.yahoo.com/mrss/'
}

UA_HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; Finalyze/1.0)"}

def _extract_channel_id_from_html(html: str) -> str | None:
    # Common JSON blobs
    m = re.search(r'"channelId"\s*:\s*"(?P<cid>UC[0-9A-Za-z_-]{22})"', html)
    if m: return m.group('cid')
    m = re.search(r'"externalId"\s*:\s*"(?P<cid>UC[0-9A-Za-z_-]{22})"', html)
    if m: return m.group('cid')
    # Canonical link
    m = re.search(r'rel="canonical"\s+href="https://www\.youtube\.com/channel/(UC[0-9A-Za-z_-]{22})"', html)
    if m: return m.group(1)
    return None

def resolve_youtube_channel_id(url: str, session: requests.Session | None = None) -> str | None:
    """
    Accepts:
      - https://www.youtube.com/channel/UCxxxx
      - https://www.youtube.com/@handle
      - https://www.youtube.com/user/Name
      - https://youtube.com/c/CustomName
    Returns UC‚Ä¶ channel ID or None.
    """
    session = session or requests.Session()
    parsed = urlparse(url)
    path = (parsed.path or '').strip('/')

    # Direct channel id path
    m = re.match(r'^channel/(UC[0-9A-Za-z_-]{22})$', path)
    if m:
        return m.group(1)

    # Try main page, then /about as a fallback
    for candidate in (url, url.rstrip('/') + '/about', url.rstrip('/') + '/videos'):
        try:
            resp = session.get(candidate, headers=UA_HEADERS, timeout=15)
            resp.raise_for_status()
            cid = _extract_channel_id_from_html(resp.text)
            if cid:
                return cid
        except Exception:
            continue
    return None

def fetch_youtube_videos(channel_or_page_url: str, max_items: int = 20) -> list[dict]:
    logger.info(f"YT FEED: using Atom feed for {channel_or_page_url}")
    """
    Returns list of {id, title, url, published, description}
    Uses public Atom feed: /feeds/videos.xml?channel_id=UC...
    """
    session = requests.Session()
    cid = resolve_youtube_channel_id(channel_or_page_url, session=session)
    if not cid:
        return []

    feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={cid}"
    try:
        r = session.get(feed_url, headers=UA_HEADERS, timeout=15)
        r.raise_for_status()
        root = ET.fromstring(r.text)
    except Exception:
        return []

    videos = []
    for entry in root.findall('atom:entry', YTD_NS)[:max_items]:
        vid_id = entry.find('yt:videoId', YTD_NS)
        vid_id = vid_id.text if vid_id is not None else None
        if not vid_id:
            continue

        title = entry.find('atom:title', YTD_NS)
        title = title.text if title is not None else ''

        link_el = entry.find('atom:link', YTD_NS)
        link = link_el.attrib.get('href') if link_el is not None else f"https://www.youtube.com/watch?v={vid_id}"

        published = entry.find('atom:published', YTD_NS)
        published = published.text if published is not None else ''

        desc = ''
        mg = entry.find('media:group', YTD_NS)
        if mg is not None:
            md = mg.find('media:description', YTD_NS)
            if md is not None and md.text:
                desc = md.text

        videos.append({
            "id": vid,
            "title": sn.get("title", "") or "",
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published": sn.get("publishedAt", "") or "",
            "description": sn.get("description", "") or "",
            "channel_id": cid,  # ‚úÖ ensure channel_id is set on every row
        })

    logger.info(f"YT FEED: parsed {len(videos)} entries (feed limit ~15 regardless of max_items={max_items})")
    return videos

def build_youtube_content_text(videos: list[dict]) -> str:
    """
    Render as 'post-like' blocks your HTML expects.
    """
    lines = []
    for v in videos:
        lines.append(f"YouTube Video {v['id']}:")
        if v.get('title'):
            lines.append(f"Title: {v['title']}")
        if v.get('url'):
            lines.append(f"URL: {v['url']}")
        if v.get('published'):
            lines.append(f"Published: {v['published']}")
        if v.get('description'):
            lines.append(v['description'])
        lines.append('')
    return "\n".join(lines).strip()

def _parse_channel_id_from_url(url: str) -> str | None:
    """Return UC‚Ä¶ id if URL already has /channel/UCxxxx."""
    try:
        m = re.match(r'^/channel/(UC[0-9A-Za-z_-]{22})/?$', urlparse(url).path or '')
        return m.group(1) if m else None
    except Exception:
        return None

def subscribe_youtube_channel(advisor_id: int, channel_id: str, page_url: str) -> None:
    """
    Ask YouTube's hub to push new upload notifications for this channel
    to our /youtube/callback endpoint (with advisor & channel baked in).
    """
    import requests
    from urllib.parse import quote

    if not channel_id:
        raise ValueError("subscribe_youtube_channel: channel_id is required")

    callback = (
        f"{YT_CALLBACK_URL}"
        f"?advisor_id={advisor_id}"
        f"&channel_id={quote(channel_id, safe='')}"
        f"&page_url={quote(page_url, safe='')}"
    )

    data = {
        "hub.mode": "subscribe",
        "hub.topic": f"https://www.youtube.com/xml/feeds/videos.xml?channel_id={channel_id}",
        "hub.callback": callback,
        "hub.verify": "async",
        "hub.verify_token": YTHUB_VERIFY_TOKEN,
        "hub.lease_seconds": str(YTHUB_LEASE_SECONDS),
        "hub.secret": YTHUB_SECRET,  # so hub signs POSTs with HMAC-SHA1
    }

    r = requests.post("https://pubsubhubbub.appspot.com/subscribe", data=data, timeout=15)
    r.raise_for_status()
    logger.info(f"üì¨ Subscribed to YT PubSub: advisor={advisor_id} channel={channel_id} callback={callback}")


def resolve_channel_id_api(url_or_handle: str) -> str | None:
    """
    Resolve channel ID (UC‚Ä¶) via YouTube Data API v3.
    Supports:
      - https://www.youtube.com/channel/UCxxxx
      - https://www.youtube.com/@handle
      - raw '@handle'
    """
    if not YOUTUBE_API_KEY:
        return None

    # Direct UC id?
    cid = _parse_channel_id_from_url(url_or_handle)
    if cid:
        return cid

    # Extract handle if present
    handle = None
    if '@' in url_or_handle:
        handle = url_or_handle.split('@', 1)[1].split('/')[0]

    # channels.list for handle (fast & cheap)
    if handle:
        r = requests.get(
            "https://www.googleapis.com/youtube/v3/channels",
            params={"part": "id", "forHandle": f"@{handle}", "key": YOUTUBE_API_KEY},
            timeout=15
        )
        r.raise_for_status()
        items = r.json().get("items", [])
        if items:
            return items[0]["id"]

    # Fallback: search (avoid when possible, costs more quota)
    r = requests.get(
        "https://www.googleapis.com/youtube/v3/search",
        params={
            "part": "snippet", "type": "channel",
            "q": url_or_handle, "maxResults": 1, "key": YOUTUBE_API_KEY
        },
        timeout=15
    )
    r.raise_for_status()
    items = r.json().get("items", [])
    if items:
        return items[0]["snippet"]["channelId"]
    return None

def fetch_youtube_videos_api(channel_or_page_url: str, max_items: int = 250) -> list[dict] | None:
    videos = []
    logger.info(f"YT API: resolving channel for {channel_or_page_url}")
    """
    Fetch many past videos via YouTube Data API v3.
    Returns list[{id,title,url,published,description}] or None on failure.
    """
    try:
        if not YOUTUBE_API_KEY:
            return []

        cid = resolve_channel_id_api(channel_or_page_url)
        if not cid:
            return []

        # Get uploads playlist id
        r = requests.get(
            "https://www.googleapis.com/youtube/v3/channels",
            params={"part": "contentDetails", "id": cid, "key": YOUTUBE_API_KEY},
            timeout=15
        )
        r.raise_for_status()
        items = r.json().get("items", [])
        if not items:
            return []
        uploads_playlist = items[0]["contentDetails"]["relatedPlaylists"]["uploads"]

        # Page through playlistItems
        videos, page_token = [], None
        while len(videos) < max_items:
            params = {
                "part": "snippet,contentDetails",
                "playlistId": uploads_playlist,
                "maxResults": 50,
                "key": YOUTUBE_API_KEY
            }
            if page_token:
                params["pageToken"] = page_token

            pr = requests.get("https://www.googleapis.com/youtube/v3/playlistItems",
                              params=params, timeout=15)
            pr.raise_for_status()
            pdata = pr.json()

            for it in pdata.get("items", []):
                vid = (it.get("contentDetails") or {}).get("videoId")
                sn  = it.get("snippet") or {}
                if not vid:
                    continue
                videos.append({
                    "id": vid,
                    "title": sn.get("title", "") or "",
                    "url": f"https://www.youtube.com/watch?v={vid}",
                    "published": sn.get("publishedAt", "") or "",
                    "description": sn.get("description", "") or "",
                    'channel_id': cid,
                })
                if len(videos) >= max_items:
                    break

            page_token = pdata.get("nextPageToken")
            if not page_token:
                break
        logger.info(f"YT API: fetched {len(videos)} total (cap {max_items}); nextPageToken? {bool(page_token)}")
        return videos
    except Exception:
        return []

def save_youtube_rows(advisor_id: int, page_url: str, page_title: str | None, videos: list[dict], snapshot_limit: int = 0) -> int:
    """
    Inserts/updates per-video rows in youtube_videos.
    Also ensures a minimal website_snapshots record exists.
    snapshot_limit=0 means: do NOT build a giant blob.
    Returns number of videos upserted.
    """
    import hashlib

    conn = get_db_connection()
    cur = conn.cursor()

    # --- ensure we have a channel_id for inserts (NOT NULL in your table) ---
    channel_id = None
    if videos:
        channel_id = videos[0].get("channel_id")

    if not channel_id:
        # API resolution first; fallback to HTML sniff
        try:
            channel_id = resolve_channel_id_api(page_url)
        except Exception:
            channel_id = None
        if not channel_id:
            try:
                channel_id = resolve_youtube_channel_id(page_url)
            except Exception:
                channel_id = None
    if not channel_id:
        logger.error("YouTube: unable to resolve channel_id for %s; aborting row inserts.", page_url)
        cur.close()
        release_db_connection(conn)
        raise ValueError("channel_id required for youtube_videos")

    # --- Minimal snapshot row so domain_details sees the channel page ---
    minimal_text = ""  # drop the blob
    content_hash = hashlib.sha256((page_url + "|youtube").encode("utf-8")).hexdigest()

    cur.execute("""
        INSERT INTO website_snapshots
            (advisor_id, page_url, page_title, content_hash, content_text, last_checked, last_scan_checked)
        VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
        ON CONFLICT (advisor_id, page_url) DO UPDATE
           SET page_title        = COALESCE(EXCLUDED.page_title, website_snapshots.page_title),
               content_hash      = EXCLUDED.content_hash,
               content_text      = EXCLUDED.content_text,
               last_scan_checked = EXCLUDED.last_scan_checked
    """, (advisor_id, page_url, (page_title or "YouTube Channel"), content_hash, minimal_text))

    # --- Upsert per-video rows (align with your table columns) ---
    inserted = 0
    for v in videos:
        cur.execute("""
            INSERT INTO youtube_videos
                (advisor_id, channel_id, channel_url, video_id, title, description, published_at, video_url)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (advisor_id, video_id) DO UPDATE
               SET title        = EXCLUDED.title,
                   description  = EXCLUDED.description,
                   published_at = EXCLUDED.published_at,
                   video_url    = EXCLUDED.video_url
        """, (
            advisor_id,
            channel_id,                        # make sure you set channel_id above
            page_url,
            v["id"],
            v.get("title", "") or "",
            v.get("description", "") or "",
            v.get("published") or None,
            v.get("url", "") or ""             # mapping the dict's 'url' -> DB's 'video_url'
        ))  
        inserted += 1

    conn.commit()
    cur.close()
    release_db_connection(conn)
    logger.info("YouTube: upserted %d videos for channel %s", inserted, channel_id)
    return inserted

    
# ---------------------------------------------------------------------------

# Use in your scraping loop
def safe_scrape_posts(driver, max_posts=50):
    posts = []
    for i in range(max_posts):
        if not check_memory_limits():
            logger.warning("Memory limits reached, stopping scrape early")
            break
            
        # Your post extraction logic
        post = extract_single_post(driver, i)
        posts.append(post)
        
        # Log every 10 posts
        if i % 10 == 0:
            log_memory_usage(f"AFTER_{i}_POSTS")
    
    return posts


def is_link_only_post(post_element):
    """
    Check if this is a link-only post by looking for LinkedIn's shared article containers
    Returns True if it's just a shared link, False if there's user contribution
    If there's user contribution, extract ONLY the user's commentary text
    """
    try:
        #logger.info("üîç DEBUGGING: is_link_only_post function called")
        
        # UPDATED LinkedIn CSS selectors based on actual structure
        shared_content_selectors = [
            '.update-components-article',                    # Main article container (we see this in logs!)
            '.feed-shared-update-v2__content .update-components-article',  # Article within content
            '.update-components-article--with-no-image',     # Article without image
            '.update-components-article--with-small-image',  # Article with small image
        ]
        
        # Check if this post contains a shared article card
        has_shared_card = False
        for selector in shared_content_selectors:
            elements = post_element.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                has_shared_card = True
                #logger.info(f"üîó Found shared content with selector: {selector}")
                break
            #else:
                #logger.info(f"üîç DEBUG: No elements found for selector: {selector}")
        
        if not has_shared_card:
            #logger.info("üîç DEBUG: No shared card found, keeping post")
            return False  # No shared card = not a link-only post
        
        # If there IS a shared card, check if there's also user commentary
        # Look for the main commentary section
        commentary_selectors = [
            '.feed-shared-update-v2__commentary',     # Main commentary container
            '.feed-shared-text__text-view',           # Text content
            '.update-components-text',                # Alternative text container
        ]
        
        user_text = ""
        for selector in commentary_selectors:
            try:
                commentary_elem = post_element.find_element(By.CSS_SELECTOR, selector)
                user_text = commentary_elem.text.strip()
                if user_text and len(user_text) > 10:  # Meaningful text
                    #logger.info(f"‚úçÔ∏è Found user commentary: {user_text[:50]}...")
                    break
            except:
                #logger.info(f"üîç DEBUG: No commentary found for selector: {selector}")
                continue
        
        # If we found a shared card but no meaningful user commentary, it's link-only
        if has_shared_card and (not user_text or len(user_text) < 10):
            logger.info(f"üîó LINK-ONLY: Found shared card with no user commentary")
            return True
        
        # If we have both shared card AND user commentary, 
        # MODIFY the post element to contain ONLY the user commentary
        if has_shared_card and user_text:
            logger.info(f"‚úçÔ∏è HAS COMMENTARY: Extracting only user text")
            
            # Remove the shared article elements from the post
            try:
                for selector in shared_content_selectors:
                    elements_to_remove = post_element.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements_to_remove:
                        post_element.parent.execute_script("arguments[0].remove();", element)
                        
                logger.info(f"üîß MODIFIED POST: Removed shared article, keeping only user commentary")
            except Exception as e:
                logger.info(f"Could not remove shared elements: {e}")
            
            return False
        
        logger.info("üîç DEBUG: Fallback - keeping post")
        return False
        
    except Exception as e:
        logger.error(f"Error checking if post is link-only: {e}")
        return False  # When in doubt, don't filter out
    
def is_reshared_content(raw_text):
    """Enhanced detection for reshared/nested content"""
    # Direct repost indicators
    repost_indicators = [
        'reposted this', 'shared this', 'reshared this',
        '‚Ä¢ Following', 'following this', 'likes this',
        'commented on this', 'reacted to this'
    ]
    
    text_lower = raw_text.lower()
    if any(indicator in text_lower for indicator in repost_indicators):
        return True
    
    # Pattern detection for nested author names (your current logic but improved)
    # Look for pattern: Name1 Name2, Title ‚Ä¢ Following Name3 Name4, Title
    nested_pattern = r'([A-Z][a-z]+\s+[A-Z][a-z]+(?:,\s*[A-Z¬Æ¬©CISSP\s,]+)?)\s*‚Ä¢\s*(?:Following|3rd\+|2nd|1st)\s+([A-Z][a-z]+\s+[A-Z][a-z]+(?:,\s*[A-Z¬Æ¬©CISSP\s,]+)?)'
    
    if re.search(nested_pattern, raw_text):
        return True
    
    return False

def extract_user_content_only(raw_text, user_name=None):
    """Extract only the main user's content, excluding reshared content"""
    
    # If it's clearly a repost, return empty
    if is_reshared_content(raw_text):
        return ""
    
    # For posts with nested content structure, extract only the user's part
    lines = raw_text.split('\n')
    user_content_lines = []
    found_nested_author = False
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        # Skip empty lines
        if not line:
            continue
            
        # Skip LinkedIn UI elements
        if any(ui_element in line.lower() for ui_element in ['like', 'comment', 'repost', 'send', 'follow', 'connect']):
            continue
            
        # Skip metadata lines (timestamps, visibility, etc.)
        if (line.endswith(' ago') or 
            'visible to anyone' in line.lower() or
            re.match(r'^\d+[mhdwyr]+\s*‚Ä¢', line) or
            '‚Ä¢ Following' in line or
            line.startswith('Feed post number')):
            continue
        
        # Detect nested author pattern (when resharing someone else's content)
        nested_author_pattern = r'^([A-Z][a-z]+\s+[A-Z][a-z]+(?:,\s*[A-Z¬Æ¬©CISSP\s,]+)?)\s*‚Ä¢?\s*(?:Verified|3rd\+|2nd|1st)?'
        
        if re.match(nested_author_pattern, line) and i > 2:  # Not in the first few lines (which would be the main user)
            # This indicates we've hit a nested/reshared author
            found_nested_author = True
            break
            
        # If we haven't found nested content yet, this is likely user content
        if not found_nested_author and len(line) > 10:  # Meaningful content
            user_content_lines.append(line)
    
    # Join the user's content
    user_content = ' '.join(user_content_lines).strip()
    
    # Additional cleanup for any remaining nested content
    # Split at common reshare boundaries
    split_patterns = [
        r'\s+([A-Z][a-z]+\s+[A-Z][a-z]+,\s*[A-Z]{3,})\s+‚Ä¢\s*(?:Verified|3rd\+)',  # Name, TITLE ‚Ä¢ Verified
        r'\s+([A-Z][a-z]+\s+[A-Z][a-z]+)\s+([A-Z][a-z]+\s+[A-Z][a-z]+)',  # Two different names in sequence
    ]
    
    for pattern in split_patterns:
        match = re.search(pattern, user_content)
        if match:
            # Keep only content before the match
            user_content = user_content[:match.start()].strip()
            break
    
    return user_content

def scrape_linkedin_refresh_only(linkedin_url, existing_posts_text):
    """LinkedIn refresh scraper - only finds NEW posts by stopping at existing ones"""
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    import time
    import random
    import re
    
    driver = None
    new_posts = []
    
    try:
        driver = get_linkedin_driver()
        
        # +++++Enhanced browser session validation with fresh driver creation
        session_valid = False
        for attempt in range(2):
            try:
                current_url = driver.current_url
                driver.execute_script("return document.readyState")
                logger.info(f"LinkedIn browser session valid: {current_url}")
                session_valid = True
                break
            except Exception as e:
                logger.info(f"LinkedIn browser session invalid (attempt {attempt + 1}): {e}")
                if attempt == 0:
                    # Kill the dead driver completely
                    try:
                        driver.quit()
                    except:
                        pass
                    driver = None
                    
                    # Force a completely fresh LinkedIn driver
                    from selenium import webdriver
                    from selenium.webdriver.chrome.options import Options
                    
                    options = Options()
                    options.add_argument('--no-sandbox')
                    options.add_argument('--disable-dev-shm-usage') 
                    options.add_argument('--disable-gpu')
                    options.add_argument('--window-size=1920,1080')
                    
                    driver = webdriver.Chrome(options=options)
                    logger.info("Created fresh LinkedIn driver session")
                else:
                    logger.error("Could not establish valid LinkedIn browser session")
                    return []
        
        if not session_valid:
            logger.error("Failed to validate LinkedIn browser session")
            return []
        
        
        ensure_linkedin_login()
        logger.info(f"Starting LinkedIn REFRESH scrape for: {linkedin_url}")
        
        # Navigate to posts feed directly
        posts_url = linkedin_url.rstrip('/') + '/recent-activity/all/'
        driver.get(posts_url)
        time.sleep(5)
        
        # Parse existing posts for comparison
        existing_post_previews = set()
        if existing_posts_text:
            post_sections = existing_posts_text.split('Post ')
            logger.info(f"DEBUG: Split into {len(post_sections)} sections")
            
            for i, section in enumerate(post_sections):
                if section.strip():
                    lines = section.strip().split('\n')
                    if len(lines) > 1:
                        post_content = '\n'.join(lines[1:]).strip()
                        if post_content:
                            preview = linkedin_clean_for_refresh_comparison(post_content)[:80].lower()
                            existing_post_previews.add(preview)
                            
                            if i < 3:
                                logger.info(f"DEBUG: Existing post {i} preview: '{preview[:50]}...'")
        
        logger.info(f"Found {len(existing_post_previews)} existing posts to check against")
        
        processed_post_previews = set()
        consecutive_existing_count = 0
        posts_checked = 0
        
        # Track processed posts to prevent memory buildup
        processed_post_locations = set()
        
        for scroll_round in range(6):
            logger.info(f"Refresh scroll round {scroll_round + 1}")
            
            # Use the SAME selectors as your main scraper
            post_selectors = [
                '.scaffold-finite-scroll__content [data-urn*="activity"]',
                '.feed-container-theme [data-urn*="activity"]', 
                'main .feed-shared-update-v2',
                '.core-rail .feed-shared-update-v2',
                '[data-view-name="profile-activity-posts"] [data-urn*="activity"]'
            ]

            post_elements = []
            for selector in post_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        post_elements.extend(elements)
                except:
                    continue

            # If no specific selectors worked, fall back with location filtering (SAME as main scraper)
            if not post_elements:
                broader_elements = driver.find_elements(By.CSS_SELECTOR, '[data-urn*="activity"]')
                
                for elem in broader_elements:
                    try:
                        location = elem.location
                        size = elem.size
                        
                        window_width = driver.execute_script("return window.innerWidth;")
                        center_start = window_width * 0.2
                        center_end = window_width * 0.8
                        
                        if (center_start <= location['x'] <= center_end and 
                            size['height'] > 100):
                            post_elements.append(elem)
                    except:
                        continue

            # Remove duplicates by location AND by text content (SAME as main scraper)
            unique_posts = []
            seen_locations = set()
            seen_texts = set()

            for post in post_elements:
                try:
                    location = (post.location['x'], post.location['y'])
                    
                    if location in processed_post_locations:
                        continue
                        
                    quick_text = post.text[:50] if post.text else ""
                    
                    if (location not in seen_locations and 
                        quick_text not in seen_texts and
                        len(quick_text) > 10):
                        unique_posts.append(post)
                        seen_locations.add(location)
                        seen_texts.add(quick_text)
                        processed_post_locations.add(location)
                except:
                    continue

            logger.info(f"Found {len(unique_posts)} unique post elements")
            
            posts_added_this_round = 0
            
            for i, post_elem in enumerate(unique_posts):
                try:
                    posts_checked += 1
                    
                    # **EXPAND "SEE MORE" CONTENT FIRST** (SAME as main scraper)
                    expand_linkedin_see_more(driver, post_elem)
                    time.sleep(0.5)

                    # **LINK-ONLY FILTER** (SAME as main scraper)
                    if is_link_only_post(post_elem):
                        logger.info(f"‚è≠Ô∏è SKIPPING link-only post {posts_checked}")
                        continue

                    # **Remove link preview cards** (SAME as main scraper)
                    try:
                        link_cards = post_elem.find_elements(By.CSS_SELECTOR, 
                            '.feed-shared-article, .feed-shared-external-article, .feed-shared-mini-update-v2__preview-container')
                        for card in link_cards:
                            driver.execute_script("arguments[0].remove();", card)
                    except:
                        pass

                    raw_text = post_elem.text.strip()

                    # **APPLY REPOST FILTERING** (SAME as main scraper)
                    if 'reposted this' in raw_text.lower():
                        logger.info(f"‚è≠Ô∏è SKIPPING actual repost {posts_checked}")
                        continue

                    # **Extract Riley's commentary from shared posts** (SAME as main scraper)
                    lines = raw_text.split('\n')
                    riley_commentary = []
                    found_shared_content = False

                    for line in lines:
                        line = line.strip()
                        # Stop when we hit another person's name with credentials or company info
                        if (re.match(r'^[A-Z][a-z]+\s+[A-Z][a-z]+,\s+[A-Z]{3}', line) or
                            re.match(r'^[A-Z][a-z]+\s+Asset\s+Management', line) or
                            'followers' in line):
                            found_shared_content = True
                            break
                        # Skip Riley's header info
                        if not (line.startswith('Riley Giauque') or 
                                line == 'Personal Financial Consultant' or
                                line.endswith('yr') or line == 'You'):
                            riley_commentary.append(line)

                    if found_shared_content and riley_commentary:
                        raw_text = ' '.join(riley_commentary).strip()
                        logger.info(f"üìù Extracted Riley's commentary: {raw_text[:50]}...")

                    # **CUT OFF at common reshare patterns** (SAME as main scraper)
                    cutoff_patterns = [
                        r'[A-Z][a-z]+\s+[A-Z][a-z]+,\s*[A-Z]{3,}',
                        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+Verified',
                        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+‚Ä¢\s*3rd\+',
                    ]

                    for pattern in cutoff_patterns:
                        match = re.search(pattern, raw_text)
                        if match:
                            raw_text = raw_text[:match.start()].strip()
                            logger.info(f"‚úÇÔ∏è CUT OFF: {match.group()}")
                            break

                    # Enhanced repost detection - SKIP entirely if it's a repost
                    if ('reposted this' in raw_text.lower() or 'shared this' in raw_text.lower()):
                        logger.info(f"‚è≠Ô∏è SKIPPING repost entirely {posts_checked}")
                        continue

                    # **COMPLETE LINKEDIN POST EXTRACTION** (SAME as main scraper)
                    post_text = ""

                    # Method 1: Try to get the COMPLETE post content
                    try:
                        text_elements = post_elem.find_elements(By.CSS_SELECTOR, 
                            '.feed-shared-text__text-view, .feed-shared-update-v2__commentary, .attributed-text-segment-list__content')

                        if text_elements:
                            all_text_parts = []
                            for elem in text_elements:
                                elem_text = elem.text.strip()
                                if elem_text and len(elem_text) > 5:
                                    all_text_parts.append(elem_text)

                            if all_text_parts:
                                post_text = ' '.join(all_text_parts)
                                logger.info(f"Method 1 (complete): {post_text[:100]}...")

                    except Exception as e:
                        logger.debug(f"Method 1 failed: {e}")

                    # Method 2: Get ALL text content and intelligently combine it
                    if not post_text or len(post_text) < 50 or post_text.startswith('Feed post number'):
                        try:
                            full_text = raw_text
                            lines = full_text.split('\n')

                            content_lines = []
                            skip_patterns = [
                                'Like', 'Comment', 'Share', 'Send', 'Repost', 'Follow', 'Connect', 'Message',
                                'reactions', 'views', 'üëç', '‚ù§Ô∏è', 'üëè', 'üòä', 'üí°'
                            ]

                            for line in lines:
                                line = line.strip()

                                if (line and 
                                    len(line) > 3 and
                                    not any(skip in line.lower() for skip in skip_patterns) and
                                    not line.endswith((' ago', ' hr', ' min', ' day', ' days', ' week', ' weeks', ' month', ' months')) and
                                    not line.startswith('‚Ä¢') and
                                    not (line.isdigit() and len(line) < 4) and
                                    not line.startswith('Riley Giauque') and
                                    not line.startswith('Personal Financial Consultant') and
                                    line not in ['1yr', '3rd+', '2nd', '1st']):

                                    content_lines.append(line)

                            if content_lines:
                                post_text = ' '.join(content_lines)

                        except Exception as e:
                            logger.debug(f"Method 2 failed: {e}")

                    # Method 3: Last resort
                    if not post_text or len(post_text) < 20 or post_text.startswith('Feed post number'):
                        try:
                            post_content = post_elem.find_element(By.CSS_SELECTOR, 
                                '.feed-shared-update-v2__description-wrapper, .feed-shared-text, .update-components-text')

                            if post_content:
                                post_text = post_content.text.strip()

                        except Exception as e:
                            logger.debug(f"Method 3 failed: {e}")

                    # **COMPREHENSIVE LINKEDIN CLEANUP** (SAME as main scraper)
                    if post_text:
                        # Remove event details and metadata
                        post_text = re.sub(r'Mon,\s+[A-Z][a-z]+\s+\d+,\s+\d+.*?EDT.*?View event', '', post_text)
                        post_text = re.sub(r'Event by.*?(?=\s|$)', '', post_text)
                        post_text = re.sub(r'Central InfoSec CTF 2021\s+Event by Central InfoSec.*?Online', '', post_text)
                        post_text = re.sub(r'üì∫\s*Online', '', post_text)
                        post_text = re.sub(r'\d+\s+reactions?', '', post_text)

                        # Add new patterns
                        post_text = re.sub(r'Wed,\s+[A-Z][a-z]+\s+\d+,\s+\d+.*?EDT.*?View event', '', post_text)
                        post_text = re.sub(r'How to Unlock Data Vault to Deliver High-Quality Data Products at Scale\s+Event by.*?Online', '', post_text)
                        post_text = re.sub(r'Live Webinar:.*?(?=\s|$)', '', post_text)
                        post_text = re.sub(r'DATE:.*?TIME:.*?PT', '', post_text)

                        # General patterns
                        post_text = re.sub(r'(Mon|Tue|Wed|Thu|Fri|Sat|Sun),\s+[A-Z][a-z]+\s+\d+.*?EDT.*?View event', '', post_text)
                        post_text = re.sub(r'Event by [^üì∫]*üì∫\s*Online', '', post_text)
                        
                        # Remove article/link preview cards
                        post_text = re.sub(r'[A-Z][^.!?]*:\s*[A-Z][^.!?]*\s+[a-z]+\.[a-z]+', '', post_text)
                        post_text = re.sub(r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+on LinkedIn', '', post_text)
                        post_text = re.sub(r'\s+[A-Z][a-z]+\s+[A-Z][a-z]+$', '', post_text)
                        post_text = re.sub(r'https?://[^\s]+', '', post_text)
                        post_text = re.sub(r'[a-z]+\.[a-z]+(?:\.[a-z]+)?(?:/[^\s]*)?', '', post_text)

                        # Remove LinkedIn post header metadata
                        post_text = re.sub(r'^.*?\d+[wdhmy]r?\s*‚Ä¢.*?ago\s*‚Ä¢\s*(Edited\s*‚Ä¢\s*)?', '', post_text)

                        # Remove LinkedIn congratulations and position announcements
                        post_text = re.sub(r'Starting a New Position.*?(?=\s|$)', '', post_text)
                        post_text = re.sub(r'Congrats?\s+\w+!?\s*üéâ.*?(?:Wishing you the best|What an achievement!?).*?üéâ', '', post_text)
                        post_text = re.sub(r'Open Emoji Keyboard', '', post_text)

                        # Skip congratulations section completely
                        post_text = re.split(r'\b(Congratulations?|Congrats|Major achievement|Well done|Wishing you|Excited for you)\b', post_text, flags=re.IGNORECASE)[0].strip()
                        
                        # Remove duplicate job titles
                        post_text = re.sub(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+\1\s+', '', post_text)

                        # Remove LinkedIn feed metadata
                        post_text = re.sub(r'Feed post number \d+.*?Follow\s*', '', post_text)
                        post_text = re.sub(r'Feed post number \d+.*?Join\s*', '', post_text)

                        # Remove duplicate group/company names and metadata patterns
                        post_text = re.sub(r'(\w+\s+\w+\s+\w+)\s+\1\s+\d+(yr|mo|d|w|h)\s*‚Ä¢\s*.*?(Join|Follow)\s*', '', post_text)
                        post_text = re.sub(r'(Innovation In Payments|Tampa Bay Business Network)\s+\1\s+', r'\1 ', post_text)

                        # Remove UI button text
                        post_text = re.sub(r'^(ing|llowing|Following)\s+', '', post_text)
                        post_text = re.sub(r'^(Connect|Message|Follow|Join)\s+', '', post_text)

                        # Remove timestamp patterns
                        post_text = re.sub(r'\d+mo\s*‚Ä¢\s*\d+\s+months?\s+ago\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*Follow\s*', '', post_text)
                        post_text = re.sub(r'\d+(mo|hr|min|d|w|yr)\s*‚Ä¢.*?(Follow|Join)\s*', '', post_text)

                        # Remove image activation text
                        post_text = re.sub(r'Activate to view larger image,?\s*', '', post_text)
                        post_text = re.sub(r'Credit:\s+Personal Finance Club\s+', '', post_text)
                        post_text = re.sub(r'Your document has finished loading\s*', '', post_text)

                        # Remove video player controls
                        post_text = re.sub(r'Play Loaded:.*?Turn fullscreen on', '', post_text)
                        post_text = re.sub(r'Remaining time \d+:\d+.*?Unmute.*?Turn fullscreen on', '', post_text)
                        post_text = re.sub(r'Playback speed.*?Turn closed captions on.*?Unmute.*?Turn fullscreen on', '', post_text)

                        # Remove hashtag labels and standalone "Play"
                        post_text = re.sub(r'\bhashtag\s+#', '#', post_text)
                        post_text = re.sub(r'\bhashtag\s+', '', post_text)
                        post_text = re.sub(r'\s+Play$', '', post_text)
                        post_text = re.sub(r'\s+Play\s+', ' ', post_text)

                        # Remove other LinkedIn UI elements
                        post_text = re.sub(r'\b(Like|Comment|Share|Send|Repost|Follow|Connect|Join)\b', '', post_text)
                        post_text = re.sub(r'\d+\s+(reactions?|views?|likes?|comments?)', '', post_text)
                        post_text = re.sub(r'Visible to anyone on or off LinkedIn', '', post_text)

                        # Remove LinkedIn group posting patterns
                        post_text = re.sub(r'Only group members can comment on this post\.\s*You can still react or repost it\.', '', post_text)

                        # Clean up multiple spaces and trim
                        post_text = re.sub(r'\s+', ' ', post_text).strip()

                        # Remove common prefixes
                        post_text = re.sub(r'^(Follow\s+|Join\s+)?Market Report', 'Market Report', post_text)
                        post_text = re.sub(r'^(Follow\s+|Join\s+)?Stock Market Report', 'Stock Market Report', post_text)

                        # Remove trailing punctuation artifacts
                        post_text = re.sub(r'\s*,\s*$', '', post_text)

                        # Final cleanup
                        post_text = post_text.strip()

                        logger.info(f"Final cleaned post: {post_text[:150]}...")

                    if not post_text or len(post_text) < 15:
                        logger.info(f"DEBUG: Post {posts_checked} too short after full cleaning: '{post_text}'")
                        continue
                    
                    # Create preview for comparison
                    post_preview = linkedin_clean_for_refresh_comparison(post_text)[:80].lower()
                    
                    # Skip if we've already processed this post in this session
                    if post_preview in processed_post_previews:
                        logger.info(f"DEBUG: Post {posts_checked} already processed in this session")
                        continue
                    
                    processed_post_previews.add(post_preview)
                    
                    # Check if this post already exists in database
                    is_existing_post = linkedin_refresh_check_existing(post_preview, existing_post_previews)
                    
                    logger.info(f"üîç Post {posts_checked}: {'EXISTING' if is_existing_post else 'NEW'} - '{post_preview[:40]}...'")
                    
                    if is_existing_post:
                        consecutive_existing_count += 1
                        logger.info(f"Found existing post ({consecutive_existing_count} consecutive)")
                        
                        if consecutive_existing_count >= 2:
                            logger.info("Found 2 consecutive existing posts - refresh complete")
                            logger.info(f"LinkedIn refresh completed: Found {len(new_posts)} new posts")
                            return new_posts
                    else:
                        consecutive_existing_count = 0
                        
                        # Simple timestamp extraction
                        timestamp = "Unknown"
                        try:
                            time_elem = post_elem.find_element(By.CSS_SELECTOR, 'time')
                            timestamp = time_elem.get_attribute('datetime') or time_elem.text or "Unknown"
                        except:
                            pass
                        
                        new_posts.append({
                            'text': post_text,
                            'timestamp': timestamp,
                            'type': 'post'
                        })
                        posts_added_this_round += 1
                        logger.info(f"‚òÖ NEW POST ADDED: {post_text[:50]}...")
                
                except Exception as e:
                    logger.error(f"Error processing refresh post {posts_checked}: {e}")
                    continue
            
            logger.info(f"Round {scroll_round + 1}: Added {posts_added_this_round} new posts (total: {len(new_posts)})")
            
            if consecutive_existing_count >= 2:
                break
                
            if scroll_round < 5:
                driver.execute_script("window.scrollBy(0, 600);")
                time.sleep(2)
        
        logger.info(f"LinkedIn refresh completed: Found {len(new_posts)} new posts")
        return new_posts
        
    except Exception as e:
        logger.error(f"Error in LinkedIn refresh scrape: {e}")
        return []

    finally:
        # **KEEP LINKEDIN DRIVER OPEN** (same pattern as main scraper)
        # DON'T close the driver - let it stay open like Facebook/Twitter
        if driver:
            try:
                driver.execute_script("document.querySelectorAll('video').forEach(v => v.pause());")
            except:
                pass
        driver = None  # Clear the variable but don't quit the driver
        
    # OLD finally block
    #finally:
        #if driver:
            #cleanup_driver_session(driver)
            #driver = None
            #gc.collect()
            

def linkedin_refresh_cutoff_patterns():
    """Cutoff patterns specific to refresh operation"""
    return [
        r'[A-Z][a-z]+\s+[A-Z][a-z]+,\s*[A-Z]{3,}',
        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+Verified',
        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+‚Ä¢\s*3rd\+',
    ]


def linkedin_clean_post_for_refresh(raw_text):
    """Clean LinkedIn post text specifically for refresh operation"""
    if not raw_text:
        return ""
    
    cleaned_text = raw_text
    
    # Remove LinkedIn metadata patterns (similar to your existing logic)
    cleaned_text = re.sub(r'Feed post number \d+\s*', '', cleaned_text)
    cleaned_text = re.sub(r'\d+[mhdwyr]+\s*‚Ä¢\s*\d+\s+(minute|hour|day|week|month|year)s?\s+ago\s*‚Ä¢\s*', '', cleaned_text)
    cleaned_text = re.sub(r'Riley Giauque, CFP¬Æ.*?Personal Financial Consultant\s*', '', cleaned_text)
    cleaned_text = re.sub(r'Visible to anyone on or off LinkedIn\s*', '', cleaned_text)
    cleaned_text = re.sub(r'\d+\s+impressions?\s*View analytics\s*', '', cleaned_text)
    cleaned_text = re.sub(r'View analytics\s*', '', cleaned_text)
    cleaned_text = re.sub(r'Like Comment Repost Send\s*', '', cleaned_text)
    cleaned_text = re.sub(r'hashtag\s*#', '#', cleaned_text)
    
    # Remove UI elements that get mixed in
    cleaned_text = re.sub(r'^(ing|llowing|Following|Connect|Message|Follow|Join)\s+', '', cleaned_text)
    
    # Remove LinkedIn congratulations (from your existing code)
    cleaned_text = re.sub(r'Starting a New Position.*?(?=\s|$)', '', cleaned_text)
    cleaned_text = re.sub(r'Congrats?\s+\w+!?\s*üéâ.*?(?:Wishing you the best|What an achievement!?).*?$', '', cleaned_text)
    
    # Cut off at congratulations
    cleaned_text = re.split(r'\b(Congratulations?|Congrats|Major achievement|Well done|Wishing you|Excited for you)\b', 
                            cleaned_text, flags=re.IGNORECASE)[0].strip()
    
    # Remove duplicate job titles
    cleaned_text = re.sub(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+\1\s+', '', cleaned_text)
    
    # Clean up spaces
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    return cleaned_text


def linkedin_clean_for_refresh_comparison(post_text):
    """Additional cleaning for comparison during refresh"""
    if not post_text:
        return ""
    
    # Remove timestamps, reactions, and other variable elements
    cleaned = re.sub(r'\d+\s+(like|comment|repost|reaction)s?', '', post_text.lower())
    cleaned = re.sub(r'\d+\s+(minute|hour|day|week|month|year)s?\s+ago', '', cleaned)
    cleaned = re.sub(r'[‚Ä¢¬∑]\s*', '', cleaned)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned


def linkedin_refresh_check_existing(post_preview, existing_previews):
    """Check if a post preview matches any existing posts"""
    # Direct match
    if post_preview in existing_previews:
        return True
    
    # Fuzzy matching for slight variations
    for existing in existing_previews:
        # Check if they start similarly (first 40 chars)
        if len(post_preview) > 40 and len(existing) > 40:
            if post_preview[:40] == existing[:40]:
                return True
        
        # Check if one is contained in the other (for truncated versions)
        if len(post_preview) > 30 and len(existing) > 30:
            if post_preview[:30] in existing or existing[:30] in post_preview:
                return True
    
    return False


def linkedin_extract_refresh_timestamp(post_elem):
    """Extract timestamp from LinkedIn post element during refresh"""
    try:
        # Try to find time element
        time_elem = post_elem.find_element(By.CSS_SELECTOR, 'time')
        timestamp = time_elem.get_attribute('datetime')
        if timestamp:
            return timestamp
        
        # Fallback to text content
        time_text = time_elem.text
        if time_text:
            return time_text
            
    except:
        pass
    
    return "Unknown"


def cleanup_driver_session(driver):
    """Clean up driver session to prevent memory leaks"""
    try:
        # Clear cookies and cache
        driver.delete_all_cookies()
        
        # Clear local storage
        driver.execute_script("window.localStorage.clear();")
        driver.execute_script("window.sessionStorage.clear();")
        
        # Stop any videos
        driver.execute_script("document.querySelectorAll('video').forEach(v => v.pause());")
        
    except Exception as e:
        logger.error(f"Error during driver cleanup: {e}")

def scrape_linkedin_simple(linkedin_url, existing_posts_text):
    """ULTRA SIMPLE LinkedIn scraper with improved sequential collection"""
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    import time
    import random
    import re
    
    driver = None
    new_posts = []
    
    try:
        driver = get_linkedin_driver()
        ensure_linkedin_login()
        logger.info(f"Starting ULTRA SIMPLE LinkedIn scrape for: {linkedin_url}")
        
        # Navigate to posts feed
        posts_url = linkedin_url.rstrip('/') + '/recent-activity/all/'
        driver.get(posts_url)
        time.sleep(5)
        
        # SIMPLE existing posts check
        existing_post_texts = set()
        if existing_posts_text:
            post_sections = existing_posts_text.split('Post ')
            for section in post_sections:
                if section.strip():
                    lines = section.strip().split('\n')
                    if len(lines) > 1:
                        post_content = '\n'.join(lines[1:]).strip()
                        if post_content:
                            # Use first 100 characters for comparison
                            existing_post_texts.add(post_content[:100].lower())
        
        logger.info(f"Found {len(existing_post_texts)} existing posts to check against")
        
        seen_posts = set()  # Track by content, not hash
        consecutive_existing = 0
        last_post_count = 0
        no_new_posts_count = 0
        
        for scroll_attempt in range(10):  # Increase attempts
            logger.info(f"Scroll attempt {scroll_attempt + 1}")
            
            # Get all post elements
            post_elements = driver.find_elements(By.CSS_SELECTOR, '.feed-shared-update-v2')
            logger.info(f"Found {len(post_elements)} post elements")
            
            if len(post_elements) == last_post_count:
                no_new_posts_count += 1
                if no_new_posts_count >= 3:
                    logger.info("No new posts loaded for 3 attempts, stopping")
                    break
            else:
                no_new_posts_count = 0
                last_post_count = len(post_elements)
            
            posts_added_this_round = 0
            
            # Process ALL post elements found
            for i, post_elem in enumerate(post_elements):
                try:

                    # ADD THIS CHECK FIRST - before any text extraction
                    if is_link_only_post(post_elem):
                        logger.info(f"‚è≠Ô∏è SKIPPING link-only post {i}")
                        continue

                    raw_text = post_elem.text.strip()


                    # SIMPLE: Cut off at common reshare patterns
                    cutoff_patterns = [
                        r'[A-Z][a-z]+\s+[A-Z][a-z]+,\s*[A-Z]{3,}',  # Remove \b
                        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+Verified',     # Remove \b  
                        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+‚Ä¢\s*3rd\+',   # Remove \b
                    ]

                    for pattern in cutoff_patterns:
                        match = re.search(pattern, raw_text)
                        if match:
                            raw_text = raw_text[:match.start()].strip()
                            logger.info(f"‚úÇÔ∏è CUT OFF: {match.group()}")
                            break

                    # Enhanced repost detection - SKIP entirely if it's a repost
                    if ('reposted this' in raw_text.lower() or 'shared this' in raw_text.lower()):
                        logger.info("Skipping repost entirely")
                        continue

                    
                    if not raw_text or len(raw_text) < 20:
                        continue
                    
                    # **IMPROVED CLEANING**
                    cleaned_text = raw_text

                    # Remove LinkedIn post header metadata for ANY user
                    post_text = re.sub(r'^.*?\d+[wdhmy]r?\s*‚Ä¢.*?ago\s*‚Ä¢\s*(Edited\s*‚Ä¢\s*)?', '', post_text)

                    # ADD THIS: Remove LinkedIn congratulations and position announcements
                    post_text = re.sub(r'Starting a New Position.*?(?=\s|$)', '', post_text)
                    post_text = re.sub(r'Congrats?\s+\w+!?\s*üéâ.*?(?:Wishing you the best|What an achievement!?).*?$', '', post_text)

                    # SKIP congratulations section completely - cut off at first congratulations word
                    post_text = re.split(r'\b(Congratulations?|Congrats|Major achievement|Well done|Wishing you|Excited for you)\b', post_text, flags=re.IGNORECASE)[0].strip()

                    # Also remove duplicate job titles that might remain
                    cleaned_text = re.sub(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+\1\s+', '', cleaned_text)

                    # Remove LinkedIn metadata patterns
                    cleaned_text = re.sub(r'Feed post number \d+\s*', '', cleaned_text)

                    
                    # Remove LinkedIn metadata patterns
                    cleaned_text = re.sub(r'Feed post number \d+\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'\d+[mhdwyr]+\s*‚Ä¢\s*\d+\s+(minute|hour|day|week|month|year)s?\s+ago\s*‚Ä¢\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'Riley Giauque, CFP¬Æ.*?Personal Financial Consultant\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'Visible to anyone on or off LinkedIn\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'\d+\s+impressions?\s*View analytics\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'View analytics\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'Like Comment Repost Send\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'Auto captions.*?Edit captions\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'hashtag\s*#', '#', cleaned_text)

                    # Remove UI button text that gets mixed in
                    cleaned_text = re.sub(r'^(ing|llowing|Following)\s+', '', cleaned_text)
                    # Remove other common UI elements that might prefix posts
                    cleaned_text = re.sub(r'^(Connect|Message|Follow|Join)\s+', '', cleaned_text)

                    
                    # Clean up spaces and get first meaningful line
                    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
                    
                    if not cleaned_text or len(cleaned_text) < 10:
                        continue
                    
                    # Extract the actual post content (first meaningful part)
                    lines = cleaned_text.split('\n')
                    post_content = cleaned_text  # Use full content initially
                    
                    # Find the main content (skip metadata lines)
                    for line in lines:
                        line = line.strip()
                        if (line and len(line) > 15 and 
                            not line.endswith(' ago') and
                            not line.startswith('‚Ä¢') and
                            not re.match(r'^\d+\s+(like|comment|repost)', line.lower())):
                            post_content = line
                            break
                    
                    # Use first 100 characters for comparison
                    comparison_text = post_content[:100].lower()
                    
                    # Skip if we've already seen this post
                    if comparison_text in seen_posts:
                        continue
                    
                    seen_posts.add(comparison_text)
                    
                    # Check if this is an existing post
                    is_existing = any(comparison_text.startswith(existing[:50]) or 
                                    existing.startswith(comparison_text[:50]) 
                                    for existing in existing_post_texts)
                    
                    logger.info(f"üîç Post {i+1}: '{comparison_text[:50]}...' - {'EXISTING' if is_existing else 'NEW'}")
                    
                    if is_existing:
                        consecutive_existing += 1
                        if consecutive_existing >= 3:
                            logger.info("Found 3 consecutive existing posts - stopping")
                            return new_posts
                    else:
                        consecutive_existing = 0
                        new_posts.append({
                            'text': post_content,
                            'timestamp': "Unknown",
                            'type': 'post'
                        })
                        posts_added_this_round += 1
                        logger.info(f"‚òÖ NEW POST ADDED: {post_content[:50]}...")
                
                except Exception as e:
                    logger.error(f"Error processing post {i}: {e}")
                    continue
            
            logger.info(f"Added {posts_added_this_round} new posts this round (total: {len(new_posts)})")
            
            # Scroll down more gradually
            if scroll_attempt < 9:
                driver.execute_script("window.scrollBy(0, 800);")  # Smaller scroll
                time.sleep(2)
                
                # Try to load more content
                try:
                    load_more_buttons = driver.find_elements(By.CSS_SELECTOR, 
                        'button[aria-label*="more"], button[data-control-name*="load_more"], .artdeco-button--secondary')
                    for button in load_more_buttons:
                        if button.is_displayed() and button.is_enabled():
                            button.click()
                            time.sleep(2)
                            break
                except:
                    pass
        
        logger.info(f"SIMPLE scraper completed: Found {len(new_posts)} new posts")
        return new_posts
        
    except Exception as e:
        logger.error(f"Error in SIMPLE LinkedIn scrape: {e}")
        return []
        
    finally:  # ADD THIS BLOCK
        if driver:
            cleanup_driver_session(driver)
            driver = None
            gc.collect()
        
def extract_favicon(website_url):
    """Extract favicon URL from a website"""
    if not website_url:
        return None
        
    try:
        # Make request to the website
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(website_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for favicon links in order of preference
        favicon_selectors = [
            'link[rel="icon"]',
            'link[rel="shortcut icon"]', 
            'link[rel="apple-touch-icon"]',
            'link[rel="apple-touch-icon-precomposed"]'
        ]
        
        for selector in favicon_selectors:
            favicon_link = soup.select_one(selector)
            if favicon_link and favicon_link.get('href'):
                href = favicon_link.get('href')
                
                # Handle relative URLs
                if href.startswith('//'):
                    favicon_url = 'https:' + href
                elif href.startswith('/'):
                    parsed_url = urlparse(website_url)
                    favicon_url = f"{parsed_url.scheme}://{parsed_url.netloc}{href}"
                elif href.startswith('http'):
                    favicon_url = href
                else:
                    favicon_url = urljoin(website_url, href)
                
                return favicon_url
        
        # Fallback: try /favicon.ico
        parsed_url = urlparse(website_url)
        favicon_url = f"{parsed_url.scheme}://{parsed_url.netloc}/favicon.ico"
        
        # Test if favicon.ico exists
        try:
            favicon_response = requests.head(favicon_url, headers=headers, timeout=5)
            if favicon_response.status_code == 200:
                return favicon_url
        except:
            pass
            
        return None
        
    except Exception as e:
        print(f"Error extracting favicon for {website_url}: {e}")
        return None

def get_twitter_driver():
    global twitter_driver
    
    # Test if existing driver is still alive
    if twitter_driver is not None:
        try:
            # Try to access current_url to test if driver is alive
            _ = twitter_driver.current_url
            logger.info("Using existing Twitter driver session")
            return twitter_driver
        except:
            # Driver is dead/broken, clean it up
            logger.info("Twitter driver is dead, creating new session")
            try:
                twitter_driver.quit()
            except:
                pass
            twitter_driver = None
    
    # Create new driver
    if twitter_driver is None:
        twitter_driver = webdriver.Chrome(options=get_chrome_options())
        logger.info("Created new Twitter driver session")
    
    return twitter_driver

def ensure_twitter_login():
    """Ensure Twitter login is completed"""
    global twitter_logged_in
    
    if not twitter_logged_in:
        driver = get_twitter_driver()
        
        # Go to Twitter login
        driver.get("https://twitter.com/login")
        time.sleep(3)
        
        # Wait for manual login
        logger.info("Please login to Twitter manually. Press Enter when done...")
        show_browser_login_confirmation("Twitter")
        
        twitter_logged_in = True
        logger.info("Twitter login completed")


# ADD THIS FUNCTION HERE - RIGHT AFTER IMPORTS
def is_duplicate_or_fragment(new_text, existing_posts):
    """Check if new text is a duplicate or fragment of existing posts"""
    new_text_clean = new_text.strip().lower()
    
    for existing_post in existing_posts:
        existing_text_clean = existing_post['text'].strip().lower()
        
        # Check if new text is contained in existing post (fragment)
        if new_text_clean in existing_text_clean:
            return True
            
        # Check if existing post is contained in new text (new text is more complete)
        if existing_text_clean in new_text_clean:
            # Replace the existing post with the more complete version
            existing_post['text'] = new_text
            return True
            
        # Check for high similarity (80% overlap)
        if len(new_text_clean) > 20 and len(existing_text_clean) > 20:
            shorter = min(len(new_text_clean), len(existing_text_clean))
            longer = max(len(new_text_clean), len(existing_text_clean))
            
            # Count overlapping words
            new_words = set(new_text_clean.split())
            existing_words = set(existing_text_clean.split())
            overlap = len(new_words.intersection(existing_words))
            
            if overlap / len(new_words.union(existing_words)) > 0.8:
                return True
    
    return False



def get_facebook_driver():
    global _facebook_driver, _facebook_logged_in
    
    # Check if driver exists and is still valid
    if _facebook_driver is not None:
        try:
            # Test if driver is still alive
            _facebook_driver.current_url
        except:
            # Driver is dead, reset it
            _facebook_driver = None
            _facebook_logged_in = False
    
    if _facebook_driver is None:
        _facebook_driver = webdriver.Chrome(options=get_chrome_options())
        _facebook_driver.get("https://www.facebook.com/login")
        logger.info("Created new Facebook driver and navigated to login")
    return _facebook_driver


def ensure_facebook_login():
    """Ensure user is logged into Facebook"""
    global _facebook_logged_in

    if not _facebook_logged_in:
        driver = get_facebook_driver()
        driver.get("https://www.facebook.com/login")
        time.sleep(3)
    
        logger.info("Please login to Facebook manually. Press Enter when done...")
        show_browser_login_confirmation("Facebook")
        _facebook_logged_in = True
        logger.info("Facebook login completed")
    else:
        logger.info("Already logged into Facebook")
        

# Add these global variables at the top of your file
alternative_cache = {}
alternative_index = {}
cache_lock = threading.Lock()
pregeneration_queue = queue.Queue()
pregeneration_active = False


def scrape_facebook_new_posts_only(facebook_url, existing_posts_text):
    """Scrape Facebook profile but STOP when we encounter posts we already have"""
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    import time
    import random
    import re
    
    driver = None
    new_posts = []
    
    try:
        # Use the same session management as your main function
        driver = get_facebook_driver()
        logger.info(f"Starting INCREMENTAL Facebook scrape for: {facebook_url}")
        
        # Navigate to the profile (driver is already logged in)
        driver.get(facebook_url)
        logger.info(f"Navigated to: {facebook_url}")
        time.sleep(10)

        # Scroll to top and wait for content to load
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(3)
        
        # Parse existing posts for comparison
        existing_post_texts = set()
        if existing_posts_text:
            post_sections = existing_posts_text.split('Post ')
            for section in post_sections:
                if section.strip():
                    lines = section.strip().split('\n')
                    if len(lines) > 1:
                        post_content = '\n'.join(lines[1:]).strip()
                        if len(post_content) > 2:
                            comparison_text = post_content[:100].lower().strip()
                            existing_post_texts.add(comparison_text)
        
        logger.info(f"Found {len(existing_post_texts)} existing posts to check against")
        
        # IMPORTANT: Track ALL processed texts in this session
        processed_texts = set()
        all_processed_locations = set()  # Track locations across ALL scroll attempts
        consecutive_existing = 0
        
        # Use the same post selectors as the main function
        POST_SELECTORS = ['[role="article"]:not([data-testid*="comment"])']
        
        for scroll_attempt in range(3):
            logger.info(f"Scroll attempt {scroll_attempt + 1}")
            
            # Find posts using the same method as your working function
            post_elements = []
            for selector in POST_SELECTORS:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        post_elements.extend(elements)
                        break  # Use first working selector like main function
                except:
                    continue

            # Remove duplicates by location - USE GLOBAL LOCATION TRACKING
            unique_posts = []
            for post in post_elements:
                try:
                    location = (post.location['x'], post.location['y'])
                    if location not in all_processed_locations:
                        unique_posts.append(post)
                        all_processed_locations.add(location)  # Add to global tracking
                except:
                    continue
            
            logger.info(f"Found {len(unique_posts)} unique post elements (after global deduplication)")
            
            # Process posts using the same logic as main function
            for post_elem in unique_posts:
                try:
                    # Use the sophisticated text extraction from main function
                    before = driver.current_url  # detect accidental nav
                    
                    logger.info(f"DEBUG: Looking for See More in post starting with: {post_elem.text[:50]}...")
                    expanded = False
                    try:
                        expanded = expand_see_more_fast(driver, post_elem)
                        logger.info(f"DEBUG: Expansion result: {expanded}")
                    except Exception as e:
                        expanded = False

                    # If we accidentally navigated (clicked link-card "See more"), go back and skip
                    if driver.current_url != before:
                        logger.info("DEBUG: Navigated away by link-card 'See more' ‚Äî going back")
                        driver.back()
                        #time.sleep(0.6)
                        time.sleep(random.uniform(0.3, 0.5))
                        continue

                    if expanded:
                        #time.sleep(1.1)  # brief settle
                        time.sleep(random.uniform(0.6, 0.9))

                    # Use the sophisticated caption extraction
                    post_text = get_full_caption(post_elem)

                    # Skip privacy-restricted posts
                    if post_text and "content isn't available right now" in post_text.lower():
                        logger.info("DEBUG: Skipping privacy-restricted post")
                        continue

                    # Skip posts that are just shared links with repetitive/mangled text
                    if post_text and (post_text.count('Shared with Public') >= 2 or
                                     post_text.count('LikeCommentShare') >= 1 or
                                     len(re.findall(r'\b(Strategic|Goals|Business|Financial)\b', post_text)) >= 4):
                        logger.info(f"DEBUG: Skipping repetitive link-share post: {post_text[:60]}...")
                        continue
                    
                    # Skip if the whole thing is just UI chrome
                    if post_text and NOISE_LINE_RE.match(post_text.strip()):
                        logger.info(f"DEBUG skip pure-UI: {post_text.strip()}")
                        continue
                    
                    # Apply the same filtering as main function
                    if (not post_text or 
                        len(post_text) <= 10):
                        logger.info(f"Skipping post with insufficient content: '{post_text}'")
                        continue
                    
                    # Replace the duplicate check with exact matching only:
                    post_texts_so_far = [p['text'].lower().strip() for p in new_posts]
                    if post_text.lower().strip() in post_texts_so_far:
                        logger.info(f"Skipping exact duplicate: '{post_text[:30]}...'")
                        continue
                    
                    # Create comparison text
                    comparison_text = post_text[:100].lower().strip()
                    
                    # CRITICAL: Skip if we've already processed this text in THIS session
                    if comparison_text in processed_texts:
                        logger.info(f"Skipping already processed post: {comparison_text[:30]}...")
                        continue
                    
                    # Add to processed texts immediately
                    processed_texts.add(comparison_text)
                    
                    # Check against existing posts from previous runs
                    is_existing = False
                    for existing_text in existing_post_texts:
                        # Only match if they're nearly identical (90%+ similar)
                        if comparison_text == existing_text:  # Exact match
                            is_existing = True
                            break
                        
                    if is_existing:
                        logger.info(f"‚úì EXISTING post found: {comparison_text[:50]}...")
                        consecutive_existing += 1
                        
                        if consecutive_existing >= 2:
                            logger.info("Found 2 consecutive existing posts - stopping")
                            return new_posts
                    else:
                        logger.info(f"‚òÖ NEW post found: {comparison_text[:50]}...")
                        consecutive_existing = 0
             
                        # Extract timestamp (simplified version)
                        timestamp = "Unknown"

                        try:
                            time_selectors = [
                                'time',
                                '[data-testid="story-subtitle"]',
                                '.timestampContent',
                                'abbr[data-utime]'
                            ]
                            for time_selector in time_selectors:
                                try:
                                    time_elem = post_elem.find_element(By.CSS_SELECTOR, time_selector)
                                    timestamp = time_elem.get_attribute('title') or time_elem.text or "Unknown"
                                    if timestamp != "Unknown":
                                        break
                                except:
                                    continue
                        except:
                            pass
                        
                        new_posts.append({
                            'text': post_text,
                            'timestamp': timestamp,
                            'type': 'post'
                        })
                
                except Exception as e:
                    logger.error(f"Error processing post: {e}")
                    continue
            
            # Stop if we found existing posts
            if consecutive_existing >= 2:
                break
                
            # Scroll for more posts using same pattern as main function
            if scroll_attempt < 2:
                scroll_distance = random.randint(800, 1500)
                driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                #time.sleep(random.uniform(1, 2.5))
                time.sleep(random.uniform(0.7, 1.5))
        
        logger.info(f"Completed: Found {len(new_posts)} new posts")
        logger.info(f"Processed {len(processed_texts)} total posts in this session")

        try:
            bulk_data = {
                'platform': 'Facebook',
                'profiles_processed': 1, 
                'profiles_with_new_content': 1 if len(new_posts) > 0 else 0,
                'total_new_posts': len(new_posts)
            }
            
            with open('bulk_facebook_complete.json', 'w') as f:
                json.dump(bulk_data, f)
        except:
            pass

        return new_posts
        
    except Exception as e:
        logger.error(f"Error in Facebook scrape: {e}")
        return []
        
    finally:
        # DON'T close the driver - let it stay open like the main function
        driver = None

        
        
def expand_see_more_content(driver, post_element):
    """Expand 'See more' content in Facebook posts"""
    try:
        # Look for "See more" text and click it
        see_more_spans = post_element.find_elements(By.XPATH, ".//span[contains(text(), 'See more')]")
        for span in see_more_spans:
            try:
                driver.execute_script("arguments[0].click();", span)
                time.sleep(1)
                #logger.info("Expanded 'See more' content")
                return True
            except:
                continue
        
        # Look for clickable "See more" buttons
        clickable_see_more = post_element.find_elements(By.XPATH, ".//*[@role='button' and contains(text(), 'See more')]")
        for button in clickable_see_more:
            try:
                driver.execute_script("arguments[0].click();", button)
                time.sleep(1)
                #logger.info("Expanded 'See more' via button")
                return True
            except:
                continue
        
        return False
        
    except Exception as e:
        logger.error(f"Error expanding see more content: {e}")
        return False


def is_original_post(element):
    """Check if element is an original post (not a comment)"""
    try:
        # Check if element is inside the comment section
        # Comments are inside elements with these specific classes/attributes
        element.find_element(By.XPATH, "./ancestor::*[contains(@class, 'UFIComment') or contains(@data-testid, 'comment') or contains(@class, 'comment') or contains(@aria-label, 'Comment')]")
        return False  # Found comment ancestor - this is a comment
    except:
        pass
    
    try:
        # Check for comment pattern: "Name\nthanks Mr./Mrs./Ms./Dr. Name"
        text_content = element.text.strip()
        if '\n' in text_content:
            lines = text_content.split('\n')
            if len(lines) >= 2 and 'thanks' in lines[1].lower():
                # Check if it follows the pattern of thanking someone with a title
                thanks_line = lines[1].lower()
                if any(title in thanks_line for title in ['mr.', 'mrs.', 'ms.', 'dr.']):
                    return False
                
        # Also check if this element itself has comment-related attributes
        class_attr = element.get_attribute('class') or ''
        aria_label = element.get_attribute('aria-label') or ''
        data_testid = element.get_attribute('data-testid') or ''
        
        if ('comment' in class_attr.lower() or 
            'comment' in aria_label.lower() or 
            'comment' in data_testid.lower()):
            return False
            
        return True  # This is a post
    except:
        return True


# SCRAPE FACEBOOK FUNCTION

import re
from selenium.webdriver.common.by import By

# anything from these tokens onward is UI/meta, not caption
STOP_TAIL_RE = re.compile(
    r"(?:"  # non-capturing group so existing behavior is unchanged
    r"See insights|Create\s+ad|Original\s+audio|Reels\s*[¬∑‚Ä¢‚ãÖ]|All\s+reactions\b"
    r"|Boost\s+this\s+post|Promote\s+post"
    r"|[¬∑‚Ä¢‚ãÖ]\s*(?:Follow|Like|Comment|Share|Send|Give!?|Donate)\b"   # NEW: action row
    r"|(?:\d{1,2}:)?\d{1,2}:\d{2}\s*/\s*(?:\d{1,2}:)?\d{1,2}:\d{2}"   # timecode
    r"|\s*(?:\d+\s*){2,}\s*$"                                         # NEW: trailing number clumps like "7 1 2"
    r")",
    re.I,
)

# drop header warnings like "Your video is partially muted..." / "Only you can see this"
HEAD_NOISE_RE = re.compile(
    r"^\s*(Only you can see this|Your\s+(video|reel)\s+.*muted.*|Muted due to copyright|.*copyright.*match)",
    re.I
)

LEAD_CRUMB_RE = re.compile(
    r"^\s*[A-Z][A-Za-z .'-]{1,60}\s*"
    r"(?:updated\s+(?:his|her|their)\s+(?:cover\s+photo|profile\s+(?:picture|photo))"
    r"|changed\s+(?:his|her|their)\s+(?:cover\s+photo|profile\s+(?:picture|photo))"
    r"|added\s+(?:a\s+)?(?:cover\s+photo|new\s+profile\s+photo))\.?\s*"  # Removed the ? here
    r"(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},\s+\d{4}"
    r"|\d+\s*(?:s|m|h|d|w|mo|y)|Just\s+now)\s*¬∑\s*"
    r"(?:Shared with [A-Za-z]+|Public|Friends|Followers)?\s*",
    re.I
)

# lines that are definitely UI/comment chrome (not caption)
NOISE_LINE_RE = re.compile(
    r"""^\s*(
         View\ more\ (comments|replies)
       | Most\ relevant
       | reactions:\s*\d+\b.*  
       | New\ activity
       | Write\ a\ comment
       | Image\smay\scontain.*  
       | Comment\ as\b
       | Boost\ this\ post.*
       | Promote\ post.*
       | (?:\d+\s*)?(?:Like|Comment|Share|Send|Follow)(?:\s+(?:\d+|Like|Comment|Share|Send|Follow))*   # UI row only
       | \d+\s*(?:s|m|h|d|w|mo|y)\s*[¬∑‚Ä¢.]?\s*(Edited\s+)?\bReply\b.*   # e.g. "15w ¬∑ Reply 2" / "2mo Reply" / "1y Reply 2"
       | .*?(?:updated|changed|added)\s+(?:a\s+|his\s+|her\s+|their\s+)?(?:new\s+)?(?:cover\s+photo|profile\s+(?:picture|photo)).*

    )\s*$""",
    re.I | re.X
)

def simulate_human_behavior(driver):
    """Add random mouse movements"""
    try:
        driver.execute_script("""
            const x = Math.random() * window.innerWidth;
            const y = Math.random() * window.innerHeight;
            
            const event = new MouseEvent('mousemove', {
                clientX: x,
                clientY: y,
                bubbles: true
            });
            document.dispatchEvent(event);
        """)
    except:
        pass

def get_full_caption(elem):
    """
    Return full post caption (all paragraphs + hashtags), trimmed before FB UI/meta.
    Prefers the 'html-div' wrapper; falls back to message-scope paragraphs only.
    """
    # 1) scope to the message area if it exists (avoids header/footer/author rows)
    try:
        scope = elem.find_element(
            By.CSS_SELECTOR,
            "[data-ad-comet-preview='message'], [data-ad-preview='message']"
        )
    except Exception:
        scope = None

    # extra fallback: Facebook's common message wrapper you highlighted
    if scope is None:
        try:
            scope = elem.find_element(By.CSS_SELECTOR, "div.x1vvkbs.x126k92a")
        except Exception:
            scope = elem  # last resort

    parts = []

    # 2) Prefer the rich-text wrapper that holds the whole caption
    wrappers = scope.find_elements(
        By.XPATH,
        ".//*[contains(concat(' ', normalize-space(@class), ' '), ' html-div ') and not(@aria-hidden='true')]"
    )
    if wrappers:
        for w in wrappers:
            t = (w.get_attribute("textContent") or w.get_attribute("innerText") or "").strip()
            if t:
                parts.append(t)
    else:
        # 3) Otherwise, collect only visible paragraph nodes inside the message scope
        paras = scope.find_elements(
            By.XPATH,
            ".//*[@dir='auto' and not(@aria-hidden='true')"
            " and not(ancestor::*[@role='button'])"
            " and not(ancestor::*[@role='link'])]"
        )
        for p in paras:
            t = (p.get_attribute("textContent") or p.get_attribute("innerText") or p.text or "").strip()
            if t:
                parts.append(t)

        # 3a) NEW: If still nothing, look anywhere in the post for link-wrapped text (how Reel headers render)
        if not parts:
            linked_paras = elem.find_elements(
                By.XPATH,
                ".//*[@dir='auto' and not(@aria-hidden='true') "
                "and not(ancestor::*[@role='button']) "
                "and ancestor::*[@role='link'] "
                "and not(.//img)]"   # avoid image alt text
            )
            for p in linked_paras:
                t = (p.get_attribute('textContent') or p.get_attribute('innerText') or p.text or '').strip()
                if t:
                    parts.append(t)

    # 4) de-dup whole chunks (some posts render duplicated blocks)
    seen, uniq = set(), []
    for p in parts:
        if p not in seen:
            seen.add(p)
            uniq.append(p)

    text = "\n\n".join(uniq).strip()
    #print(f"DEBUG: Raw text after collection: '{text[:100]}...'")


    # 4b) remove leading warning banners (muted/copyright/visibility)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    while lines and HEAD_NOISE_RE.match(lines[0]):
        lines.pop(0)
    text = "\n\n".join(lines).strip()
    #print(f"DEBUG: After HEAD_NOISE_RE: '{text[:100]}...'")

    text = LEAD_CRUMB_RE.sub("", text)
    #print(f"DEBUG: After LEAD_CRUMB_RE: '{text[:100]}...'")

    if not re.search(r"[A-Za-z#@]|https?://", text):  # no words, tags, or links
        #print(f"DEBUG: Failed content check - text: '{text}'")

        text = ""                                      # drop empty/irrelevant posts
    #else:
        #print(f"DEBUG: Passed content check")
    

    # 5) hard-stop at the first UI/meta token (fixes the ‚ÄúOriginal audio ¬∑ Reels ¬∑ ‚Ä¶‚Äù tails)
    m = STOP_TAIL_RE.search(text)
    if m:
        #print(f"DEBUG: STOP_TAIL_RE matched at position {m.start()}: '{text[m.start():m.end()]}'")
        text = text[:m.start()].rstrip()
        #print(f"DEBUG: After STOP_TAIL_RE: '{text}'")

    # NEW: Remove metadata header only if it matches specific timestamp patterns
    if re.match(r'^[A-Z][A-Za-z\s&‚Ñ¢¬Æ]*\d+[smhd]\s*¬∑\s*(?:Shared with Public\s*)', text):
        # Remove "Blue Ox Advisors2h ¬∑ Shared with Public" type headers
        text = re.sub(r'^[^¬∑]*\d+[smhd]\s*¬∑\s*(?:Shared with Public\s*)?', '', text).strip()
    elif re.match(r'^[A-Z][A-Za-z\s&‚Ñ¢¬Æ]*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)', text):
        # Remove "Blue Ox AdvisorsJanuary 1, 2024 ¬∑ Shared with Public" type headers
        text = re.sub(r'^[^¬∑]*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},?\s+\d{4}\s*¬∑\s*(?:Shared with Public\s*)?', '', text).strip()
    


    # 5a) remove any lines that look like comment/timestamp UI
    if text:
        text = "\n\n".join(
            ln for ln in (l.strip() for l in text.splitlines())
            if ln and not NOISE_LINE_RE.match(ln)
        ).strip()

    if text:
        # kill glued-on " ¬∑ Follow/Give/Like..." tails and any trailing number clumps
        # was: r"\s*[‚Ä¢¬∑.]\s*(Follow|Like|Comment|Share|Send|Give!?|Donate)\b.*$"
        text = re.sub(r"\s*[‚Ä¢¬∑]\s*(Follow|Like|Comment|Share|Send|Give!?|Donate)\b.*$", "", text, flags=re.I)
        text = re.sub(r"\s*(?:\d+\s*){2,}\s*$", "", text)


    # 5b) drop any trailing author/date crumb like "Riley Giauque ¬∑ ..."
    lines = [ln.strip() for ln in text.splitlines()]
    while lines and ('¬∑' in lines[-1]) and not re.search(r"(#|http)", lines[-1]):
        lines.pop()
    text = "\n\n".join(lines).strip()

    text = re.sub(r"(?m)^\s*as\s+[A-Z][A-Za-z .'-]{1,60}\s*$", "", text).strip()

    # --- If there's no caption text AND the post is just a photo, skip it ---
    if not text:
        try:
            # any image present but no visible caption ‚Üí treat as picture-only
            if scope.find_elements(By.XPATH, ".//img | .//*[@role='img'] | .//div[contains(@style,'background-image')]"):
                return ""   # ignore picture-only posts
        except Exception:
            pass

    # 5c) if the page/author name got glued to the end, drop it
    try:
        author = (elem.find_element(By.XPATH, ".//h3//*[self::a or self::strong or self::span][1]")
                        .get_attribute("innerText") or "").strip()
    except Exception:
        author = ""
    if author:
        text = re.sub(rf"\s*\b{re.escape(author)}\b\s*$", "", text)


    # --- RESCUE: if caption vanished or is suspiciously short, take the longest html-div in the article ---
    if not text:  # length threshold is conservative; adjust if you like
        rescue = ""
        # look for any rich-text wrapper in the whole article (avoid buttons/links)
        for d in elem.find_elements(
            By.XPATH,
            ".//*[contains(concat(' ', normalize-space(@class), ' '), ' html-div ')]"
            "[not(@aria-hidden='true') and not(ancestor::*[@role='button'])]"
        ):
            s = (d.get_attribute('textContent') or d.get_attribute('innerText') or "").strip()
            if not s:
                continue
            # drop obvious UI chrome lines
            lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
            lines = [ln for ln in lines if not NOISE_LINE_RE.match(ln) and not HEAD_NOISE_RE.match(ln) and not LEAD_CRUMB_RE.match(ln)]
            s = "\n\n".join(lines).strip()
            if not re.search(r"[A-Za-z#@]|https?://", s): continue   # no content ‚Üí skip

            if s and len(s) > len(rescue):
                rescue = s

        if rescue:
            text = rescue
            # re-apply the tail/crumb trimming we use above
            m = STOP_TAIL_RE.search(text)
            if m:
                text = text[:m.start()].rstrip()
            lines = [ln.strip() for ln in text.splitlines() if ln.strip() and not NOISE_LINE_RE.match(ln)]
            while lines and ('¬∑' in lines[-1]) and not re.search(r"(#|http)", lines[-1]):
                lines.pop()
            text = "\n\n".join(lines).strip()
            text = LEAD_CRUMB_RE.sub("", text)
            if not re.search(r"[A-Za-z#@]|https?://", text):  # no words, tags, or links
                text = ""                                      # drop empty/irrelevant posts


    # --- Hashtag recovery: ensure all tags present even if innerText shows just "#" ---
    try:
        tag_elems = scope.find_elements(
            By.XPATH,
            ".//a[contains(@href,'/hashtag/')]"
            " | .//*[@role='link' and contains(@aria-label, '#')]"
            " | .//a[starts-with(normalize-space(.), '#')]"
        )
        found = []
        for el in tag_elems:
            tag = None
            href = el.get_attribute("href") or ""
            m = re.search(r"/hashtag/([^/?#]+)", href)
            if m:
                tag = "#" + m.group(1)
            else:
                label = (el.get_attribute("aria-label") or el.get_attribute("innerText") or el.text or "").strip()
                if label.startswith("#") and len(label) > 1:
                    tag = label
            if tag and tag not in found:
                found.append(tag)

        # append any missing tags to the end of the caption (preserve order)
        for tag in found:
            if tag not in text:
                if text and not text.endswith(" "):
                    text += " "
                text += tag
    except Exception:
        pass
    # --- end hashtag recovery ---


    # 6) normalize spacing; ensure a space before hashtags if glued
    text = re.sub(r"\s{3,}", "  ", text)
    text = re.sub(r"(?<=\w)(#)", r" \1", text)


    # Remove everything after "See less" (case insensitive)
    if 'see less' in text.lower():
        see_less_pos = text.lower().find('see less')
        text = text[:see_less_pos].strip()

    return text


def expand_see_more_fast(driver, post_elem, timeout=3):
    """
    Expand the truncated post text. Works for normal and shared/nested posts.
    Skips 'See more' inside link previews.
    Returns True if any expansion occurred.
    """
    # 1) Collect likely message scopes (outer and nested/shared)
    scopes = post_elem.find_elements(
        By.CSS_SELECTOR,
        "[data-ad-comet-preview='message'], [data-ad-preview='message']"
    )
    if not scopes:
        # try nested messages under child <article> (shared posts)
        scopes = post_elem.find_elements(
            By.CSS_SELECTOR,
            "[role='article'] [data-ad-comet-preview='message'], "
            "[role='article'] [data-ad-preview='message']"
        )
    if not scopes:
        # try the common message wrapper class as fallback
        fallback_scopes = post_elem.find_elements(By.CSS_SELECTOR, "div.x1vvkbs.x126k92a")
        if fallback_scopes:
            scopes = fallback_scopes
        else:
            scopes = [post_elem]

    # Additional fallback: if we found scopes but no "See more" elements, also check the whole post
    if scopes and scopes != [post_elem]:
        has_see_more_in_scopes = any(
            scope.find_elements(By.XPATH, ".//*[contains(translate(normalize-space(.),'SEE MORE','see more'),'see more')]")
            for scope in scopes
        )
        if not has_see_more_in_scopes:
            # Check if "See more" exists anywhere in the post (like reel overlays)
            whole_post_see_more = post_elem.find_elements(By.XPATH, ".//*[contains(translate(normalize-space(.),'SEE MORE','see more'),'see more')]")
            if whole_post_see_more:
                scopes.append(post_elem)  # Add whole post as additional scope

    # A) Only skip Reels that have no text/caption to capture
    try:
        reel_elements = post_elem.find_elements(
            By.XPATH,
            ".//*[@aria-label='Original audio']"
            " | .//a[contains(@href,'/reel/')]"
            " | .//*[contains(@class,'reel') or contains(@data-testid,'reel')]"
        )
        is_reel = bool(reel_elements)
        if is_reel:
            # Check if there's any meaningful text content to capture
            text_content = get_full_caption(post_elem)
            if not text_content or len(text_content.strip()) < 10:
                return False
    except Exception:
        is_reel = False

    expanded_any = False

    # B) For reels, find and click the "See more" link
    if is_reel:
        try:
            # Simple, direct search for "See more" text in the reel
            see_more_elements = post_elem.find_elements(
                By.XPATH,
                ".//*[normalize-space(text())='See more' or contains(normalize-space(text()),'See more')]"
            )
        
            for i, elem in enumerate(see_more_elements):
                try:
                    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", elem)
                    elem.click()
                    time.sleep(random.uniform(0.3, 0.6))
                    expanded_any = True
                    break
                except Exception as e:
                    try:
                        driver.execute_script("arguments[0].click();", elem)
                        time.sleep(random.uniform(0.3, 0.6))
                        expanded_any = True
                        break
                    except Exception as e2:
                        post_id = post_elem.get_attribute('data-pagelet') or post_elem.get_attribute('id') or 'unknown'
                        print(f"DEBUG: Cannot click reel See more [{i}] - Post ID: {post_id}")
                    
        except Exception as e:
            print(f"DEBUG: Reel See more search failed: {e}")

    # 2) In each scope, prefer button-like 'See more' and exclude links
    SEE_MORE_BTN = (
        ".//*[@role='button' and "
        "contains(translate(normalize-space(.),'SEE MORE','see more'),'see more') and "
        "not(ancestor::a) and not(ancestor::*[@role='link'])]"
    )
    SEE_MORE_FALLBACK = (
        ".//*[self::div or self::span]"
        "[contains(translate(normalize-space(.),'SEE MORE','see more'),'see more')]"
        "[not(ancestor::a) and not(ancestor::*[@role='link'])]"
    )
    SEE_MORE_LINK_WRAPPED = (
        ".//*[contains(translate(normalize-space(.),'SEE MORE','see more'),'see more')]"
    )
    SEE_MORE_LINK_IN_MSG = (
        ".//a[contains(translate(normalize-space(.),'SEE MORE','see more'),'see more')]"
        "[not(contains(@href,'/reel')) and not(contains(@href,'/reels')) and not(contains(@href,'/watch'))]"
    )
    SEE_MORE_ANY = (
        ".//*[contains(translate(normalize-space(.),'SEE MORE','see more'),'see more')]"
    )

    for scope in scopes:
        candidates = scope.find_elements(By.XPATH, SEE_MORE_BTN)
        if not candidates:
            candidates = scope.find_elements(By.XPATH, SEE_MORE_FALLBACK)

            if not candidates:
                # "See more" text is present but wrapped inside a clickable link/button ancestor
                wrapped = scope.find_elements(By.XPATH, SEE_MORE_LINK_WRAPPED)
                safe_links = []
                for node in wrapped:
                    # nearest clickable ancestor (a | role=link | role=button)
                    anc = node.find_elements(
                        By.XPATH,
                        "ancestor::*[(self::a) or (@role='link') or (@role='button')][1]"
                    )
                    if not anc:
                        continue
                    a = anc[0]
                    href = (a.get_attribute("href") or "")
                    # exclude reels/watch, but allow normal /video/ links
                    if href and re.search(r"/reel|/reels|/watch", href, flags=re.I):
                        continue
                    safe_links.append(a)
                candidates = safe_links

            # If still no candidates, try safe "See more" links within the message scope
            if not candidates:
                link_candidates = scope.find_elements(By.XPATH, SEE_MORE_LINK_IN_MSG)
                candidates = link_candidates

        for btn in candidates:
            # Defensive: skip unsafe links (reels/watch), but allow message "See more" links
            if btn.tag_name.lower() == "a" or btn.get_attribute("role") == "link":
                href = btn.get_attribute("href") or ""
                if href and re.search(r"/reel|/reels|/watch", href, flags=re.I):
                    continue

            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
            try:
                WebDriverWait(driver, timeout).until(EC.element_to_be_clickable(btn))
            except Exception:
                pass  # we'll still try JS click

            # Click: normal first, then JS
            try:
                btn.click()
            except Exception as e:
                try:
                    driver.execute_script("arguments[0].click();", btn)
                except Exception as e2:
                    post_id = post_elem.get_attribute('data-pagelet') or post_elem.get_attribute('id') or 'unknown'
                    print(f"DEBUG: Cannot click See more button - Post ID: {post_id}")

            # Wait until this scope no longer has a button-like 'See more'
            try:
                WebDriverWait(driver, timeout).until(
                    lambda d: len(scope.find_elements(By.XPATH, SEE_MORE_ANY)) == 0
                )
            except Exception:
                # Best-effort; not fatal if another 'See more' remains
                pass

            expanded_any = True

    # No aggressive fallback - stick to the safe existing logic

    return expanded_any


def extract_all_posts_batch(driver):
    """Extract all posts in one JavaScript call, preserving your filtering logic"""
    return driver.execute_script("""
        // Your exact same selectors, but in JavaScript
        const posts = document.querySelectorAll('[role="article"]:not([data-testid*="comment"])');
        const results = [];
        
        posts.forEach((post, index) => {
            try {
                // Quick pre-filter (like your fast_post_filter)
                const textContent = post.textContent || post.innerText || '';
                if (!textContent || textContent.length < 20) {
                    return; // Skip empty posts
                }
                
                // Check for message container (like your scope finding)
                const messageScope = post.querySelector('[data-ad-comet-preview="message"], [data-ad-preview="message"]') 
                                  || post.querySelector('div.x1vvkbs.x126k92a') 
                                  || post;
                
                if (!messageScope) {
                    return; // Skip posts without message containers
                }
                
                // Check if "See more" exists (case insensitive)
                const hasSeMore = textContent.toLowerCase().includes('see more');
                
                // Basic content filtering
                const lowerText = textContent.toLowerCase();
                if (lowerText.includes("content isn't available right now") ||
                    lowerText.includes('write a comment') ||
                    lowerText.includes('most relevant')) {
                    return; // Skip UI-only or restricted posts
                }
                
                results.push({
                    index: index,
                    element: post,
                    hasSeMore: hasSeMore,
                    previewText: textContent.substring(0, 200), // For deduplication
                    location: {
                        x: post.getBoundingClientRect().left,
                        y: post.getBoundingClientRect().top + window.pageYOffset
                    }
                });
                
            } catch (e) {
                // Skip problematic posts silently
            }
        });
        
        return results;
    """)

def smart_scrape_batch(driver, existing_post_texts):
    """Process posts in batches while keeping all your functionality"""
    
    # 1. ONE JavaScript call to get ALL posts info
    js_results = extract_all_posts_batch(driver)
    
    if not js_results:
        return []
    
    # 2. Quick deduplication by location (like your current logic)
    unique_posts = []
    seen_locations = set()
    
    for result in js_results:
        loc = (result['location']['x'], result['location']['y'])
        if loc not in seen_locations:
            seen_locations.add(loc)
            unique_posts.append(result)
    
    # 3. Process only posts that need "See more" expansion
    posts_needing_expansion = [p for p in unique_posts if p['hasSeMore']]
    posts_ready = [p for p in unique_posts if not p['hasSeMore']]
    
    if len(unique_posts) > 5:  # Only print for significant batches
        print(f"Batch found {len(unique_posts)} posts: {len(posts_needing_expansion)} need expansion, {len(posts_ready)} ready")
    
    final_posts = []
    
    # 4. Expand "See more" with retry logic
    expansion_failed = []   # Track posts that failed expansion
    
    for post_info in posts_needing_expansion:
        max_retries = 2
        expansion_succeeded = False
        
        for retry in range(max_retries):
            try:
                before = driver.current_url
                time.sleep(0.4)  # Wait for DOM stability
            
                expanded = expand_see_more_fast(driver, post_info['element'])
            
                if driver.current_url != before:
                    driver.back()
                    time.sleep(0.5)
                    expansion_succeeded = True
                    break  # Don't retry if we navigated away
                
                if expanded:
                    time.sleep(random.uniform(0.8, 1.2))
                    expansion_succeeded = True  # Mark as successful
                    break  # Success - stop retrying
                elif retry == 0:  # First attempt failed, try once more
                    time.sleep(0.5)  # Wait longer before retry
                    continue
                else:
                    break  # Max retries reached
                
            except Exception as e:
                if retry == max_retries - 1:  # Last retry
                    continue
                time.sleep(0.3)

        # NEW: Track failed expansions
        if not expansion_succeeded:
            expansion_failed.append(post_info)

    # NEW: Add failed expansions to ready posts so they get processed anyway
    posts_ready.extend(expansion_failed)
    if expansion_failed:
        print(f"Skipped {len(expansion_failed)} stubborn 'See more' buttons - processing anyway")

    
    # 5. Extract final text using your EXACT get_full_caption function
    all_candidate_posts = posts_needing_expansion + posts_ready
    final_posts = []
    
    for post_info in all_candidate_posts:
        try:
            # Use your existing get_full_caption function unchanged
            text = get_full_caption(post_info['element'])
            
            # Your existing filtering logic
            if (text and 
                len(text) > 10 and 
                text not in existing_post_texts and
                not ("content isn't available right now" in text.lower()) and
                not (text.count('Shared with Public') >= 2) and
                not (text.count('LikeCommentShare') >= 1) and
                not (len(re.findall(r'\b(Strategic|Goals|Business|Financial)\b', text)) >= 4) and
                not NOISE_LINE_RE.match(text.strip())):
                
                final_posts.append({
                    'text': text,
                    'timestamp': 'Unknown',
                    'type': 'post'
                })
                existing_post_texts.add(text)
                
        except Exception as e:
            continue
    
    # Add debug info about actual new posts found
    if len(unique_posts) > 5:  # Only print for significant batches
        print(f"Batch found {len(unique_posts)} posts: {len(posts_needing_expansion)} need expansion, {len(posts_ready)} ready ‚Üí {len(final_posts)} NEW posts")
    
    return final_posts

def cleanup_browser_memory(driver):
    """Cleanup browser memory to prevent crashes"""
    try:
        driver.execute_script("""
            // Clear console
            if (typeof console.clear === 'function') {
                console.clear();
            }
            
            // Force garbage collection if available
            if (window.gc) {
                window.gc();
            }
            
            // Clear some cached data
            if (window.caches) {
                caches.keys().then(names => {
                    names.forEach(name => {
                        if (name.includes('facebook')) {
                            caches.delete(name);
                        }
                    });
                });
            }
        """)
    except:
        pass


def recover_browser_if_needed(driver, scroll_attempt):
    """Check browser health and recover if needed"""
    try:
        # Quick health check
        driver.execute_script("return document.readyState;")
        return True
    except Exception as e:
        print(f"Browser health check failed at scroll {scroll_attempt}: {e}")
        try:
            # Try to recover
            driver.refresh()
            time.sleep(3)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.8);")  # Scroll back down
            time.sleep(2)
            return True
        except:
            print("Browser recovery failed")
            return False


def scrape_facebook_profile(facebook_url):
    """Fast Facebook profile scraper with login wait"""

    driver = None
    posts = []
    existing_post_texts = set()

    try:
        driver = get_facebook_driver()
        
        # **LOGIN WAIT LOGIC**
        print("Waiting for Facebook login...")
        login_timeout = 180  # 3 minutes
        login_start_time = time.time()
        
        while time.time() - login_start_time < login_timeout:
            try:
                current_url = driver.current_url
                if 'login' in current_url or 'checkpoint' in current_url:
                    time.sleep(5.0)
                    continue
                
                # Check for logged-in elements
                if (driver.find_elements(By.CSS_SELECTOR, '[data-testid="FB_logo"]') or 
                    driver.find_elements(By.CSS_SELECTOR, '[role="banner"]')):
                    print("Login detected!")
                    break
                    
                time.sleep(5)
                
            except Exception as e:
                time.sleep(5)
        else:
            print("Login timeout!")
            return []

        # Navigate to profile after login
        print(f"Navigating to: {facebook_url}")
        driver.get(facebook_url)
        time.sleep(3)

        # Scroll to top to ensure we're at posts
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)

        # Posts only (exclude comments and avoid message blocks)
        POST_SELECTORS = ['[role="article"]:not([data-testid*="comment"])']


        # Fast scrolling loop
        max_scrolls = 500
        no_new_content_count = 0

        for scroll_attempt in range(max_scrolls):
            # Only log every 3rd attempt
            #if scroll_attempt % 8 == 0:
            if scroll_attempt % 15 == 0:
                print(f"Scroll {scroll_attempt + 1}/{max_scrolls}")

            # Ultra-aggressive cleanup every 5 scrolls (instead of 10)
            if scroll_attempt % 8 == 0 and scroll_attempt >= 8:
                articles_before = len(driver.find_elements(By.CSS_SELECTOR, '[role="article"]'))
        
                driver.execute_script("""
                    // Remove old posts more aggressively
                    const articles = document.querySelectorAll('[role="article"]');
                    const keepLast = 20; // Keep only 20 posts
            
                    for (let i = 0; i < articles.length - keepLast; i++) {
                        articles[i].remove();
                    }
                    
                    // Aggressive cleanup of other memory hogs
                    document.querySelectorAll('video, iframe, [role="img"]').forEach(el => {
                        if (el.getBoundingClientRect().top < -500) {
                            el.src = '';
                            el.remove();
                        }
                    });
                    
                    // Remove cached images that are far off screen
                    document.querySelectorAll('img').forEach(img => {
                        if (img.getBoundingClientRect().top < -1000 || img.getBoundingClientRect().top > window.innerHeight + 1000) {
                            img.src = '';
                            img.remove();
                        }
                    });
                    
                    // Clear console and force garbage collection
                    if (typeof console.clear === 'function') console.clear();
                    if (window.gc) window.gc();
                """)
        
                articles_after = len(driver.find_elements(By.CSS_SELECTOR, '[role="article"]'))
                print(f"DOM cleanup: {articles_before} ‚Üí {articles_after} posts")
                
                # Force Python garbage collection too
                import gc
                gc.collect()
                
            # ADD THIS LINE:
            if scroll_attempt % 3 == 0:  # Every 3rd scroll
                simulate_human_behavior(driver)

            
            # NEW: Batch processing every 2 scrolls for maximum efficiency
            new_posts_found = 0
            
            # Process expansion EVERY scroll to catch all "See more" buttons
            process_this_scroll = True  # Always process
            if process_this_scroll:
                new_posts = smart_scrape_batch(driver, existing_post_texts)
                posts.extend(new_posts)
                new_posts_found = len(new_posts)

    
                # ADD THIS BLOCK RIGHT HERE:
                if new_posts_found == 0:
                    no_new_content_count += 1
                    # Stop if no new content for 5 consecutive attempts AND we're past a minimum scroll count
                    if no_new_content_count >= 5 and scroll_attempt >= 20:
                        print(f"No new content found after {no_new_content_count} attempts at scroll {scroll_attempt} - stopping")
                        break
        
                    # Also check if Facebook stopped serving new content
                    if scroll_attempt > 30:
                        current_post_count = len(driver.find_elements(By.CSS_SELECTOR, '[role="article"]'))
                        if hasattr(scrape_facebook_profile, 'last_post_count'):
                            if current_post_count == scrape_facebook_profile.last_post_count and no_new_content_count >= 3:
                                print(f"Facebook appears to have stopped loading new content (same {current_post_count} posts) - stopping")
                                break
                        scrape_facebook_profile.last_post_count = current_post_count
                else:
                    no_new_content_count = 0
                
            else:
                # Fallback: quick individual processing on off-scrolls
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, '[role="article"]:not([data-testid*="comment"])')
                    if elements:
                        # Quick processing without expansion - check last 8 posts instead of 5
                        for elem in elements[-8:]:  
                            try:
                                if not elem.get_attribute("textContent") or len(elem.get_attribute("textContent")) < 20:
                                    continue
                    
                                # Quick pre-check for duplicates using preview text
                                preview = elem.get_attribute("textContent")[:100].lower()
                                if any(preview in existing_text.lower()[:100] for existing_text in list(existing_post_texts)[-20:]):
                                    continue
                        
                                text = get_full_caption(elem)
                                if (text and len(text) > 10 and text not in existing_post_texts):
                                    posts.append({
                                        'text': text,
                                        'timestamp': 'Unknown', 
                                        'type': 'post'
                                    })
                                    existing_post_texts.add(text)
                                    new_posts_found += 1
                            except:
                                continue
                except Exception as e:
                    pass
            # Smart stopping logic for long scrolling
            #if new_posts_found == 0:
                #no_new_content_count += 1
                # More lenient stopping - only stop if we're past scroll 40 AND no content
                #if no_new_content_count >= 10 and scroll_attempt >= 40:
                    #print(f"No new content found after {no_new_content_count} attempts and {scroll_attempt} scrolls - stopping")
                    #break
                #elif no_new_content_count >= 15:  # Absolute maximum attempts
                    #print(f"Maximum attempts reached ({no_new_content_count}) - stopping")
                    #break
            #else:
                #no_new_content_count = 0

            # Browser health check every 20 scrolls
            if scroll_attempt % 15 == 0 and scroll_attempt > 0:
                if not recover_browser_if_needed(driver, scroll_attempt):
                    print("Browser recovery failed - stopping to prevent crash")
                    break

            # Human-like scrolling (simplified)
            scroll_distance = random.randint(800, 1500)
            try:
                driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
            except Exception as e:
                print(f"Scroll failed at attempt {scroll_attempt}: {e}")
                if not recover_browser_if_needed(driver, scroll_attempt):
                    break
                    
            time.sleep(random.uniform(0.7, 1.5))

        print(f"Completed: {len(posts)} posts")
        return posts

    except Exception as e:
        print(f"Error: {e}")
        return []
    finally:
        if driver:
            driver = None


# END SCRAPE FACEBOOK POSTS SECTION

# NEW FACEBOOK SCRAPER USING API

import requests

def get_facebook_page_id(page_url: str) -> str:
    """
    Convert vanity URL into a numeric page ID.
    Example input: https://www.facebook.com/riley.giauque.2025/
    """
    username = page_url.rstrip("/").split("/")[-1]
    url = f"https://graph.facebook.com/v21.0/{username}?access_token={ACCESS_TOKEN}"
    resp = requests.get(url).json()
    if "id" not in resp:
        raise Exception(f"Error resolving page ID: {resp}")
    return resp["id"]

def fetch_all_public_posts(page_url: str, limit: int = 100):
    """
    Fetch all historical public posts from a Facebook page using Page Public Content Access.
    """
    page_id = get_facebook_page_id(page_url)
    posts = []

    url = f"https://graph.facebook.com/v21.0/{page_id}/posts"
    params = {
        "fields": "id,message,permalink_url,created_time",  # Only these 4 safe fields
        "limit": limit,
        "access_token": ACCESS_TOKEN
    }

    while url:
        resp = requests.get(url, params=params).json()
        if "error" in resp:
            raise Exception(f"Graph API Error: {resp['error']}")
        
        posts.extend(resp.get("data", []))

        # pagination
        url = resp.get("paging", {}).get("next", None)
        params = None  # already included in 'next'

    return posts

# Helper for ~ Add Site button and RAFQP
def save_facebook_snapshot(advisor_id, page_url, page_title, posts):
    """Save Facebook posts into website_snapshots (shared by single + bulk)."""
    content_lines = []
    for idx, post in enumerate(posts, start=1):
        fb_id = post.get("id")
        msg = post.get("message", "")
        permalink = post.get("permalink_url")
        created = post.get("created_time")

        content_lines.append(f"Post {idx}: {fb_id} ({created})")
        if msg:
            content_lines.append(msg)
        if permalink:
            content_lines.append(f"üîó {permalink}")
        content_lines.append("")

    content_text = "\n".join(content_lines).strip()

    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("""
        SELECT id FROM website_snapshots
        WHERE advisor_id = %s AND page_url = %s
    """, (advisor_id, page_url))
    row = cur.fetchone()

    if row:
        cur.execute("""
            UPDATE website_snapshots
            SET content_text = %s, last_checked = CURRENT_TIMESTAMP
            WHERE id = %s
        """, (content_text, row[0]))
    else:
        content_hash = hashlib.sha256(content_text.encode("utf-8")).hexdigest()
        cur.execute("""
            INSERT INTO website_snapshots (advisor_id, page_url, page_title, content_hash, content_text)
            VALUES (%s, %s, %s, %s, %s)
        """, (advisor_id, page_url, page_title or page_url, content_hash, content_text))

    conn.commit()
    cur.close()
    release_db_connection(conn)

    app.logger.info(f"‚úÖ Saved {len(posts)} posts into website_snapshots for advisor {advisor_id}")
    return len(posts)


# ~ Add Site button on the advisor details HTML
def handle_facebook_graph(advisor_id, page_url, page_title):
    """Fetch ALL posts from a Facebook Page and save them in ONE blob (same as manual scrape)"""
    try:
        username = page_url.rstrip("/").split("/")[-1]

        # Resolve to Page ID
        basic_resp = requests.get(
            f"https://graph.facebook.com/v21.0/{username}",
            params={"access_token": ACCESS_TOKEN}
        ).json()
        logger.info(f"üî• BASIC PAGE INFO: {basic_resp}")

        page_id = basic_resp.get("id")
        if not page_id:
            logger.error(f"Could not resolve page ID for {username}")
            return redirect(url_for("advisor_details", advisor_id=advisor_id))

        # === Fetch ALL posts with pagination + rate limit handling ===
        all_posts = []
        url = f"https://graph.facebook.com/v21.0/{page_id}/posts"
        params = {
            "fields": "id,message,permalink_url,created_time",
            "limit": 100,  # max per request
            "access_token": ACCESS_TOKEN
        }

        while url:
            try:
                resp = requests.get(url, params=params).json()

                # ‚úÖ Handle rate limiting (error 4 or 17 etc.)
                if "error" in resp:
                    err = resp["error"]
                    if err.get("code") in (4, 17, 613):  # API limit errors
                        wait_time = int(err.get("fbtrace_id", "5")) if "fbtrace_id" in err else 60
                        logger.warning(f"‚ö†Ô∏è Rate limited: {err}. Sleeping {wait_time}s...")
                        time.sleep(wait_time)
                        continue  # retry same URL

                    raise Exception(f"Facebook API error: {err}")

                data = resp.get("data", [])
                all_posts.extend(data)

                paging = resp.get("paging", {})
                url = paging.get("next")  # already includes token/params
                params = {}  # clear params when using next URL

                # Small pause between requests to be kind to API
                time.sleep(0.2)

            except Exception as e:
                logger.error(f"Error fetching posts: {e}")
                break
        logger.info(f"üî• Retrieved {len(all_posts)} posts for {username}")

        save_facebook_snapshot(advisor_id, page_url, page_title, all_posts)

        return redirect(url_for("advisor_details", advisor_id=advisor_id))

    except Exception as e:
        logger.error(f"Debug error: {str(e)}", exc_info=True)
        return redirect(url_for("advisor_details", advisor_id=advisor_id))

# Fucntion for pulling posts from Facebook using 15 workers in parallel (Retrieve All Facebook Queued Profiles Button)
async def fetch_facebook_page(session, url, advisor_id):
    """Fetch all posts for one advisor's FB page with jittered sleeps"""
    results = []

    # Extract username from the full FB URL
    username = url.rstrip("/").split("/")[-1]

    # Resolve username ‚Üí Page ID
    basic_url = f"https://graph.facebook.com/v21.0/{username}"
    async with session.get(basic_url, params={"access_token": ACCESS_TOKEN}) as resp:
        basic_resp = await resp.json()
        page_id = basic_resp.get("id")

    if not page_id:
        app.logger.error(f"‚ùå Could not resolve page ID for {url}")
        return 0

    app.logger.info(f"üîë Resolved {url} ‚Üí page_id={page_id}")

    # Now fetch posts from the Page ID
    page_url = f"https://graph.facebook.com/v21.0/{page_id}/posts"
    params = {
        "fields": "id,message,permalink_url,created_time",
        "limit": 100,
        "access_token": ACCESS_TOKEN
    }

    while page_url:
        async with session.get(page_url, params=params) as resp:
            data = await resp.json()
            if "error" in data:
                app.logger.error(f"‚ùå Error fetching posts for {url}: {data['error']}")
                break
            results.extend(data.get("data", []))
            page_url = data.get("paging", {}).get("next")
            params = {}  # use next URL directly
            await asyncio.sleep(random.uniform(0.1, 0.5))

    save_facebook_snapshot(advisor_id, url, url, results)

    return len(results)

    
# END NEW FACEBOOK SCRAPER FUNCTION
            
            
def scrape_twitter_new_posts_only(twitter_url, existing_tweets_text):
    """Scrape Twitter profile but STOP when we encounter tweets we already have"""
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    import time
    import random
    from __main__ import show_browser_login_confirmation
    
    def click_show_more_buttons(tweet_elem):
        """Click all 'Show more' buttons within a tweet element to expand content"""
        try:
            xpath_selectors = [
                './/span[contains(text(), "Show more")]',
                './/div[contains(text(), "Show more")]',
                './/span[contains(text(), "show more")]',
                './/div[contains(text(), "show more")]'
            ]
            
            show_more_buttons = []
            for xpath in xpath_selectors:
                try:
                    buttons = tweet_elem.find_elements(By.XPATH, xpath)
                    show_more_buttons.extend(buttons)
                except:
                    continue
            
            # Remove duplicates
            unique_buttons = []
            for button in show_more_buttons:
                if button not in unique_buttons:
                    unique_buttons.append(button)
            
            if unique_buttons:
                logger.info(f"Found {len(unique_buttons)} 'Show more' button(s)")
                
                for i, button in enumerate(unique_buttons):
                    try:
                        if button.is_displayed() and button.is_enabled():
                            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                            time.sleep(random.uniform(0.5, 1.0))
                            
                            try:
                                button.click()
                            except:
                                driver.execute_script("arguments[0].click();", button)
                            
                            time.sleep(random.uniform(1.0, 2.0))
                            logger.info("Successfully clicked 'Show more' button")
                            
                    except Exception as e:
                        logger.warning(f"Failed to click 'Show more' button {i+1}: {e}")
                        continue
                        
        except Exception as e:
            logger.warning(f"Error finding/clicking 'Show more' buttons: {e}")

    def click_show_more_replies_and_scrape_thread(tweet_elem):
        """ENHANCED VERSION 2.0 - Enhanced function that searches AROUND the tweet element"""
        thread_tweets = []
    
        try:
            # **UNIQUE LOG MESSAGE TO CONFIRM THIS VERSION IS RUNNING**
            logger.info("üöÄ ENHANCED VERSION 2.0 IS RUNNING - SEARCHING FOR SHOW MORE REPLIES")
            
            # **STEP 1: Log what we're working with**
            tweet_full_text = tweet_elem.text.strip()
            logger.info(f"üîç Processing tweet: '{tweet_full_text[:50]}...'")
        
            # **STEP 2: Search in MULTIPLE locations around the tweet**
            search_areas = []
        
            # Area 1: The tweet element itself
            search_areas.append(("tweet_element", tweet_elem))
        
            # Area 2: Parent container (most likely location)
            try:
                parent = tweet_elem.find_element(By.XPATH, "..")
                search_areas.append(("parent_container", parent))
                logger.info("‚úÖ Found parent container to search")
            except Exception as e:
                logger.info(f"‚ùå Could not find parent container: {e}")
            
            # Area 3: Following siblings (elements that come after the tweet)
            try:
                parent = tweet_elem.find_element(By.XPATH, "..")
                siblings = parent.find_elements(By.XPATH, ".//following-sibling::*")
                logger.info(f"‚úÖ Found {len(siblings)} sibling elements to search")
                for i, sibling in enumerate(siblings[:5]):  # Check first 5 siblings
                    search_areas.append((f"sibling_{i+1}", sibling))
            except Exception as e:
                logger.info(f"‚ùå Could not find siblings: {e}")
        
            logger.info(f"üîç Total search areas: {len(search_areas)}")
        
            # **STEP 3: Search each area for "Show more replies"**
            for area_name, search_elem in search_areas:
                try:
                    elem_text = search_elem.text.strip().lower()
                    contains_text = "show more replies" in elem_text
                
                    logger.info(f"üîç Searching {area_name}: contains_show_more_replies={contains_text}")
                
                    if contains_text:
                        logger.info(f"üéØ JACKPOT! FOUND 'show more replies' in {area_name}!")
                        logger.info(f"üìù Text preview: '{elem_text[:100]}...'")
                    
                        # Now find the actual clickable element
                        xpath_selectors = [
                            './/a[contains(translate(text(), "ABCDEFGHIJKLMNOPQRSTUVWXYZ", "abcdefghijklmnopqrstuvwxyz"), "show more replies")]',
                            './/span[contains(translate(text(), "ABCDEFGHIJKLMNOPQRSTUVWXYZ", "abcdefghijklmnopqrstuvwxyz"), "show more replies")]',
                            './/div[contains(translate(text(), "ABCDEFGHIJKLMNOPQRSTUVWXYZ", "abcdefghijklmnopqrstuvwxyz"), "show more replies")]',
                            './/*[contains(text(), "Show more replies")]',
                            './/*[contains(text(), "show more replies")]'
                        ]
                    
                        reply_button = None
                    
                        logger.info(f"üîç Searching for clickable button in {area_name}...")
                        for j, xpath in enumerate(xpath_selectors):
                            try:
                                buttons = search_elem.find_elements(By.XPATH, xpath)
                                logger.info(f"   Selector {j+1}: Found {len(buttons)} potential buttons")
                                for button in buttons:
                                    button_text = button.text.strip()
                                    logger.info(f"   Button text: '{button_text}'")
                                    if "show more replies" in button_text.lower():
                                        reply_button = button
                                        logger.info(f"üéØ Found clickable button: '{button.text.strip()}'")
                                        break
                                if reply_button:
                                    break
                            except Exception as e:
                                logger.info(f"   Selector {j+1} failed: {e}")
                                continue
                    
                        if reply_button:
                            logger.info(f"üñ±Ô∏è Attempting to click 'Show more replies' button...")
                        
                            # Simple click attempt for now
                            try:
                                reply_button.click()
                                logger.info("‚úÖ Successfully clicked 'Show more replies'!")
                            
                                # Wait for thread page
                                time.sleep(5)
                                logger.info("‚úÖ Waited 5 seconds for thread page to load")
                            
                                # For now, just return empty list but log success
                                logger.info("üéØ THREAD CLICK SUCCESSFUL - Would scrape thread here")
                                return []
                            
                            except Exception as e:
                                logger.warning(f"‚ùå Failed to click the button: {e}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Found text in {area_name} but no clickable element")
                        
                except Exception as e:
                    logger.error(f"‚ùå Error searching {area_name}: {e}")
                    continue
        
            logger.info("‚ùå 'Show more replies' not found in any search area")
            return thread_tweets
        
        except Exception as e:
            logger.error(f"‚ùå Error in ENHANCED show more replies detection: {e}")
            return thread_tweets
    
    def scrape_thread_posts():
        """Scrape all posts in a thread from the same user"""
        thread_tweets = []
        current_username = None
        
        try:
            # Scroll to top of thread first
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(random.uniform(1.0, 2.0))
            
            # Get the username from the thread
            try:
                username_elem = driver.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"] span')
                current_username = username_elem.text.strip()
                logger.info(f"Thread username: {current_username}")
            except:
                logger.warning("Could not determine thread username")
            
            # Scroll through thread and collect posts
            max_thread_scrolls = 5
            processed_thread_texts = set()
            
            for scroll in range(max_thread_scrolls):
                tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                
                found_other_user = False
                
                for tweet_elem in tweet_elements:
                    try:
                        # Check if this tweet is from the same user
                        try:
                            tweet_username_elem = tweet_elem.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"] span')
                            tweet_username = tweet_username_elem.text.strip()
                        except:
                            tweet_username = ""
                        
                        # If we found a different user's tweet, we've reached the end of the thread
                        if current_username and tweet_username and tweet_username != current_username and not tweet_username.startswith('@'):
                            logger.info(f"Found different user '{tweet_username}' - end of thread reached")
                            found_other_user = True
                            break
                        
                        # Extract tweet text (with show more expansion)
                        click_show_more_buttons(tweet_elem)
                        tweet_text = extract_tweet_text_from_element(tweet_elem)
                        
                        if tweet_text and tweet_text not in processed_thread_texts:
                            processed_thread_texts.add(tweet_text)
                            
                            # Get timestamp
                            try:
                                time_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'time')
                                timestamp = time_elem.get_attribute('datetime')
                                if timestamp:
                                    from datetime import datetime
                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                    readable_timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                                else:
                                    readable_timestamp = "Unknown"
                            except:
                                readable_timestamp = "Unknown"
                            
                            thread_tweets.append({
                                'text': tweet_text,
                                'timestamp': readable_timestamp,
                                'type': 'thread_tweet'
                            })
                            logger.info(f"Added thread tweet: {tweet_text[:50]}...")
                            
                    except Exception as e:
                        logger.error(f"Error processing thread tweet: {e}")
                        continue
                
                if found_other_user:
                    logger.info("Stopping thread scraping - reached other user's content")
                    break
                
                # Scroll down for more thread content
                if scroll < max_thread_scrolls - 1:
                    driver.execute_script("window.scrollBy(0, 800);")
                    time.sleep(random.uniform(1.5, 2.5))
            
            logger.info(f"Collected {len(thread_tweets)} tweets from thread")
            
        except Exception as e:
            logger.error(f"Error scraping thread: {e}")
        
        return thread_tweets

    def extract_tweet_text_from_element(tweet_elem):
        """Extract tweet text from element using existing methods"""
        tweet_text = ""
        
        selectors_to_try = [
            '[data-testid="tweetText"]',
            '[dir="auto"]',
            '[lang]',
            'span[dir="auto"]'
        ]
        
        for selector in selectors_to_try:
            try:
                text_elem = tweet_elem.find_element(By.CSS_SELECTOR, selector)
                tweet_text = text_elem.text.strip()
                if tweet_text and len(tweet_text) >= 1:
                    break
            except:
                continue
        
        if not tweet_text:
            full_text = tweet_elem.text.strip()
            lines = full_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if (line and 
                    len(line) >= 1 and 
                    not line.startswith('@') and 
                    not line.startswith('¬∑') and
                    not line.endswith('ago') and
                    not line.isdigit() and
                    line not in ['h', 'm', 's', 'Pinned', 'pinned']):
                    tweet_text = line
                    break
        
        return tweet_text
    
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    
    driver = None
    new_tweets = []

    
    # SAFETY CHECK
    try:
        # Use the same session management as Facebook
        driver = get_twitter_driver()

        try:
            # Test if the current window/session is still valid
            current_url = driver.current_url
            logger.info(f"Current browser URL: {current_url}")
        except:
            logger.info("Browser window was closed - reopening Twitter session")
            # Force a completely new driver session
            try:
                driver.quit()
            except:
                pass
            driver = None
    
            # Create fresh driver
            driver = get_twitter_driver()
            
    # **END OF IMPROVED SAFETY CHECK SNIPPET**

        
        
        logger.info(f"Starting INCREMENTAL Twitter scrape for: {twitter_url}")
        
        # Navigate to the profile (driver is already logged in)
        driver.get(twitter_url)

        # Check if we're actually logged in
        time.sleep(random.uniform(2, 4))  # Wait for page load
        
        # Check for login indicators
        page_source = driver.page_source.lower()
        current_url = driver.current_url.lower()
        
        if ('sign in' in page_source or 'log in' in page_source or 
            'login' in current_url or '/i/flow/login' in current_url or
            'authentication' in page_source):
            logger.warning("Twitter login required - not logged in")
            try:
                show_browser_login_confirmation('Twitter')
                logger.info("User confirmed Twitter login - continuing")
                time.sleep(3)  # Give time for login to complete
            except Exception as e:
                logger.info(f"Twitter login cancelled: {e}")
                return []
        
        logger.info(f"Navigated to: {twitter_url}")
        time.sleep(random.uniform(3, 6))
        
        # Parse existing tweets
        existing_tweet_texts = set()
        if existing_tweets_text:
            tweet_sections = existing_tweets_text.split('Tweet ')
            for section in tweet_sections:
                if section.strip():
                    lines = section.strip().split('\n')
                    if len(lines) > 1:
                        tweet_content = '\n'.join(lines[1:]).strip()
                        if tweet_content:
                            existing_tweet_texts.add(tweet_content.lower())
        
        logger.info(f"Found {len(existing_tweet_texts)} existing tweets to check against")
        
        processed_texts = set()
        consecutive_existing = 0
        
        for scroll_attempt in range(3):
            logger.info(f"Scroll attempt {scroll_attempt + 1}")
            
            tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
            logger.info(f"Found {len(tweet_elements)} tweet elements")
            
            for i, tweet_elem in enumerate(tweet_elements):
                clear_memory_every_n_posts(10)
                
                try:
                    # **NEW: Click 'Show more' buttons first to expand content**
                    click_show_more_buttons(tweet_elem)
                    
                    # Extract tweet text first
                    tweet_text = ""
                    
                    selectors_to_try = [
                        '[data-testid="tweetText"]',
                        '[dir="auto"]',
                        '[lang]',
                        'span[dir="auto"]'
                    ]
                    
                    for selector in selectors_to_try:
                        try:
                            text_elem = tweet_elem.find_element(By.CSS_SELECTOR, selector)
                            tweet_text = text_elem.text.strip()
                            if tweet_text and len(tweet_text) >= 1:
                                break
                        except:
                            continue
                    
                    if not tweet_text:
                        full_text = tweet_elem.text.strip()
                        lines = full_text.split('\n')
                        
                        for line in lines:
                            line = line.strip()
                            if (line and 
                                len(line) >= 1 and 
                                not line.startswith('@') and 
                                not line.startswith('¬∑') and
                                not line.endswith('ago') and
                                not line.isdigit() and
                                line not in ['h', 'm', 's', 'Pinned', 'pinned']):
                                tweet_text = line
                                break
                    
                    if not tweet_text or len(tweet_text) < 1:
                        continue
                    
                    if tweet_text in processed_texts:
                        continue
                    processed_texts.add(tweet_text)
                    
                    # **PINNED TWEET LOGIC GOES HERE**
                    is_pinned = False
                    try:
                        tweet_html = tweet_elem.get_attribute('innerHTML')
                        full_text = tweet_elem.text
                        
                        if ('Pinned' in tweet_html or 'pinned' in full_text or 'üìå' in full_text):
                            is_pinned = True
                    except:
                        pass
                    
                    if is_pinned:
                        logger.info(f"üìå Found pinned tweet: {tweet_text[:50]}...")
                        
                        # Add pinned tweet if it's new
                        if tweet_text.lower() not in existing_tweet_texts:
                            # Get timestamp
                            try:
                                time_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'time')
                                timestamp = time_elem.get_attribute('datetime')
                                if timestamp:
                                    from datetime import datetime
                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                    readable_timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                                else:
                                    readable_timestamp = "Unknown"
                            except:
                                readable_timestamp = "Unknown"
                            
                            new_tweets.append({
                                'text': tweet_text,
                                'timestamp': readable_timestamp,
                                'type': 'tweet_pinned'
                            })
                            logger.info(f"‚òÖ NEW pinned tweet added")
                        
                        # **DON'T affect consecutive_existing counter**
                        continue  # Skip to next tweet without affecting logic
                    
                    # **REGULAR TWEET PROCESSING CONTINUES HERE**
                    logger.info(f"Processing tweet {i+1}: '{tweet_text[:50]}...'")
                    
                    if tweet_text.lower() in existing_tweet_texts:
                        logger.info(f"‚úì EXISTING tweet found: {tweet_text[:50]}...")
                        consecutive_existing += 1
                        
                        if consecutive_existing >= 2:
                            logger.info("Found 2 consecutive existing tweets - stopping")
                            return new_tweets
                    else:
                        logger.info(f"‚òÖ NEW tweet found: {tweet_text[:50]}...")
                        consecutive_existing = 0
                        
                        # Get timestamp
                        try:
                            time_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'time')
                            timestamp = time_elem.get_attribute('datetime')
                            if timestamp:
                                from datetime import datetime
                                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                readable_timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                            else:
                                readable_timestamp = "Unknown"
                        except:
                            readable_timestamp = "Unknown"
                        
                        new_tweets.append({
                            'text': tweet_text,
                            'timestamp': readable_timestamp,
                            'type': 'tweet'
                        })
                        
                        # **NEW: Check for 'Show more replies' and scrape thread**
                        thread_tweets = click_show_more_replies_and_scrape_thread(tweet_elem)
                        if thread_tweets:
                            logger.info(f"Adding {len(thread_tweets)} thread tweets")
                            new_tweets.extend(thread_tweets)
                
                except Exception as e:
                    logger.error(f"Error processing tweet {i}: {e}")
                    continue
            
            if consecutive_existing < 2 and scroll_attempt < 2:
                driver.execute_script("window.scrollBy(0, 500);")
                time.sleep(2)
        
        logger.info(f"Completed: Found {len(new_tweets)} new tweets")
        return new_tweets
        
    except Exception as e:
        logger.error(f"Error in Twitter scrape: {e}")
        return []
        
    finally:
        # DON'T close the driver - let it stay open like Facebook
        driver = None


def scrape_twitter_profile(twitter_url):
    """Scrape Twitter profile for all visible tweets using Selenium with human-like scrolling"""
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    import time
    import random
    
    def click_show_more_buttons(tweet_elem):
        """Click all 'Show more' buttons within a tweet element to expand content"""
        try:
            xpath_selectors = [
                './/span[contains(text(), "Show more")]',
                './/div[contains(text(), "Show more")]',
                './/span[contains(text(), "show more")]',
                './/div[contains(text(), "show more")]'
            ]
            
            show_more_buttons = []
            for xpath in xpath_selectors:
                try:
                    buttons = tweet_elem.find_elements(By.XPATH, xpath)
                    show_more_buttons.extend(buttons)
                except:
                    continue
            
            # Remove duplicates
            unique_buttons = []
            for button in show_more_buttons:
                if button not in unique_buttons:
                    unique_buttons.append(button)
            
            if unique_buttons:
                logger.info(f"Found {len(unique_buttons)} 'Show more' button(s)")
                
                for i, button in enumerate(unique_buttons):
                    try:
                        if button.is_displayed() and button.is_enabled():
                            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                            time.sleep(random.uniform(0.5, 1.0))
                            
                            try:
                                button.click()
                            except:
                                driver.execute_script("arguments[0].click();", button)
                            
                            time.sleep(random.uniform(1.0, 2.0))
                            logger.info("Successfully clicked 'Show more' button")
                            
                    except Exception as e:
                        logger.warning(f"Failed to click 'Show more' button {i+1}: {e}")
                        continue
                        
        except Exception as e:
            logger.warning(f"Error finding/clicking 'Show more' buttons: {e}")

    # Add this at the top of your file with other global variables
    learned_show_more_replies_selectors = []
    show_more_replies_learning_active = True

    def click_show_more_replies_and_scrape_thread(tweet_elem):
        """Simple search for 'Show more replies' and simple back arrow click"""
        thread_tweets = []
    
        try:
            logger.info("üöÄ SIMPLE SEARCH FOR 'SHOW MORE REPLIES'")
        
            # Search for "Show more replies" (already working)
            try:
                elements = driver.find_elements(By.XPATH, "//*[contains(text(), 'Show more replies')]")
                logger.info(f"Found {len(elements)} elements with 'Show more replies' text")
                
                for element in elements:
                    element_text = element.text.strip()
                    logger.info(f"Element text: '{element_text}'")
                
                    if "show more replies" in element_text.lower():
                        logger.info("üéØ FOUND 'Show more replies'! Clicking...")
                        element.click()
                        logger.info("‚úÖ Clicked successfully!")
                        time.sleep(3)
                    
                        # Scrape the thread
                        thread_tweets = scrape_thread_posts()
                    
                        # Simple back arrow click - look for the arrow button
                        logger.info("üîô Looking for back arrow button...")
                        try:
                            # Find any clickable element with back arrow (‚Üê) or "Back" text
                            back_elements = driver.find_elements(By.XPATH, "//*[contains(@aria-label, 'Back') or contains(text(), '‚Üê')]")
                            logger.info(f"Found {len(back_elements)} potential back elements")
                        
                            if back_elements:
                                back_elements[0].click()
                                logger.info("‚úÖ Back arrow clicked successfully!")
                                time.sleep(2)
                            else:
                                logger.warning("‚ùå No back arrow found, using browser back")
                                driver.back()
                                time.sleep(2)
                            
                        except Exception as e:
                            logger.warning(f"‚ùå Back arrow click failed: {e}, using browser back")
                            driver.back()
                            time.sleep(2)
                    
                        return thread_tweets
            
                logger.info("‚ùå No 'Show more replies' found")
            
            except Exception as e:
                logger.error(f"Error: {e}")
            
        except Exception as e:
            logger.error(f"Error in show more replies: {e}")
    
        return thread_tweets

    def attempt_click_button(button, button_id):
        """Helper function to attempt clicking a button with multiple strategies"""
        try:
            logger.info(f"üñ±Ô∏è Attempting to click {button_id}: {button.tag_name} with text '{button.text.strip()}'")
            
            # Check if clickable
            if not button.is_displayed():
                logger.info(f"{button_id} not displayed")
                return False
        
            # Scroll into view
            driver.execute_script("arguments[0].scrollIntoView({block: 'center', inline: 'center'});", button)
            time.sleep(1)
        
            # Record current URL to detect navigation
            current_url = driver.current_url
        
            # Try multiple click methods
            click_methods = [
                ("Regular click", lambda: button.click()),
                ("JavaScript click", lambda: driver.execute_script("arguments[0].click();", button)),
                ("Action chain", lambda: ActionChains(driver).move_to_element(button).click().perform()),
            ]
        
            for method_name, click_func in click_methods:
                try:
                    logger.info(f"   Trying {method_name}...")
                    click_func()
                
                    # Wait and check for navigation
                    time.sleep(3)
                    new_url = driver.current_url
                
                    if new_url != current_url:
                        logger.info(f"‚úÖ {method_name} successful! Navigated from {current_url} to {new_url}")
                        return True
                    else:
                        logger.info(f"‚ö†Ô∏è {method_name} executed but no navigation detected")
                except Exception as e:
                    logger.info(f"‚ùå {method_name} failed: {e}")
                    continue
        
            return False
        
        except Exception as e:
            logger.error(f"Error in attempt_click_button for {button_id}: {e}")
            return False

    def navigate_back_to_profile():
        """Helper function to navigate back to profile"""
        logger.info("üîô Navigating back to profile...")
    
        back_selectors = [
            '[data-testid="app-bar-back"]',
            '[aria-label="Back"]',
            'button[aria-label="Back"]',
            'div[role="button"][aria-label="Back"]'
        ]
    
        for selector in back_selectors:
            try:
                back_button = driver.find_element(By.CSS_SELECTOR, selector)
                if back_button and back_button.is_displayed():
                    back_button.click()
                    time.sleep(3)
                    logger.info("‚úÖ Successfully navigated back")
                    return
            except:
                continue
    
        # Fallback to browser back
        logger.info("Using browser back...")
        driver.back()
        time.sleep(3)

    def scrape_thread_posts():
        """Scrape all posts in a thread from the same user"""
        thread_tweets = []
        current_username = None
        
        try:
            # Scroll to top of thread first
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(random.uniform(1.0, 2.0))
            
            # Get the username from the thread
            try:
                username_elem = driver.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"] span')
                current_username = username_elem.text.strip()
                logger.info(f"Thread username: {current_username}")
            except:
                logger.warning("Could not determine thread username")
            
            # Scroll through thread and collect posts
            max_thread_scrolls = 5
            processed_thread_texts = set()
            
            for scroll in range(max_thread_scrolls):
                tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                
                found_other_user = False
                
                for tweet_elem in tweet_elements:
                    try:
                        # Check if this tweet is from the same user
                        try:
                            tweet_username_elem = tweet_elem.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"] span')
                            tweet_username = tweet_username_elem.text.strip()
                        except:
                            tweet_username = ""
                        
                        # If we found a different user's tweet, we've reached the end of the thread
                        if current_username and tweet_username and tweet_username != current_username and not tweet_username.startswith('@'):
                            logger.info(f"Found different user '{tweet_username}' - end of thread reached")
                            found_other_user = True
                            break
                        
                        # Extract tweet text (with show more expansion)
                        click_show_more_buttons(tweet_elem)
                        tweet_text = extract_tweet_text_from_element(tweet_elem)
                        
                        if tweet_text and tweet_text not in processed_thread_texts:
                            processed_thread_texts.add(tweet_text)
                            
                            # Get timestamp
                            try:
                                time_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'time')
                                timestamp = time_elem.get_attribute('datetime')
                                if timestamp:
                                    from datetime import datetime
                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                    readable_timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                                else:
                                    readable_timestamp = "Unknown"
                            except:
                                readable_timestamp = "Unknown"
                            
                            thread_tweets.append({
                                'text': tweet_text,
                                'timestamp': readable_timestamp,
                                'type': 'thread_tweet'
                            })
                            logger.info(f"Added thread tweet: {tweet_text[:50]}...")
                            
                    except Exception as e:
                        logger.error(f"Error processing thread tweet: {e}")
                        continue
                
                if found_other_user:
                    logger.info("Stopping thread scraping - reached other user's content")
                    break
                
                # Scroll down for more thread content
                if scroll < max_thread_scrolls - 1:
                    driver.execute_script("window.scrollBy(0, 800);")
                    time.sleep(random.uniform(1.5, 2.5))
            
            logger.info(f"Collected {len(thread_tweets)} tweets from thread")
            
        except Exception as e:
            logger.error(f"Error scraping thread: {e}")
        
        return thread_tweets

    def extract_tweet_text_from_element(tweet_elem):
        """Extract tweet text from element using existing methods"""
        tweet_text = ""
        
        selectors_to_try = [
            '[data-testid="tweetText"]',
            '[dir="auto"]',
            '[lang]',
            'span[dir="auto"]'
        ]
        
        for selector in selectors_to_try:
            try:
                text_elem = tweet_elem.find_element(By.CSS_SELECTOR, selector)
                tweet_text = text_elem.text.strip()
                if tweet_text and len(tweet_text) >= 1:
                    break
            except:
                continue
        
        if not tweet_text:
            full_text = tweet_elem.text.strip()
            lines = full_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if (line and 
                    len(line) >= 1 and 
                    not line.startswith('@') and 
                    not line.startswith('¬∑') and
                    not line.endswith('ago') and
                    not line.isdigit() and
                    line not in ['h', 'm', 's', 'Pinned', 'pinned']):
                    tweet_text = line
                    break
        
        return tweet_text
    
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    
    driver = None
    tweets = []
    
    try:
        # Use the same session management as Facebook
        driver = get_twitter_driver()
        
        logger.info(f"Starting Twitter scrape for: {twitter_url}")
        
        # Navigate to the profile (driver is already logged in)
        driver.get(twitter_url)
        time.sleep(random.uniform(3, 6))
        
        # **HUMAN-LIKE SCROLLING APPROACH**
        max_scrolls = 20       # Increased max attempts
        no_new_content_count = 0
        max_no_new_content = 4  # Allow more attempts before giving up
        
        last_tweet_count = 0
        reading_break_counter = 0
        suspected_bottom_attempts = 0  # Track how many times we've suspected bottom
        
        for scroll_attempt in range(max_scrolls):
            logger.info(f"Human-like scroll attempt {scroll_attempt + 1}/{max_scrolls}")
            
            # **HUMAN BEHAVIOR: Sometimes pause to "read" content**
            if scroll_attempt > 0 and random.random() < 0.3:  # 30% chance
                reading_pause = random.uniform(2, 8)
                logger.info(f"Taking a 'reading break' for {reading_pause:.1f} seconds...")
                time.sleep(reading_pause)
            
            # Extract tweets at current position
            try:
                tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                logger.info(f"Found {len(tweet_elements)} tweet elements on page")
                
                # Process new tweets (avoid duplicates)
                existing_tweet_texts = {tweet['text'] for tweet in tweets}
                new_tweets_this_scroll = 0
                
                for tweet_elem in tweet_elements:
                    try:
                        # **NEW: Click 'Show more' buttons first**
                        click_show_more_buttons(tweet_elem)
                        
                        # **DEBUG: Log all text content for each tweet element**
                        full_element_text = tweet_elem.text.strip()
                        logger.info(f"=== TWEET ELEMENT DEBUG ===")
                        logger.info(f"Full element text: '{full_element_text}'")
        
                        # **IMPROVED TWEET TEXT EXTRACTION**
                        tweet_text = ""
        
                        # Method 1: Direct tweetText selector
                        try:
                            tweet_text_elem = tweet_elem.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
                            tweet_text = tweet_text_elem.text.strip()
                            logger.info(f"Method 1 (tweetText): '{tweet_text}'")
                        except Exception as e:
                            logger.info(f"Method 1 failed: {e}")
        
                        # Method 2: Dir auto selector (better for emoji)
                        if not tweet_text:
                            try:
                                tweet_text_elem = tweet_elem.find_element(By.CSS_SELECTOR, '[dir="auto"]')
                                tweet_text = tweet_text_elem.text.strip()
                                logger.info(f"Method 2 (dir-auto): '{tweet_text}'")
                            except Exception as e:
                                logger.info(f"Method 2 failed: {e}")
        
                        # Method 3: All span elements
                        if not tweet_text:
                            try:
                                span_elements = tweet_elem.find_elements(By.CSS_SELECTOR, 'span')
                                for span in span_elements:
                                    span_text = span.text.strip()
                                    if span_text and len(span_text) >= 1 and "ago" not in span_text and span_text not in ['h', 'm', 's']:
                                        tweet_text = span_text
                                        logger.info(f"Method 3 (span-search): '{tweet_text}'")
                                        break
                            except Exception as e:
                                logger.info(f"Method 3 failed: {e}")
        
                        # Method 4: Fallback parsing
                        if not tweet_text:
                            lines = full_element_text.split('\n')
                            for line in lines:
                                line = line.strip()
                                if (line and 
                                    len(line) >= 1 and  # Lowered for emoji tweets
                                    not line.startswith('@') and 
                                    not line.startswith('¬∑') and
                                    not line.endswith('ago') and
                                    not line.isdigit() and
                                    line not in ['h', 'm', 's', 'Pinned', 'pinned']):
                                    tweet_text = line
                                    logger.info(f"Method 4 (fallback): '{tweet_text}'")
                                    break
        
                        logger.info(f"Final extracted text: '{tweet_text}'")                        

                        # Extract timestamp
                        try:
                            time_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'time')
                            timestamp = time_elem.get_attribute('datetime')
                            if timestamp:
                                from datetime import datetime
                                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                readable_timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                            else:
                                readable_timestamp = "Unknown"
                        except:
                            readable_timestamp = "Unknown"
                        
                        if (tweet_text and 
                            len(tweet_text) >= 1 and 
                            tweet_text not in existing_tweet_texts):
                            
                            tweets.append({
                                'text': tweet_text,
                                'timestamp': readable_timestamp,
                                'type': 'tweet'
                            })
                            existing_tweet_texts.add(tweet_text)
                            new_tweets_this_scroll += 1
                            logger.info(f"Extracted new tweet: {tweet_text[:50]}...")
                            
                            # **NEW: Check for 'Show more replies' and scrape thread**
                            thread_tweets = click_show_more_replies_and_scrape_thread(tweet_elem)
                            if thread_tweets:
                                logger.info(f"Adding {len(thread_tweets)} thread tweets")
                                for thread_tweet in thread_tweets:
                                    if thread_tweet['text'] not in existing_tweet_texts:
                                        tweets.append(thread_tweet)
                                        existing_tweet_texts.add(thread_tweet['text'])
                                        new_tweets_this_scroll += 1
                            
                    except Exception as e:
                        logger.error(f"Error extracting individual tweet: {e}")
                        continue
                
                logger.info(f"Added {new_tweets_this_scroll} new tweets this scroll (total: {len(tweets)})")
                
                # Check if we found new content
                if new_tweets_this_scroll == 0:
                    no_new_content_count += 1
                    logger.info(f"No new content found. Count: {no_new_content_count}/{max_no_new_content}")
                else:
                    no_new_content_count = 0
                
                if no_new_content_count >= max_no_new_content:
                    logger.info(f"Stopping: No new content found for {max_no_new_content} consecutive scrolls")
                    break
                        
            except Exception as e:
                logger.error(f"Error scraping tweets at scroll position {scroll_attempt}: {e}")
            
            # **HUMAN-LIKE SCROLLING PATTERNS** (rest of your existing scrolling logic)
            previous_position = driver.execute_script("return window.pageYOffset;")
            
            # Choose random scrolling pattern
            scroll_pattern = random.choice(['small', 'medium', 'variable', 'burst'])
            
            if scroll_pattern == 'small':
                # Small, careful scrolls
                scroll_distance = random.randint(300, 800)
                driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                pause_time = random.uniform(2, 4)
                
            elif scroll_pattern == 'medium':
                # Medium scrolls
                scroll_distance = random.randint(800, 1500)
                driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                pause_time = random.uniform(1.5, 3.5)
                
            elif scroll_pattern == 'variable':
                # Variable speed - multiple small scrolls with micro-pauses
                total_scroll = random.randint(600, 1200)
                num_mini_scrolls = random.randint(2, 4)
                scroll_per_mini = total_scroll // num_mini_scrolls
                
                for mini_scroll in range(num_mini_scrolls):
                    driver.execute_script(f"window.scrollBy(0, {scroll_per_mini});")
                    time.sleep(random.uniform(0.2, 0.6))  # Micro-pause between mini scrolls
                
                pause_time = random.uniform(2, 4)
                
            elif scroll_pattern == 'burst':
                # Quick burst followed by longer pause (like someone scrolling fast then stopping to read)
                scroll_distance = random.randint(1200, 2000)
                driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                pause_time = random.uniform(3, 7)  # Longer pause after burst
            
            logger.info(f"Used '{scroll_pattern}' scroll pattern, pausing for {pause_time:.1f}s")
            time.sleep(pause_time)
            
            # **HUMAN BEHAVIOR: Occasional scroll back up (like checking something)**
            if random.random() < 0.15:  # 15% chance
                scroll_back = random.randint(100, 400)
                logger.info(f"Human behavior: Scrolling back up {scroll_back}px to 'recheck' something")
                driver.execute_script(f"window.scrollBy(0, -{scroll_back});")
                time.sleep(random.uniform(1, 3))
                # Then scroll back down
                driver.execute_script(f"window.scrollBy(0, {scroll_back + random.randint(50, 200)});")
                time.sleep(random.uniform(1, 2))
            
            # **HUMAN BEHAVIOR: Longer break every so often**
            reading_break_counter += 1
            if reading_break_counter >= random.randint(5, 8):  # Every 5-8 scrolls
                long_break = random.uniform(5, 12)
                logger.info(f"Taking a longer 'reading/thinking' break for {long_break:.1f} seconds...")
                time.sleep(long_break)
                reading_break_counter = 0
            
            # **NEW: Check if we actually scrolled (suspected bottom detection)**
            new_position = driver.execute_script("return window.pageYOffset;")
            if new_position == previous_position:
                suspected_bottom_attempts += 1
                logger.info(f"üîç SUSPECTED BOTTOM REACHED (attempt {suspected_bottom_attempts}) - Triggering content loading strategy...")
                
                # **MULTI-SCROLL UP THEN BOTTOM STRATEGY**
                original_position = new_position
                
                # Phase 1: Multiple scrolls up with pauses to trigger lazy loading
                logger.info("üìà Phase 1: Scrolling up multiple times to trigger content loading...")
                scroll_up_attempts = random.randint(4, 7)  # 4-7 upward scrolls
                
                for up_attempt in range(scroll_up_attempts):
                    scroll_up_distance = random.randint(800, 1500)
                    driver.execute_script(f"window.scrollBy(0, -{scroll_up_distance});")
                    
                    # Random pause between upward scrolls
                    up_pause = random.uniform(1.5, 3.5)
                    logger.info(f"   Scroll up #{up_attempt + 1}: {scroll_up_distance}px, pausing {up_pause:.1f}s")
                    time.sleep(up_pause)
                    
                    # Occasionally pause longer (human behavior)
                    if random.random() < 0.3:
                        extra_pause = random.uniform(2, 5)
                        logger.info(f"   Extra pause: {extra_pause:.1f}s")
                        time.sleep(extra_pause)
                
                # Phase 2: Extended pause to let Twitter's lazy loading system work
                major_pause = random.uniform(8, 15)
                logger.info(f"‚è≥ Phase 2: Extended pause for {major_pause:.1f}s to let Twitter load more content...")
                time.sleep(major_pause)
                
                # Phase 3: Scroll back to bottom (possibly with new content)
                logger.info("üìâ Phase 3: Scrolling back to bottom to check for new content...")
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
                # Give it time to load new content
                bottom_load_pause = random.uniform(5, 8)
                logger.info(f"‚è≥ Waiting {bottom_load_pause:.1f}s for new content to load at bottom...")
                time.sleep(bottom_load_pause)
                
                # Check if new content appeared
                post_strategy_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                if len(post_strategy_elements) > len(tweet_elements):
                    logger.info(f"‚úÖ SUCCESS! Found {len(post_strategy_elements) - len(tweet_elements)} more tweet elements after strategy")
                    suspected_bottom_attempts = 0  # Reset counter since we found new content
                    continue  # Continue with the main loop
                else:
                    logger.info("‚ùå No new content found after up-scroll strategy")
                
                # If we've tried the strategy multiple times without success, we're probably done
                if suspected_bottom_attempts >= 2:
                    logger.info(f"üèÅ Confirmed bottom reached after {suspected_bottom_attempts} attempts with loading strategy")
                    break
                else:
                    logger.info(f"üîÑ Will try the loading strategy again if needed (attempt {suspected_bottom_attempts}/2)")
            else:
                # We successfully scrolled, reset the suspected bottom counter
                suspected_bottom_attempts = 0
            
            current_position = new_position
            logger.info(f"Current scroll position: {current_position}px")
            
            # **HUMAN BEHAVIOR: Random micro-movements (like adjusting position)**
            if random.random() < 0.1:  # 10% chance
                micro_adjustment = random.randint(-50, 100)
                driver.execute_script(f"window.scrollBy(0, {micro_adjustment});")
                time.sleep(random.uniform(0.5, 1.5))
        
        logger.info(f"Completed human-like Twitter scrape: {len(tweets)} total tweets collected")
        return tweets
        
    except Exception as e:
        logger.error(f"Error scraping Twitter profile: {e}")
        return []
        
    finally:
        # DON'T close the driver - let it stay open like Facebook
        driver = None


# Add this after your Facebook scraping functions

def scrape_linkedin_new_posts_only(linkedin_url, existing_posts_text):
    """Scrape LinkedIn profile but STOP when we encounter posts we already have"""
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import time
    import random
    import re
    
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    
    driver = None
    new_posts = []
    
    try:
        # Use the same session management as Facebook/Twitter with enhanced checking
        driver = get_linkedin_driver()
        
        # Enhanced browser session validation
        session_valid = False
        for attempt in range(2):  # Try twice
            try:
                current_url = driver.current_url
                # Test if we can actually interact with the page
                driver.execute_script("return document.readyState")
                logger.info(f"Current browser URL: {current_url}")
                session_valid = True
                break
            except Exception as e:
                logger.info(f"Browser session invalid (attempt {attempt + 1}): {e}")
                if attempt == 0:  # First attempt failed, create new session
                    try:
                        driver.quit()
                    except:
                        pass
                    driver = get_linkedin_driver()
                else:  # Second attempt failed, give up
                    logger.error("Could not establish valid browser session")
                    return []
        
        if not session_valid:
            logger.error("Failed to validate browser session")
            return []
        
        logger.info(f"Starting INCREMENTAL LinkedIn scrape for: {linkedin_url}")        
        # Navigate to the profile first
        driver.get(linkedin_url)
        logger.info(f"Navigated to: {linkedin_url}")
        time.sleep(random.uniform(3, 6))

        # Check if we're actually logged in
        time.sleep(random.uniform(1, 2))
        
        # Check for login indicators
        page_source = driver.page_source.lower()
        current_url = driver.current_url.lower()
        
        if ('sign in' in page_source or 'sign up' in page_source or 
            'login' in current_url or '/login' in current_url or
            'authentication' in page_source or 'authwall' in current_url):
            logger.warning("LinkedIn login required - not logged in")
            try:
                from __main__ import show_browser_login_confirmation
                show_browser_login_confirmation('LinkedIn')
                logger.info("User confirmed LinkedIn login - continuing")
                time.sleep(3)
            except Exception as e:
                logger.info(f"LinkedIn login cancelled: {e}")
                return []

        # **CRITICAL: Make sure we're on the POSTS feed, not just the profile**
        # Try to navigate to the posts feed specifically
        posts_url = linkedin_url.rstrip('/') + '/recent-activity/all/'
        logger.info(f"Navigating to posts feed: {posts_url}")
        try:
            driver.get(posts_url)
            time.sleep(random.uniform(5, 8))
        except:
            logger.warning("Could not navigate to posts feed, trying alternative method")
            
            # **CRITICAL: Click "Show all posts" to access the posts feed**
            logger.info("Looking for 'Show all posts' button...")
            show_all_posts_clicked = False
            
            # Try the same selectors as the full scraper
            show_posts_selectors = [
                'a[href*="/recent-activity/all/"]',
                'a[data-control-name="recent_activity_posts_see_all"]',
                'button[data-control-name="recent_activity_posts_see_all"]',
            ]
            
            for selector in show_posts_selectors:
                try:
                    show_posts_button = driver.find_element(By.CSS_SELECTOR, selector)
                    if show_posts_button and show_posts_button.is_displayed():
                        logger.info(f"Found 'Show all posts' button")
                        driver.execute_script("arguments[0].scrollIntoView(true);", show_posts_button)
                        time.sleep(2)
                        driver.execute_script("arguments[0].click();", show_posts_button)
                        logger.info("Successfully clicked 'Show all posts' button")
                        show_all_posts_clicked = True
                        time.sleep(random.uniform(5, 8))
                        break
                except:
                    continue
            
            if not show_all_posts_clicked:
                logger.warning("Could not find 'Show all posts' button - you may need to click it manually")
                from app import request_login_confirmation
                request_login_confirmation("LinkedIn", "Please click 'Show all posts' button in LinkedIn browser")
        
        # Parse existing posts - IMPROVED COMPARISON (clean both sides)
        existing_post_texts = set()
        existing_post_ids = set()

        if existing_posts_text:
            post_sections = existing_posts_text.split('Post ')
            for section in post_sections:
                if section.strip():
                    lines = section.strip().split('\n')
                    if len(lines) > 1:
                        # Extract post ID and content
                        post_id = lines[0].strip()
                        post_content = '\n'.join(lines[1:]).strip()
                
                        if post_content:
                            # APPLY THE SAME CLEANING TO DATABASE CONTENT
                            cleaned_db_content = post_content
                    
                            # Apply the same cleaning logic as the scraper
                            cleaned_db_content = re.sub(r'^Feed post number \d+\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'Riley Giauque, CFP¬Æ\s*Riley Giauque, CFP¬Æ\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'Riley Giauque, CFP¬Æ\s*‚Ä¢\s*You\s*You\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'Personal Financial Consultant\s*Personal Financial Consultant\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'\d+m\s*‚Ä¢\s*\d+\s+minutes?\s+ago\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'\d+(mo|hr|min|d|w|yr)\s*‚Ä¢.*?‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'now\s*‚Ä¢\s*Just now\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'\d+yr\s*‚Ä¢\s*\d+\s+years?\s+ago\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'\d+\s+(impressions?|views?|likes?|comments?)\s*View analytics.*?$', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'View analytics.*?$', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'Auto captions have been added to your video.*?$', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'Edit captions.*?$', '', cleaned_db_content)
                            cleaned_db_content = re.sub(r'\s+', ' ', cleaned_db_content).strip()
                    
                            # Add both original and cleaned versions
                            existing_post_texts.add(post_content.lower())
                            existing_post_texts.add(cleaned_db_content.lower())
                            existing_post_ids.add(post_id.lower())
                    
                            # Add key phrases for better matching
                            if "market update" in cleaned_db_content.lower():
                                date_match = re.search(r'market update\s+([0-9./]+)', cleaned_db_content.lower())
                                if date_match:
                                    existing_post_texts.add(f"market update {date_match.group(1)}")
                    
                            if "stock market report" in cleaned_db_content.lower():
                                date_match = re.search(r'stock market report.*?([0-9./]+)', cleaned_db_content.lower())
                                if date_match:
                                    existing_post_texts.add(f"stock market report {date_match.group(1)}")
                    
                            if "stock market review" in cleaned_db_content.lower():
                                date_match = re.search(r'stock market review\s+([0-9./]+)', cleaned_db_content.lower())
                                if date_match:
                                    existing_post_texts.add(f"stock market review {date_match.group(1)}")

        logger.info(f"Found {len(existing_post_texts)} existing post texts and {len(existing_post_ids)} existing post IDs")        
        processed_texts = set()
        consecutive_existing = 0
        
        for scroll_attempt in range(3):
            logger.info(f"Scroll attempt {scroll_attempt + 1}")
            
            # LinkedIn post selectors - IMPROVED AND EXPANDED
            post_elements = []
            
            # Try multiple selector strategies
            selector_strategies = [
                # Strategy 1: Main feed posts
                '.feed-shared-update-v2',
                '[data-urn*="activity"]',
                
                # Strategy 2: Activity feed posts  
                '.profile-creator-shared-feed-update__container',
                '.artdeco-card',
                
                # Strategy 3: Recent activity posts
                '[data-test-id="main-feed-activity-card"]',
                '.activity-card',
                
                # Strategy 4: Generic post containers
                '.scaffold-finite-scroll__content [data-urn*="activity"]',
                '.feed-container-theme [data-urn*="activity"]',
                'main .feed-shared-update-v2',
                '.core-rail .feed-shared-update-v2',
                
                # Strategy 5: Very broad fallback
                '[data-view-name="profile-activity-posts"] [data-urn*="activity"]'
            ]
            
            for strategy in selector_strategies:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, strategy)
                    if elements:
                        post_elements.extend(elements)
                        logger.info(f"Found {len(elements)} elements with strategy: {strategy}")
                        break  # Use first successful strategy
                except:
                    continue
            
            logger.info(f"Found {len(post_elements)} post elements")
            
            # Remove duplicates by location
            unique_posts = []
            seen_locations = set()
            for post in post_elements:
                try:
                    location = (post.location['x'], post.location['y'])
                    if location not in seen_locations:
                        unique_posts.append(post)
                        seen_locations.add(location)
                except:
                    continue
            
            logger.info(f"Found {len(unique_posts)} unique post elements")
            
            for i, post_elem in enumerate(unique_posts, 1):
                try:
                    # **EXPAND "SEE MORE" CONTENT FIRST**
                    expand_linkedin_see_more(driver, post_elem)
                    time.sleep(0.5)

                    # Get all text content for debugging
                    all_text = post_elem.text.strip()
                    logger.info(f"=== POST {i} DEBUG ===")
                    logger.info(f"Full text length: {len(all_text)}")
                    logger.info(f"Text preview: {all_text[:200]}...")
                    
                    # Check if this contains your actual post content
                    if "Market update" in all_text or "7/17" in all_text:
                        logger.info("*** FOUND YOUR ACTUAL POST! ***")
                    
                    # **COMPLETE LINKEDIN POST EXTRACTION**
                    post_text = ""

                    # Method 1: Try to get the COMPLETE post content (all paragraphs combined)
                    try:
                        # First try getting all text content elements within the post
                        text_elements = post_elem.find_elements(By.CSS_SELECTOR, 
                            '.feed-shared-text__text-view, .feed-shared-update-v2__commentary, .attributed-text-segment-list__content')

                        if text_elements:
                            all_text_parts = []
                            for elem in text_elements:
                                elem_text = elem.text.strip()
                                if elem_text and len(elem_text) > 5:
                                    all_text_parts.append(elem_text)
    
                            if all_text_parts:
                                post_text = ' '.join(all_text_parts)
                                logger.info(f"Method 1 (complete): {post_text[:100]}...")

                    except Exception as e:
                        logger.debug(f"Method 1 failed: {e}")

                    # Method 2: Get ALL text content and intelligently combine it
                    if not post_text or len(post_text) < 50:  # If we didn't get much content
                        try:
                            full_text = post_elem.text.strip()
                            lines = full_text.split('\n')
    
                            # Filter and combine ALL meaningful content lines
                            content_lines = []
                            skip_patterns = [
                                'Like', 'Comment', 'Share', 'Send', 'Repost', 'Follow', 'Connect', 'Message',
                                'reactions', 'views', 'üëç', '‚ù§Ô∏è', 'üëè', 'üòä', 'üí°'
                            ]
    
                            for line in lines:
                                line = line.strip()
        
                                # Skip UI elements and metadata
                                if (line and 
                                    len(line) > 3 and  # Allow shorter lines for titles like "$20,000 a month to retire?!"
                                    not any(skip in line.lower() for skip in skip_patterns) and
                                    not line.endswith((' ago', ' hr', ' min', ' day', ' days', ' week', ' weeks', ' month', ' months')) and
                                    not line.startswith('‚Ä¢') and
                                    not (line.isdigit() and len(line) < 4) and  # Skip small numbers
                                    not line.startswith('Riley Giauque') and  # Skip author name
                                    not line.startswith('Personal Financial Consultant') and  # Skip job title
                                    line not in ['1yr', '3rd+', '2nd', '1st']):  # Skip post age indicators
            
                                    content_lines.append(line)
    
                            # Combine all meaningful lines into complete post
                            if content_lines:
                                post_text = ' '.join(content_lines)
                                logger.info(f"Method 2 (combined): {post_text[:100]}...")

                        except Exception as e:
                            logger.debug(f"Method 2 failed: {e}")

                    # Method 3: Last resort - try specific LinkedIn post structure
                    if not post_text or len(post_text) < 20:
                        try:
                            # Look for the main post container and extract everything
                            post_content = post_elem.find_element(By.CSS_SELECTOR, 
                                '.feed-shared-update-v2__description-wrapper, .feed-shared-text, .update-components-text')
    
                            if post_content:
                                post_text = post_content.text.strip()
                                logger.info(f"Method 3 (container): {post_text[:100]}...")
        
                        except Exception as e:
                            logger.debug(f"Method 3 failed: {e}")

                    # Clean up the final text - COMPREHENSIVE LINKEDIN CLEANUP
                    if post_text:
                        # STEP 1: Remove the "Feed post number X" prefix completely
                        post_text = re.sub(r'^Feed post number \d+\s*', '', post_text)
    
                        # Author cleanup for names
                        post_text = re.sub(r'([A-Z][a-z]+\s+[A-Z][a-z]+(?:,\s*[A-Z¬Æ¬©]+)?)\s*\1\s*', r'\1 ', post_text)  # Remove any name duplications
                        post_text = re.sub(r'‚Ä¢\s*You\s*You\s*', '', post_text)  # Remove "‚Ä¢ You You" patterns
    
                        # STEP 3: Remove timestamp and visibility metadata
                        post_text = re.sub(r'\d+m\s*‚Ä¢\s*\d+\s+minutes?\s+ago\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', post_text)
                        post_text = re.sub(r'\d+(mo|hr|min|d|w|yr)\s*‚Ä¢.*?‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', post_text)
                        post_text = re.sub(r'now\s*‚Ä¢\s*Just now\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', post_text)
                        post_text = re.sub(r'\d+yr\s*‚Ä¢\s*\d+\s+years?\s+ago\s*‚Ä¢\s*Visible to anyone on or off LinkedIn\s*', '', post_text)
    
                        # STEP 4: Remove analytics and engagement metadata
                        post_text = re.sub(r'\d+\s+(impressions?|views?|likes?|comments?)\s*View analytics.*?$', '', post_text)
                        post_text = re.sub(r'View analytics.*?$', '', post_text)
                        post_text = re.sub(r'\d+\s+(reactions?|views?|likes?|comments?)', '', post_text)
    
                        # STEP 5: Remove video/image metadata
                        post_text = re.sub(r'Auto captions have been added to your video.*?$', '', post_text)
                        post_text = re.sub(r'Edit captions.*?$', '', post_text)
                        post_text = re.sub(r'Activate to view larger image,?\s*', '', post_text)
    
                        # STEP 6: Remove group posting patterns
                        post_text = re.sub(r'Small Business Florida\s*Small Business Florida\s*', 'Small Business Florida ', post_text)
                        post_text = re.sub(r'Innovation In Payments\s*Innovation In Payments\s*', 'Innovation In Payments ', post_text)
                        post_text = re.sub(r'Tampa Bay Business Network\s*Tampa Bay Business Network\s*', 'Tampa Bay Business Network ', post_text)
                        post_text = re.sub(r'Chicago Young Professionals\s*Chicago Young Professionals\s*', 'Chicago Young Professionals ', post_text)
    
                        # STEP 7: Clean up group visibility
                        post_text = re.sub(r'Only visible to members of [^‚Ä¢]*‚Ä¢\s*', '', post_text)
                        post_text = re.sub(r'Edited\s*‚Ä¢\s*\d+\s+years?\s+ago\s*‚Ä¢\s*Edited\s*‚Ä¢\s*', '', post_text)
    
                        # STEP 8: Remove other LinkedIn UI elements
                        post_text = re.sub(r'\b(Like|Comment|Share|Send|Repost|Follow|Connect|Join)\b', '', post_text)
                        post_text = re.sub(r'hashtag\s+#', '#', post_text)
                        post_text = re.sub(r'\bhashtag\s+', '', post_text)
    
                        # STEP 9: Clean up multiple spaces and trim
                        post_text = re.sub(r'\s+', ' ', post_text).strip()
    
                        # STEP 10: Remove any remaining leading/trailing punctuation artifacts
                        post_text = re.sub(r'^[‚Ä¢\s]+', '', post_text)  # Remove leading bullets/spaces
                        post_text = re.sub(r'[‚Ä¢\s]+$', '', post_text)  # Remove trailing bullets/spaces
                        post_text = re.sub(r'\s*,\s*$', '', post_text)  # Remove trailing commas
    
                        # STEP 11: Final cleanup - if it starts with timestamp stuff, remove it
                        post_text = re.sub(r'^[^A-Za-z]*(?:\d+(?:mo|hr|min|d|w|yr|m)\s*‚Ä¢\s*)*', '', post_text)
    
                        # Final trim
                        post_text = post_text.strip()
    
                        logger.info(f"Final cleaned post: {post_text[:150]}...")

                    if not post_text or len(post_text) < 1:
                        logger.info(f"Skipping post {i} with insufficient content")
                        continue
                    
                    if post_text in processed_texts:
                        logger.info(f"Skipping duplicate post: {post_text[:30]}...")
                        continue
                    processed_texts.add(post_text)
                    
                    logger.info(f"Processing post {i}: '{post_text[:50]}...'")
                    
                    # Improved comparison - works with cleaned text
                    post_text_lower = post_text.lower()

                    # Extract key identifying phrases from the cleaned post text
                    key_phrases = []
                    if "market update" in post_text_lower:
                        date_match = re.search(r'market update\s+([0-9./]+)', post_text_lower)
                        if date_match:
                            key_phrases.append(f"market update {date_match.group(1)}")

                    if "stock market report" in post_text_lower:
                        date_match = re.search(r'stock market report.*?([0-9./]+)', post_text_lower)
                        if date_match:
                            key_phrases.append(f"stock market report {date_match.group(1)}")

                    # Check if this post already exists
                    is_existing = (post_text_lower in existing_post_texts or 
                                   any(phrase in existing_post_texts for phrase in key_phrases))

                    if is_existing:
                        logger.info(f"‚úì EXISTING post found: {post_text[:50]}...")
                        consecutive_existing += 1
                        
                        if consecutive_existing >= 2:
                            logger.info("Found 2 consecutive existing posts - stopping")
                            return new_posts
                    else:
                        logger.info(f"‚òÖ NEW post found: {post_text[:50]}...")
                        consecutive_existing = 0
                        
                        # Get timestamp
                        try:
                            time_elem = post_elem.find_element(By.CSS_SELECTOR, 'time, [data-test-id="main-feed-activity-card"] time')
                            timestamp = time_elem.get_attribute('datetime') or time_elem.text or "Unknown"
                            if timestamp != "Unknown":
                                from datetime import datetime
                                # Try to parse LinkedIn timestamp
                                readable_timestamp = timestamp
                            else:
                                readable_timestamp = "Unknown"
                        except:
                            readable_timestamp = "Unknown"
                        
                        new_posts.append({
                            'text': post_text,
                            'timestamp': readable_timestamp,
                            'type': 'post'
                        })
                
                except Exception as e:
                    logger.error(f"Error processing post {i}: {e}")
                    continue
            
            if consecutive_existing < 2 and scroll_attempt < 2:
                # Enhanced scrolling for LinkedIn
                try:
                    # Get current page height and scroll position
                    previous_height = driver.execute_script("return document.body.scrollHeight")
                    previous_scroll = driver.execute_script("return window.pageYOffset")

                    # **MULTI-PHASE SCROLLING TO FORCE LINKEDIN LAZY LOADING**

                    # Phase 1: Aggressive scroll to trigger loading
                    logger.info(f"Phase 1: Aggressive scroll from {previous_scroll}")
                    driver.execute_script("window.scrollBy(0, 2000);")  # Bigger scroll
                    time.sleep(1)

                    # Phase 2: Scroll to near bottom to trigger more loading
                    current_height = driver.execute_script("return document.body.scrollHeight")
                    near_bottom = current_height - 1000  # 1000px from bottom
                    driver.execute_script(f"window.scrollTo(0, {near_bottom});")
                    time.sleep(2)

                    # Phase 3: Scroll to actual bottom
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)

                    # Phase 4: Scroll back up slightly then down again (triggers LinkedIn's load)
                    driver.execute_script("window.scrollBy(0, -500);")
                    time.sleep(1)
                    driver.execute_script("window.scrollBy(0, 800);")
                    time.sleep(3)  # Longer wait for LinkedIn

                    # Check final results
                    final_height = driver.execute_script("return document.body.scrollHeight")
                    final_scroll = driver.execute_script("return window.pageYOffset")

                    logger.info(f"Multi-phase scroll: {previous_height}‚Üí{final_height}, scroll: {previous_scroll}‚Üí{final_scroll}")

                except Exception as scroll_error:
                    logger.error(f"Enhanced scroll failed: {scroll_error}")
                    # Fallback to simple scroll
                    driver.execute_script("window.scrollBy(0, 1500);")
                    time.sleep(3)
        
        logger.info(f"Completed: Found {len(new_posts)} new posts")

        # Notify completion for bulk operations
        try:
            bulk_data = {
                'platform': 'LinkedIn',
                'profiles_processed': 1,
                'profiles_with_new_content': 1 if len(new_posts) > 0 else 0,
                'total_new_posts': len(new_posts)
            }
            
            with open('bulk_linkedin_complete.json', 'w') as f:
                json.dump(bulk_data, f)
        except:
            pass
        
        return new_posts
        
    except Exception as e:
        logger.error(f"Error in LinkedIn scrape: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return []
        
    finally:
        # DON'T close the driver - let it stay open like Facebook/Twitter
        driver = None
        
def expand_linkedin_see_more(driver, post_element):
    """Expand 'See more' content in LinkedIn posts (your version, minimal tweaks)"""
    from selenium.webdriver.common.by import By
    import time
    try:
        css_selectors = [
            ".feed-shared-text__see-more-link",
            ".feed-shared-inline-show-more-text__see-more-less-toggle",
            "button[aria-label*='see more']",
            ".see-more",
        ]
        for selector in css_selectors:
            try:
                see_more_elem = post_element.find_element(By.CSS_SELECTOR, selector)
                if see_more_elem and see_more_elem.is_displayed():
                    driver.execute_script("arguments[0].click();", see_more_elem)
                    time.sleep(0.6)
                    try:
                        driver.execute_script("arguments[0].blur && arguments[0].blur();", see_more_elem)
                    except:
                        pass
                    return True
            except:
                continue

        # XPath fallback
        try:
            see_more_elem = post_element.find_element(By.XPATH, ".//button[contains(text(), 'See more')]")
            if see_more_elem and see_more_elem.is_displayed():
                driver.execute_script("arguments[0].click();", see_more_elem)
                time.sleep(0.6)
                try:
                    driver.execute_script("arguments[0].blur && arguments[0].blur();", see_more_elem)
                except:
                    pass
                return True
        except:
            pass

        return False
    except Exception as e:
        try:
            logger.error(f"Error expanding LinkedIn see more content: {e}")
        except:
            pass
        return False

# LINKEDIN SCRAPER SECTION

# Global driver management for LinkedIn
_linkedin_driver = None
_linkedin_logged_in = False

def get_linkedin_driver():
    global _linkedin_driver, _linkedin_logged_in
    if _linkedin_driver is None:
        _linkedin_driver = webdriver.Chrome(options=get_chrome_options())  # ADD get_chrome_options()
        logger.info("Created new LinkedIn driver")

        try:
            _linkedin_driver.command_executor.set_timeout(900)  # 900s = 15 min HTTP read-timeout for WebDriver
        except Exception as e:
            logger.warning(f"Could not set WebDriver HTTP timeout: {e}")

        
    return _linkedin_driver

def ensure_linkedin_login():
    """Ensure user is logged into LinkedIn"""
    global _linkedin_logged_in

    if not _linkedin_logged_in:
        driver = get_linkedin_driver()
        driver.get("https://www.linkedin.com/login")
        time.sleep(3)
    
        logger.info("Please login to LinkedIn manually. Press Enter when done...")
        show_browser_login_confirmation("LinkedIn")
        _linkedin_logged_in = True
        logger.info("LinkedIn login completed")
    else:
        logger.info("Already logged into LinkedIn")


def scrape_linkedin_profile(linkedin_url):
    """SLOWER (NO-SKIP) but still quick: Scrape LinkedIn profile posts with adaptive pacing and thorough bottom-out."""
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import time

    posts = []
    driver = None

    try:
        driver = get_linkedin_driver()

        # Keep bandwidth low but don't block all CSS; allow minimal styling for layout stability.
        try:
            driver.execute_cdp_cmd('Network.setBlockedURLs', {
                "urls": [
                    "*.jpg", "*.jpeg", "*.png", "*.gif", "*.webp", "*.svg",
                    "*.mp4", "*.webm", "*.m3u8", "*.ts",
                    "*.woff*", "*.ttf*",
                    "*ads*", "*doubleclick*", "*gstatic*"
                ]
            })
        except Exception:
            pass

        # Disable animations to reduce jank; preserves layout.
        try:
            driver.execute_script("""
                (function(){
                    var style = document.createElement('style');
                    style.innerHTML = '*{animation:none!important;transition:none!important;scroll-behavior:auto!important}';
                    document.head.appendChild(style);
                })();
            """)
        except Exception:
            pass

        # Ensure login
        driver.get("https://www.linkedin.com/feed/")
        time.sleep(1.0)
        request_login_confirmation("LinkedIn", "Please login to LinkedIn in the browser window, then click 'Continue'")

        # Go straight to activity stream
        posts_url = linkedin_url.rstrip('/') + "/recent-activity/all/"
        logger.info(f"Navigating directly to posts feed: {posts_url}")
        driver.get(posts_url)

        # Wait for any activity node
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '[data-urn*="activity"], .feed-shared-update-v2, main[role="main"]'))
            )
        except Exception:
            pass

        # One async script runs the whole collection with adaptive pacing + robust bottom detection
        driver.set_script_timeout(900) # 15 minute timeout
        results = driver.execute_async_script(r"""
const done = arguments[arguments.length - 1];
const sleep = ms => new Promise(r => setTimeout(r, ms));

/* WATCHDOG: ensure we always return before host 120s timeout */
let __finished = false;
const safeDone = (payload) => {
  if (__finished) return;
  __finished = true;
  try { mo && mo.disconnect && mo.disconnect(); } catch(e){}
  try { done(payload); } catch(e){}
};
// We‚Äôll cap the main loop at 80s and also arm a watchdog at ~83s.
setTimeout(() => {
  try { if (hasShowMore()) { clickShowMoreOnce().then(()=>collectOnce()); } } catch(e){}
  try { safeDone(out || []); } catch(e) { try { done([]); } catch(_){} }
}, 873000);   // ~14.55 minutes watchdog


/* ---------------- helpers ---------------- */

// --- "Show more results" helpers ---
const showMoreButtons = () => Array.from(document.querySelectorAll("button, a")).filter(b => {
  const txt  = (b.textContent || "").toLowerCase();
  const aria = (b.getAttribute("aria-label") || "").toLowerCase();
  const id   = (b.getAttribute("data-test-id") || "").toLowerCase();
  return (
    /show more results|load more/i.test(txt) ||
    /show more/i.test(aria) ||
    /load-more/.test(id)
  ) && b.offsetParent !== null && !b.disabled;
});
const hasShowMore = () => showMoreButtons().length > 0;
const clickShowMoreOnce = async () => {
  const btns = showMoreButtons();
  if (!btns.length) return false;
  try { btns[0].click(); } catch (e) {}
  await sleep(900);
  return true;
};

// --- placeholder detection (blank loading cards) ---
const isPlaceholderCard = (el) => {
  if (!el) return false;
  const r = el.getBoundingClientRect();
  const txt = (el.innerText || "").trim();

  // Treat *post shells* (real containers) with no content/time as placeholders
  if (el.matches('.feed-shared-update-v2,[data-urn^="urn:li:activity:"],[data-activity-urn^="urn:li:activity:"]')) {
    const hasTime = !!el.querySelector('time');
    const hasContent = txt.length > 2;
    return (!hasContent || !hasTime) && r.height >= 60 && r.width >= 200;
  }

  // Generic big empty box fallback
  if (txt.length > 0) return false;
  if (el.querySelector('img, video')) return false;
  return r.height >= 120 && r.width >= 200;
};


const placeholdersInViewport = () => {
  const els = document.querySelectorAll("div, section, article");
  const hits = [];
  for (const el of els) {
    if (!isPlaceholderCard(el)) continue;
    const r = el.getBoundingClientRect();
    if (r.bottom > 0 && r.top < window.innerHeight) hits.push(el);
  }
  return hits;
};
const scrollToElTopish = (el) => {
  try {
    el.scrollIntoView({ block: "start", inline: "nearest", behavior: "auto" });
    window.scrollBy(0, -Math.floor(window.innerHeight * 0.2));
  } catch (e) {}
};

// find the topmost real post currently visible
const topmostVisiblePost = () => {
  const nodes = findPostNodes();
  let best = null, minTop = Infinity;
  for (const n of nodes) {
    const r = n.getBoundingClientRect();
    if (r.bottom <= 0 || r.top >= window.innerHeight) continue;
    const t = getPostText(n);
    if (!t || t.length < 3) continue;
    if (r.top < minTop) { minTop = r.top; best = n; }
  }
  return best;
};

// find the first real post *below* an anchor (or below viewport if no anchor)
const firstRealPostBelow = (anchor) => {
  const nodes = findPostNodes();
  const anchorY = anchor
    ? (anchor.getBoundingClientRect().bottom + window.scrollY)
    : (window.scrollY + window.innerHeight);
  let best = null, minDy = Infinity;
  for (const n of nodes) {
    const r = n.getBoundingClientRect();
    const absTop = r.top + window.scrollY;
    if (absTop <= anchorY + 12) continue;            // must be below
    const t = getPostText(n);
    if (!t || t.length < 3) continue;                 // must be a real post
    const dy = absTop - anchorY;
    if (dy < minDy) { minDy = dy; best = n; }
  }
  return best;
};

// perform the "gap-recovery bounce": down -> up -> down
const recoverFromPlaceholders = async () => {
  const anchor = lastCollectedEl || topmostVisiblePost();
  const target = firstRealPostBelow(anchor);
  if (target) {
    // go *past* the blanks so LinkedIn starts hydrating them
    scrollToElTopish(target);
    await sleep(700);

    // return to the freshest harvested post and give it a beat to hydrate neighbors
    if (anchor) { scrollToElTopish(anchor); await sleep(2300); }


    // now step forward again to re-cross the previously blank gap
    const s = Math.max(Math.floor(window.innerHeight * 0.45), 350);
    await gentleStep(s); await sleep(700);
    await gentleStep(s); // second half-screen to re-cross hydrated cards

    // pick up any new text that just hydrated
    collectOnce();
    return true;
  }
  return false;
};


const textClean = s => (s||"")
  .replace(/https?:\/\/\S+/g, "")
  .replace(/\b(Like|Comment|Share|Send|Repost|Follow|Connect|Join)\b/gi, "")
  .replace(/\d+\s+(reactions?|views?|likes?|comments?)/gi, "")
  .replace(/\bVisible to anyone on or off LinkedIn\b/gi, "")
  .replace(/\s+/g, " ")
  .trim();

const expandSeeMore = (root) => {
  const btns = root.querySelectorAll(
    ".feed-shared-text__see-more-link, \
     .feed-shared-inline-show-more-text__see-more-less-toggle, \
     button[aria-label*='see more' i], \
     .see-more"
  );
  for (const b of btns) { try { b.click(); } catch(e) {} }
};

const getPostText = (node) => {
  const candidates = node.querySelectorAll(
    ".feed-shared-text__text-view, \
     .feed-shared-update-v2__commentary, \
     .attributed-text-segment-list__content, \
     .feed-shared-update-v2__description-wrapper, \
     .feed-shared-text, \
     .update-components-text"
  );
  let parts = [];
  for (const c of candidates) {
    const t = (c.innerText || "").trim();
    if (t && t.length > 1) parts.push(t);
  }
  if (!parts.length) {
    const t = (node.innerText || "").trim();
    if (t) parts.push(t);
  }
  return textClean(parts.join(" ").trim());
};

const getTimestamp = (node) => {
  const t = node.querySelector("time");
  if (!t) return "Unknown";
  return t.getAttribute("datetime") || (t.innerText || "Unknown").trim();
};

const hashLite = (s) => {
  let h = 0; for (let i=0; i<s.length; i++) h = ((h<<5)-h) + s.charCodeAt(i) | 0;
  return String(h>>>0);
};

const findPostNodes = () => {
  const set = new Set();
  document.querySelectorAll('[data-urn^="urn:li:activity:"]').forEach(n => set.add(n));
  document.querySelectorAll('[data-activity-urn^="urn:li:activity:"]').forEach(n => set.add(n));
  document.querySelectorAll('.feed-shared-update-v2').forEach(n => set.add(n));
  return Array.from(set);
};

// scroll targets (window + inner containers)
const scrollContainers = Array.from(document.querySelectorAll(
  ".scaffold-finite-scroll__content, .scaffold-layout__main, main[role='main']"
)).filter(Boolean);

// loader presence check
const hasLoader = () =>
  !!document.querySelector('[data-test-scrollcontainer] [data-test-reusable-loader], .artdeco-spinner');

/* ---- bottom detection helpers ---- */
const nearBottom = () =>
  (window.scrollY + window.innerHeight) >= (document.body.scrollHeight - 4);

const noVisibleLoaders = () => !hasLoader();

let bottomHits = 0;                     // consecutive confirmations we are at bottom
const BOTTOM_HITS_NEEDED = 3;           // require multiple confirms

// Soft runtime cap so Selenium doesn't time out upstream (120s host read-timeout)
const MAX_RUNTIME_MS = 870000;           // (~14.5 minutes cap)
const startedAt = Date.now();

// --- unified "ready to stop" check (bottom + no spinner + no show-more) ---
const readyToStop = () => nearBottom() && !hasLoader() && !hasShowMore();

// --- bottom confirmation: retry up to N times before truly stopping ---
const confirmTrueBottom = async (retries = 3) => {
  for (let i = 0; i < retries; i++) {
    if (!readyToStop()) return false;
    await gentleStep(-Math.max(300, Math.floor(window.innerHeight * 0.6)));
    await sleep(600);
    if (await clickShowMoreOnce()) {
      collectOnce();
      return false;
    }
    await hardBottom(2, 500);
    await sleep(700);
    collectOnce();
  }
  return readyToStop();
};

/* ---------------- mutation observer to pace ---------------- */
let mutationAdds = 0;
const mo = new MutationObserver((muts) => {
  for (const m of muts) {
    if (m.addedNodes && m.addedNodes.length) {
      mutationAdds += m.addedNodes.length;
    }
  }
});
mo.observe(document.body, {childList: true, subtree: true});

/* ---------------- collection state ---------------- */
const seen = new Set();
const out = [];
let lastCollectedEl = null;   // last post element we actually harvested

const collectOnce = () => {
  const nodes = findPostNodes();
  let added = 0;
  for (const n of nodes) {
    expandSeeMore(n);
    const urn = n.getAttribute("data-urn") ||
                n.getAttribute("data-activity-urn") ||
                (n.closest('[data-urn^=\"urn:li:activity:\"]')?.getAttribute("data-urn")) || "";
    const text = getPostText(n);
    const ts = getTimestamp(n);
    if (!text || text.length < 3) continue;
    const key = (urn && urn.startsWith("urn:li:activity:")) ? urn : ("synthetic:" + hashLite((ts||"")+"|"+text.slice(0,160)));
    if (seen.has(key)) continue;
    seen.add(key);
    out.push({urn: urn || key, text, timestamp: ts || "Unknown", type: "post"});
    lastCollectedEl = n;  // remember the actual node we just harvested
    added++;
  }
  return added;
};

const getHeights = () => {
  const arr = [document.body.scrollHeight];
  for (const el of scrollContainers) arr.push(el.scrollHeight || 0);
  return arr;
};
const heightsEqual = (a,b) => a.length === b.length && a.every((v,i)=>v===b[i]);

const gentleStep = async (px) => {
  window.scrollBy(0, px);
  for (const el of scrollContainers) { try { el.scrollBy(0, px); } catch(e){} }
};

const bounce = async () => {
  // small upward bounce to re-trigger lazy loaders
  await gentleStep(-Math.max(400, Math.floor(window.innerHeight*0.8)));
  await sleep(900);
};

const hardBottom = async (cycles=4, wait=500) => {
  for (let i=0;i<cycles;i++){
    window.scrollTo(0, document.body.scrollHeight);
    for (const el of scrollContainers) { try { el.scrollTop = el.scrollHeight; } catch(e){} }
    await sleep(wait);
  }
};

/* ---------------- main loop with adaptive pacing + bottom checks ---------------- */
let delay = 420;                 // base delay between steps (ms) ‚Äî deliberately slower
let stepPx = Math.max(550, Math.floor(window.innerHeight*0.75)); // smaller steps to avoid skipping
let stagnant = 0;
let noHeight = 0;
let loops = 0;

let placeholderRecoveries = 0;
const RECOVER_LIMIT = 12;   // safety so we don't loop forever

const MAX_STAGNANT = 8;          // tolerate more stagnation for safety
const MAX_NOHEIGHT = 8;
const MAX_POSTS = 4000;          // large cap for long histories
const MAX_LOOPS = 1500;

collectOnce();
let lastHeights = getHeights();

while (loops < MAX_LOOPS && out.length < MAX_POSTS && stagnant < MAX_STAGNANT && noHeight < MAX_NOHEIGHT) {
  loops++;

  // runtime cap to avoid upstream timeout
  if ((Date.now() - startedAt) > MAX_RUNTIME_MS) { safeDone(out); return; }

  // clear mutation counter each loop to measure new content arrival
  mutationAdds = 0;

  // 3 gentle steps with pauses
  for (let i=0;i<3;i++){
    await gentleStep(stepPx);
    await sleep(delay);
  }

  // If hydrating shells (white cards) are visible, WAIT until they fill, then (if needed) bounce
  let ph = placeholdersInViewport();
  if (ph.length) {
    if (lastCollectedEl) { scrollToElTopish(lastCollectedEl); }
    const t0 = Date.now();
    // Wait up to ~4s, harvesting as soon as they hydrate
    while (placeholdersInViewport().length && (Date.now() - t0) < 4000) {
      await sleep(300);
      collectOnce(); // grab any post that just populated
    }
    ph = placeholdersInViewport();
  }

  // If some blanks stubbornly remain, do one targeted recovery bounce
  if (ph.length && placeholderRecoveries < RECOVER_LIMIT) {
    const did = await recoverFromPlaceholders();
    placeholderRecoveries += (did ? 1 : 0);
    const addedAfter = collectOnce();
    if (addedAfter > 0) { stagnant = 0; noHeight = 0; }
  }



  // collect & measure
  const added = collectOnce();
  const hNow = getHeights();
  if (heightsEqual(hNow, lastHeights)) noHeight++; else noHeight = 0;

  // adaptive pacing
  if (mutationAdds > 40 || added >= 5) {
    delay = Math.max(250, Math.floor(delay * 0.85)); // speed up but stay safe
    stagnant = 0;
  } else if (mutationAdds === 0 && added === 0) {
    stagnant++;
    delay = Math.min(900, Math.floor(delay * 1.15)); // slow down to give time

    // try "show more results" (canonical)
    const clicked = await clickShowMoreOnce();
    if (clicked) { noHeight = 0; stagnant = 0; }

    // bounce + hard bottom to unstick
    await bounce();
    await hardBottom(3, 550);
    collectOnce();

    // after recovery attempts, bottom check
    if (readyToStop()) bottomHits++; else bottomHits = 0;
    if (bottomHits >= BOTTOM_HITS_NEEDED) {
      const sure = await confirmTrueBottom(3);   // 3x confirmation
      if (sure) { safeDone(out); return; }
      bottomHits = 0;          // not actually bottom; keep going
      continue;
    }

    // re-evaluate height change after forced loads
    const hAfter = getHeights();
    if (!heightsEqual(hAfter, hNow)) noHeight = 0;

  } else {
    delay = Math.max(300, Math.floor(delay * 0.95));
    stagnant = 0;
  }

  // Light pruning to keep DOM light
  const prunable = document.querySelectorAll('[data-urn*="activity"], .feed-shared-update-v2');
  if (prunable.length > 160) {
    const vpTop = window.scrollY;
    for (const p of prunable) {
      const r = p.getBoundingClientRect();
      const absBottom = r.bottom + window.scrollY;
      if (absBottom < vpTop - window.innerHeight*2) { try { p.remove(); } catch(e){} }
    }
  }

  lastHeights = getHeights();

  // If loaders are present, give them a breath
  if (hasLoader()) await sleep(650);

  // loop-end bottom + runtime checks
  if (readyToStop()) bottomHits++; else bottomHits = 0;
  if (bottomHits >= BOTTOM_HITS_NEEDED) {
    const sure = await confirmTrueBottom(3);
    if (sure) { safeDone(out); return; }
    bottomHits = 0;
  }
  if ((Date.now() - startedAt) > MAX_RUNTIME_MS) { safeDone(out); return; }
}

// Final assurance: consume any remaining Show-More, then deep bottom
for (let i = 0; i < 4 && hasShowMore(); i++) {
  const clicked = await clickShowMoreOnce();
  if (!clicked) break;
  collectOnce();
}
if (!readyToStop()) {
  await hardBottom(4, 600);
  await sleep(800);
  collectOnce();
}

// final certainty pass: if not truly bottom, do not exit yet
if (!(await confirmTrueBottom(3))) {
  // fall through ‚Äî outer code will continue until watchdog or loop caps
}

try { mo.disconnect(); } catch(e){}
safeDone(out);
        """)

        posts = [
            {
                "urn": r.get("urn"),
                "text": r.get("text", "").strip(),
                "timestamp": r.get("timestamp", "Unknown"),
                "type": r.get("type", "post")
            }
            for r in (results or [])
            if r.get("text")
        ]

        logger.info(f"Completed LinkedIn scrape (SLOWER no-skip): {len(posts)} posts")
        return posts

    except Exception as e:
        logger.error(f"Error scraping LinkedIn profile (SLOWER no-skip): {e}")
        return []

    finally:
        if driver:
            try:
                driver.execute_script("document.querySelectorAll('video').forEach(v=>{try{v.pause()}catch(e){}});")
            except Exception:
                pass


# END LINKEDIN SCRAPER SECTION

def scrape_page(self, url):
    try:
        # Add random delay between 3-8 seconds
        time.sleep(random.uniform(3, 8))
        
        # Rotate user agents to avoid detection
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
        ]
        
        # Create browser-like headers
        headers = self.session.headers.copy()
        headers['User-Agent'] = random.choice(user_agents)
        headers['Referer'] = 'https://www.google.com/'
        headers['Accept-Language'] = 'en-US,en;q=0.9'
        headers['Cache-Control'] = 'no-cache'
        headers['Sec-Fetch-Dest'] = 'document'
        headers['Sec-Fetch-Mode'] = 'navigate'
        headers['Sec-Fetch-Site'] = 'none'
        
        response = self.session.get(url, timeout=20, headers=headers)
        
        if response.status_code == 403:
            logger.warning(f"403 on {url}, trying fallback approach")
            return self.handle_403_error(url)
        
        response.raise_for_status()
        return BeautifulSoup(response.content, 'html.parser')
    
    except Exception as e:
        logger.error(f"Error scraping {url}: {str(e)}")
        return None
    
def handle_403_error(self, url):
    """Handle 403 errors with alternative approaches"""
    try:
        # Wait longer before retry
        time.sleep(random.uniform(15, 25))
        
        # Try with different headers that look like a different browser
        fallback_headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Referer': 'https://www.bing.com/',
            'Connection': 'keep-alive'
        }
        
        response = self.session.get(url, timeout=30, headers=fallback_headers)
        if response.status_code == 200:
            logger.info(f"Fallback approach worked for {url}")
            return BeautifulSoup(response.content, 'html.parser')
        
        # If still failing, mark as inaccessible
        logger.error(f"Website {url} is blocking access - marking as inaccessible")
        return None
        
    except Exception as e:
        logger.error(f"Fallback scraping failed for {url}: {str(e)}")
        return None
    

def normalize_content_for_comparison(content):
    """Remove title content and normalize whitespace for comparison"""
    if not content:
        return ""
    
    # Remove any title content at the beginning
    # Pattern: "TITLE: something\nCONTENT: actual content" or just "TITLE: something\nactual content"
    if content.startswith("TITLE:"):
        # Find the end of the title line and remove everything up to there
        first_newline = content.find('\n')
        if first_newline != -1:
            content = content[first_newline + 1:]
            # If the next line starts with "CONTENT:", remove that too
            if content.startswith("CONTENT:"):
                content = content[8:]  # Remove "CONTENT:"
    
    # Normalize whitespace
    content = re.sub(r'\s+', ' ', content).strip()
    
    return content

def update_scan_schema():
    """Add last_scan_checked column to website_snapshots table"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check if last_scan_checked column exists
            c.execute("PRAGMA table_info(website_snapshots)")
            columns = [column[1] for column in c.fetchall()]
            
            if 'last_scan_checked' not in columns:
                try:
                    c.execute('ALTER TABLE website_snapshots ADD COLUMN last_scan_checked TIMESTAMP')
                    logger.info("Added last_scan_checked column to website_snapshots table")
                except Exception as e:
                    logger.error(f"Error adding last_scan_checked column: {str(e)}")
            
            conn.commit()
            conn.close()
            logger.info("Scan schema update completed successfully")
            
    except Exception as e:
        logger.error(f"Error updating scan schema: {str(e)}")
        

def schedule_advisor_monitoring():
    """Schedule automatic monitoring of all advisors every 30 minutes"""
    # DISABLED - Remove this return statement to re-enable
    return
    
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            c.execute('SELECT id, name FROM advisors WHERE website_url IS NOT NULL AND website_url != ""')
            advisors = c.fetchall()
            conn.close()
        
        logger.info(f"Starting scheduled monitoring of {len(advisors)} advisors")
        for advisor_id, advisor_name in advisors:
            try:
                monitor_advisor(advisor_id)
                logger.info(f"Completed monitoring for {advisor_name}")
                time.sleep(5)
            except Exception as e:
                logger.error(f"Error monitoring {advisor_name}: {str(e)}")
        
        logger.info("Completed scheduled monitoring cycle")
        
    except Exception as e:
        logger.error(f"Error in scheduled monitoring: {str(e)}")

def run_scheduler():
    """Run the scheduler in a background thread"""
    # DISABLED - Remove this return statement to re-enable
    return
    
    logger.info("Starting advisor monitoring scheduler (every 30 minutes)")
    
    # Schedule the monitoring to run every 30 minutes
    schedule.every(30).minutes.do(schedule_advisor_monitoring)
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # Check every minute for scheduled jobs


def update_advisor_db_schema():
    """Update existing advisor table to add missing columns"""
    try:
        with db_lock:
            conn = get_db_connection()
            c = conn.cursor()
            
            # Check current table structure
            c.execute("PRAGMA table_info(advisors)")
            columns = [column[1] for column in c.fetchall()]
            logger.info(f"Current advisors table columns: {columns}")
            
            # Add firm column if it doesn't exist
            if 'firm' not in columns:
                try:
                    c.execute('ALTER TABLE advisors ADD COLUMN firm TEXT')
                    logger.info("Added firm column to advisors table")
                except Exception as e:
                    logger.error(f"Error adding firm column: {str(e)}")
            
            # Add broker column if it doesn't exist
            if 'broker' not in columns:
                try:
                    c.execute('ALTER TABLE advisors ADD COLUMN broker TEXT')
                    logger.info("Added broker column to advisors table")
                except Exception as e:
                    logger.error(f"Error adding broker column: {str(e)}")

            # Add CRD column if it doesn't exist
            if 'crd' not in columns:
                try:
                    c.execute('ALTER TABLE advisors ADD COLUMN crd TEXT')
                    logger.info("Added crd column to advisors table")
                except Exception as e:
                    logger.error(f"Error adding crd column: {str(e)}")
            
            # Add advisor_location column if it doesn't exist  
            if 'advisor_location' not in columns:
                try:
                    c.execute('ALTER TABLE advisors ADD COLUMN advisor_location TEXT')
                    logger.info("Added advisor_location column to advisors table")
                except Exception as e:
                    logger.error(f"Error adding advisor_location column: {str(e)}")
            
            conn.commit()
            c.close()
            release_db_connection(conn)
            logger.info("Database schema update completed successfully")
            
    except Exception as e:
        logger.error(f"Error updating advisor database schema: {str(e)}")

def update_advisor_db():
    """Add firm and broker fields to advisors table"""
    try:
        with db_lock:
            conn = get_db_connection()
            c = conn.cursor()
            
            # Check if firm column exists
            c.execute("PRAGMA table_info(advisors)")
            columns = [column[1] for column in c.fetchall()]
            
            if 'firm' not in columns:
                try:
                    c.execute('ALTER TABLE advisors ADD COLUMN firm TEXT')
                    logger.info("Added firm column to advisors table")
                except Exception as e:
                    logger.error(f"Error adding firm column: {str(e)}")
            
            if 'broker' not in columns:
                try:
                    c.execute('ALTER TABLE advisors ADD COLUMN broker TEXT')
                    logger.info("Added broker column to advisors table")
                except Exception as e:
                    logger.error(f"Error adding broker column: {str(e)}")
            
            conn.commit()
            c.close()
            release_db_connection(conn)
            logger.info("Database update completed successfully")
            
    except Exception as e:
        logger.error(f"Error updating advisor database: {str(e)}")
        
def get_db_connection_sqlite():
    """Get SQLite database connection for advisor monitoring"""
    conn = sqlite3.connect('advisor_monitor.db', timeout=20.0)
    conn.execute('PRAGMA journal_mode=WAL')
    return conn

def init_advisor_db():
    """Initialize advisor monitoring database"""
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Advisors table with firm and broker fields included from the start
        c.execute('''CREATE TABLE IF NOT EXISTS advisors
                     (id INTEGER PRIMARY KEY, bd_id INTEGER, name TEXT, location TEXT, 
                      website_url TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      compliance_checked BOOLEAN DEFAULT FALSE, firm TEXT, broker TEXT)''')
        
        # Changes table
        c.execute('''CREATE TABLE IF NOT EXISTS changes
                     (id INTEGER PRIMARY KEY, advisor_id INTEGER, change_type TEXT,
                      before_content TEXT, after_content TEXT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      description TEXT, page_url TEXT, changed_words TEXT,
                      FOREIGN KEY (advisor_id) REFERENCES advisors (id))''')
        
        # Website snapshots
        c.execute('''CREATE TABLE IF NOT EXISTS website_snapshots
                     (id INTEGER PRIMARY KEY, advisor_id INTEGER, page_url TEXT,
                      content_hash TEXT, page_title TEXT, content_text TEXT, 
                      last_checked TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      FOREIGN KEY (advisor_id) REFERENCES advisors (id))''')
        
        # Compliance issues table
        c.execute('''CREATE TABLE IF NOT EXISTS compliance_issues
                     (id INTEGER PRIMARY KEY, advisor_id INTEGER, page_url TEXT,
                      flagged_text TEXT, compliant_alternative TEXT, confidence TEXT,
                      rationale TEXT, detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      FOREIGN KEY (advisor_id) REFERENCES advisors (id))''')
        
        conn.commit()
        conn.close()
        logger.info("Advisor monitoring database tables created successfully")

def init_carousel_monitoring():
    """Initialize carousel monitoring table"""
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        c.execute('''CREATE TABLE IF NOT EXISTS carousel_snapshots
                     (id INTEGER PRIMARY KEY, 
                      advisor_id INTEGER, 
                      page_url TEXT,
                      carousel_data TEXT,  -- JSON string of carousel content
                      last_checked TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      FOREIGN KEY (advisor_id) REFERENCES advisors (id))''')
        
        conn.commit()
        conn.close()
    
class CarouselAwareMonitor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.selenium_driver = None
        self.current_proxy = None  # Track current proxy for this monitor instance
    
    def scrape_page(self, url):
        try:
            # Get next proxy in rotation
            proxy_endpoint = get_next_proxy()
        
            # Configure proxy for this request
            proxies = {
                'http': f'http://{PROXYMESH_USERNAME}:{PROXYMESH_PASSWORD}@{proxy_endpoint}',
                'https': f'http://{PROXYMESH_USERNAME}:{PROXYMESH_PASSWORD}@{proxy_endpoint}'
            }
        
            self.current_proxy = proxy_endpoint
            logger.info(f"Using proxy {proxy_endpoint} for {url}")
        
            # Add delay and headers as before
            time.sleep(random.uniform(5, 8))
        
            headers = self.session.headers.copy()
            headers['Referer'] = 'https://www.legacywealthfargo.com/'
        
            # Make request with proxy
            response = self.session.get(url, timeout=45, headers=headers, proxies=proxies)
        
            if response.status_code == 403:
                logger.warning(f"403 on carousel page {url}, trying alternative approach")
                return self.scrape_carousel_page_alternative(url)
        
            response.raise_for_status()
            logger.info(f"‚úÖ Successfully scraped {url} using proxy {proxy_endpoint}")
            return BeautifulSoup(response.content, 'html.parser')
    
        except Exception as e:
            logger.error(f"Error scraping {url} with proxy {self.current_proxy}: {str(e)}")
            return None

    def scrape_multiple_pages_with_proxy_rotation(self, pages):
        """Scrape multiple pages using a different proxy for each page"""
        results = {}
    
        for i, page_url in enumerate(pages):
            try:
                # Force a new proxy for each page
                proxy_endpoint = get_next_proxy()
                logger.info(f"üìÑ Page {i+1}/{len(pages)}: Using proxy {proxy_endpoint} for {page_url}")
            
                # Check if this page likely has carousels
                if self.likely_has_carousel(page_url):
                    logger.info(f"Detected carousel page, using enhanced scraping: {page_url}")
                    soup, carousel_data = self.scrape_carousel_content(page_url)
                
                    if soup is None:
                        logger.warning(f"Could not scrape carousel page: {page_url}")
                        continue
                else:
                    # Use regular scraping for non-carousel pages
                    soup = self.scrape_page(page_url)  # This will use the rotated proxy
                    carousel_data = []
            
                if soup:
                    results[page_url] = {
                        'soup': soup,
                        'carousel_data': carousel_data,
                        'proxy_used': proxy_endpoint
                    }
                    logger.info(f"‚úÖ Successfully scraped {page_url} with proxy {proxy_endpoint}")
            
                # Small delay between pages to be respectful
                time.sleep(random.uniform(2, 4))
            
            except Exception as e:
                logger.error(f"Error scraping {page_url}: {str(e)}")
                continue
        
        return results

    def scrape_carousel_page_alternative(self, url):
        """Alternative scraping method for carousel pages with proxy rotation"""
        try:
            # Get a different proxy for the retry
            proxy_endpoint = get_next_proxy()
            proxies = {
                'http': f'http://{PROXYMESH_USERNAME}:{PROXYMESH_PASSWORD}@{proxy_endpoint}',
                'https': f'http://{PROXYMESH_USERNAME}:{PROXYMESH_PASSWORD}@{proxy_endpoint}'
            }
        
            logger.info(f"Alternative scraping using proxy {proxy_endpoint} for {url}")
        
            # Simulate browsing behavior - visit main page first
            main_page_url = '/'.join(url.split('/')[:3])  # Get domain root
            self.session.get(main_page_url, timeout=10, proxies=proxies)
    
            # Wait before requesting the actual page
            time.sleep(random.uniform(3, 6))
    
            # Add headers that simulate carousel interaction
            carousel_headers = {
                'Referer': main_page_url,
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Cache-Control': 'max-age=0'
            }
    
            response = self.session.get(url, timeout=45, headers=carousel_headers, proxies=proxies)
            response.raise_for_status()
            logger.info(f"‚úÖ Successfully scraped alternative {url} using proxy {proxy_endpoint}")
            return BeautifulSoup(response.content, 'html.parser')
    
        except Exception as e:
            logger.error(f"Alternative carousel scraping failed for {url}: {str(e)}")
            return None
    
    def remove_carousel_content(self, soup):
        """Remove all carousel/slider content using universal detection patterns"""
        if not soup:
            return soup
        
        # Universal carousel selectors - covers 95% of carousel implementations
        carousel_selectors = [

            # Your specific blocked selectors
            'section#suggested.c-matters.c-matters--small.c-matter-list',
            'section.c-matter.content.left.asset-334',
            'section.c-matter.content.asset-1071',
            'section.c-matter.content.asset-252',
            'section#additional.c-additional__content.c-additional__content--large',
            'section#matterContent.c-matters.c-matters--large',
            'section.c-matter__form-container',
            'ul.c-additional__content--small.js-additional',
            
            # Class-based selectors
            '[class*="carousel"]', '[class*="slider"]', '[class*="slideshow"]', 
            '[class*="swiper"]', '[class*="slick"]', '[class*="owl-carousel"]',
            '[class*="glide"]', '[class*="splide"]', '[class*="keen-slider"]',
            '[class*="flickity"]', '[class*="cycle"]', '[class*="rotating"]',
            '[class*="banner-rotator"]', '[class*="image-rotator"]',
            
            # ID-based selectors  
            '[id*="carousel"]', '[id*="slider"]', '[id*="slideshow"]',
            '[id*="banner"]', '[id*="rotator"]',
            
            # Data attribute selectors (common in modern frameworks)
            '[data-ride="carousel"]', '[data-carousel]', '[data-slider]',
            '[data-slick]', '[data-swiper]', '[data-cycle]',
            
            # Specific framework classes
            '.carousel', '.carousel-inner', '.carousel-item',  # Bootstrap
            '.slick-slider', '.slick-list', '.slick-track',    # Slick
            '.swiper-container', '.swiper-wrapper', '.swiper-slide',  # Swiper
            '.owl-carousel', '.owl-stage', '.owl-item',        # Owl Carousel
            '.glide', '.glide__track', '.glide__slides',       # Glide
            '.flickity-slider', '.flickity-viewport',          # Flickity
        ]
        
        # Remove all carousel elements
        for selector in carousel_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    element.decompose()
            except Exception:
                continue  # Skip invalid selectors
        
        # Detect carousels by structure patterns
        self._remove_carousel_structures(soup)
        
        return soup

    def _remove_carousel_structures(self, soup):
        """Detect and remove carousel-like structures by analyzing DOM patterns"""
        
        # Pattern 1: Multiple similar child elements (common carousel pattern)
        containers = soup.find_all(['div', 'section', 'ul'])
        for container in containers:
            children = container.find_all(['div', 'li', 'article'], recursive=False)
            
            # If container has 3+ similar direct children, likely a carousel
            if len(children) >= 3:
                # Check if children have similar structure (same tags, similar classes)
                first_child = children[0]
                similar_count = 0
                
                for child in children[1:]:
                    if (child.name == first_child.name and 
                        self._similar_class_structure(child, first_child)):
                        similar_count += 1
                
                # If 80%+ of children are similar, treat as carousel
                if similar_count >= len(children) * 0.8:
                    # Check if it has carousel-like navigation
                    if self._has_carousel_navigation(container):
                        container.decompose()
                        continue
        
        # Pattern 2: Elements with carousel-like text content
        carousel_text_patterns = [
            r'previous\s*slide', r'next\s*slide', r'slide\s*\d+',
            r'dot\s*navigation', r'carousel\s*control',
            r'slider\s*navigation', r'image\s*\d+\s*of\s*\d+'
        ]
        
        for pattern in carousel_text_patterns:
            elements = soup.find_all(text=re.compile(pattern, re.IGNORECASE))
            for element in elements:
                # Find the carousel container (usually 2-4 levels up)
                parent = element.parent
                levels_up = 0
                while parent and levels_up < 4:
                    if parent.name in ['div', 'section', 'article']:
                        # Check if this looks like a carousel container
                        if self._looks_like_carousel_container(parent):
                            parent.decompose()
                            break
                    parent = parent.parent
                    levels_up += 1

    def _similar_class_structure(self, elem1, elem2):
        """Check if two elements have similar class structures"""
        classes1 = set(elem1.get('class', []))
        classes2 = set(elem2.get('class', []))
        
        # If both have no classes, consider similar
        if not classes1 and not classes2:
            return True
        
        # If one has classes and other doesn't, not similar
        if bool(classes1) != bool(classes2):
            return False
        
        # Calculate similarity ratio
        if classes1 and classes2:
            intersection = len(classes1.intersection(classes2))
            union = len(classes1.union(classes2))
            similarity = intersection / union if union > 0 else 0
            return similarity > 0.5
        
        return False

    def scrape_page_with_retry(self, url, max_retries=2):
        """Scrape page with automatic retry on connection errors"""
        import time
    
        connection_error_phrases = [
            "site can't be reached",  # Removed "this" and "the" for broader matching
            "err_socket_not_connected",
            "err_connection_refused", 
            "err_connection_timed_out",
            "temporarily down",
            "might be temporarily down",
            "moved permanently",
            "new web address",
            "connection timed out",
            "website is offline"
        ]
    
        for attempt in range(max_retries + 1):
            try:
                soup = self.scrape_page(url)
                if soup:
                    # Check if the content indicates a connection error
                    page_text = soup.get_text().lower()
                
                    # Debug logging to see what we're actually getting
                    logger.info(f"DEBUG: Page text for {url}: {page_text[:200]}...")
                    
                    if any(error_phrase in page_text for error_phrase in connection_error_phrases):
                        if attempt < max_retries:
                            logger.warning(f"Connection error detected on attempt {attempt + 1} for {url}, retrying...")
                            time.sleep(3)  # Increased wait time
                            continue
                        else:
                            logger.error(f"Connection error persists after {max_retries + 1} attempts for {url}")
                            return soup  # Return the error page so it can be detected and handled
                    return soup
                else:
                    if attempt < max_retries:
                        logger.warning(f"Failed to scrape {url} on attempt {attempt + 1}, retrying...")
                        time.sleep(3)
                        continue
                    
            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f"Exception scraping {url} on attempt {attempt + 1}: {e}, retrying...")
                    time.sleep(3)
                    continue
                else:
                    logger.error(f"Failed to scrape {url} after {max_retries + 1} attempts: {e}")
                
        return None

    def _has_carousel_navigation(self, container):
        """Check if container has typical carousel navigation elements"""
        nav_indicators = [
            'button', 'a[href="#"]', '.prev', '.next', '.arrow',
            '[class*="nav"]', '[class*="control"]', '[class*="indicator"]',
            '[class*="dot"]', '[class*="bullet"]'
        ]
        
        for indicator in nav_indicators:
            if container.select(indicator):
                return True
        
        # Check for text-based navigation
        nav_text_patterns = ['prev', 'next', '‚Äπ', '‚Ä∫', '‚óÄ', '‚ñ∂', '‚¨Ö', '‚û°']
        text_content = container.get_text().lower()
        for pattern in nav_text_patterns:
            if pattern in text_content:
                return True
        
        return False

    def _looks_like_carousel_container(self, element):
        """Analyze if an element looks like a carousel container"""
        
        # Check for carousel-related attributes
        attrs_to_check = ['class', 'id', 'data-role', 'role']
        for attr in attrs_to_check:
            attr_value = str(element.get(attr, '')).lower()
            carousel_keywords = ['carousel', 'slider', 'slideshow', 'banner', 'rotator']
            if any(keyword in attr_value for keyword in carousel_keywords):
                return True
        
        # Check if it has multiple child images/content blocks
        images = element.find_all('img')
        content_blocks = element.find_all(['div', 'article', 'section'])
        
        return len(images) >= 2 or len(content_blocks) >= 3
    
    def extract_content(self, soup):
        if not soup:
            return {"title": "", "text": ""}

        # Remove unwanted elements
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", 
                            "iframe", "embed", "object", "form", "input", "button", "select", 
                            "textarea", "link", "meta"]):
            element.decompose()

        # Remove disclaimer and legal text that appears inconsistently
        for element in soup.find_all(text=re.compile(r'\b(certain services are provided|educational and guidance basis|disclaimer|legal notice|terms and conditions)\b', re.IGNORECASE)):
            element.extract()

        # Remove small footer/disclaimer sections
        for element in soup.select('.disclaimer, .legal, .footer-disclaimer, [class*="disclaimer"], [class*="legal"]'):
            element.decompose()

        # Remove common button/link text patterns
        for element in soup.find_all(text=re.compile(r'\b(Learn More|Read More|Click Here|View More|See More|Get Started|Contact Us|Learn Now|Shopping Cart|Continue Shopping|you have nothing in your shopping cart|cart is empty|checkout|add to cart)\b', re.IGNORECASE)):
            element.extract()

        # Remove standalone numbers and cart indicators that appear inconsistently
        for element in soup.find_all(text=re.compile(r'^\s*\d+\s*$')):
            element.extract()

        # Remove cart badge/indicator elements
        for element in soup.select('.badge, .cart-count, .cart-badge, .notification, [class*="badge"], [class*="count"]'):
            element.decompose()

        # Remove ALL carousel content
        soup = self.remove_carousel_content(soup)

        # Extract title (but we won't include it in the text content)
        title = soup.title.string.strip() if soup.title else ""

        # Extract main text content
        main_content_selectors = [
            'main', 'article', '.content', '.main-content', '.post-content',
            '.entry-content', '.page-content', '#content', '#main-content',
            '.container', '.wrapper', 'section', '.section'
        ]

        main_text = ""
        for selector in main_content_selectors:
            main_elements = soup.select(selector)
            if main_elements:
                for element in main_elements:
                    main_text += element.get_text(separator=' ', strip=True) + " "
                break

        if not main_text.strip():
            main_text = soup.get_text(separator=' ', strip=True)

        text_content = ' '.join(main_text.split())
        text_content = re.sub(r'\s+', ' ', text_content).strip()

        # REMOVE TITLE CONTENT FROM TEXT - this is the key fix
        if title and text_content.startswith(title):
            text_content = text_content[len(title):].strip()
    
        # Remove common title/header patterns from the beginning
        text_content = re.sub(r'^[^.!?]*(?:Financial Strategies?|Get in Touch|Contact|About|Home)\s*', '', text_content, flags=re.IGNORECASE)
    
        # Remove any remaining standalone company/contact info at the start
        text_content = re.sub(r'^\s*[A-Z][a-z]+\s+Financial\s+Strategies?\s*', '', text_content)

        # Remove any remaining whitespace-only content and normalize
        text_content = re.sub(r'\s+', ' ', text_content).strip()

        # If content is essentially empty or just placeholder text, return empty
        if (len(text_content) <= 3 or 
            any(placeholder in text_content.lower() for placeholder in ['content removed', 'no content', 'content filtered', 'content blocked']) or
            text_content.strip() in ['0', '|', '-', '‚Ä¢']):
            text_content = ""

        return {
            "title": title,
            "text": text_content  # Now actually excludes title content
        }

    def normalize_url(self, url):
        """Normalize URL to prevent duplicates"""
        # Remove trailing slash except for root domain
        if url.endswith('/') and url.count('/') > 2:
            url = url.rstrip('/')
    
        # Remove common URL fragments
        if '#' in url:
            url = url.split('#')[0]
    
        # Remove query parameters for cleaner URLs (optional)
        if '?' in url:
            url = url.split('?')[0]
    
        return url
        
    def find_all_pages(self, base_url):
        """Find all pages on the website with detailed logging"""
        try:
            logger.info(f"üîç STARTING PAGE DISCOVERY for {base_url}")
        
            soup = self.scrape_page(base_url)
            if not soup:
                logger.error(f"‚ùå Could not scrape base page: {base_url}")
                return [base_url]
        
            logger.info(f"‚úÖ Successfully scraped base page: {base_url}")
        
            pages = {self.normalize_url(base_url)}
            base_domain = urlparse(base_url).netloc
            logger.info(f"üåê Base domain: {base_domain}")
        
            # Find all links
            all_links = soup.find_all('a', href=True)
            logger.info(f"üîó Found {len(all_links)} total links on page")
        
            valid_links = []
            skipped_links = []
        
            for link in all_links:
                href = link['href']
            
                # Process relative URLs
                if href.startswith('/'):
                    full_url = urljoin(base_url, href)
                    logger.debug(f"  üìç Relative link: {href} ‚Üí {full_url}")
                # Process absolute URLs on same domain
                elif href.startswith('http') and urlparse(href).netloc == base_domain:
                    full_url = href
                    logger.debug(f"  üìç Absolute link: {full_url}")
                else:
                    skipped_links.append(href)
                    continue

                # Skip BrokerCheck URLs
                if 'brokercheck.finra.org' in full_url.lower():
                    logger.debug(f"  ‚è≠Ô∏è Skipping BrokerCheck: {full_url}")
                    skipped_links.append(full_url)
                    continue
            
                # Skip file downloads
                if any(full_url.lower().endswith(ext) for ext in ['.pdf', '.doc', '.jpg', '.png', '.gif']):
                    logger.debug(f"  ‚è≠Ô∏è Skipping file: {full_url}")
                    skipped_links.append(full_url)
                    continue
                
                # Skip anchor links
                if '#' in full_url:
                    clean_url = full_url.split('#')[0]
                    if clean_url and clean_url not in pages:
                        logger.debug(f"  üìç Anchor link cleaned: {full_url} ‚Üí {clean_url}")
                        valid_links.append(clean_url)
                    else:
                        skipped_links.append(full_url)
                    continue
            
                # Normalize URL to prevent duplicates
                normalized_url = self.normalize_url(full_url)
                if normalized_url not in pages:
                    valid_links.append(normalized_url)
            
                # Safety limit
                if len(pages) + len(valid_links) >= 50:
                    logger.warning(f"‚ö†Ô∏è Reached 50 page limit, stopping discovery")
                    break
        
            # Add valid links to pages set
            for link in valid_links:
                pages.add(link)
        
            logger.info(f"üìä PAGE DISCOVERY SUMMARY for {base_url}:")
            logger.info(f"  ‚úÖ Valid pages found: {len(pages)}")
            logger.info(f"  ‚è≠Ô∏è Links skipped: {len(skipped_links)}")
            logger.info(f"  üìù Page list:")
        
            for i, page in enumerate(sorted(pages), 1):
                logger.info(f"    {i:2d}. {page}")
        
            if skipped_links:
                logger.info(f"  üö´ Sample skipped links:")
                for skip in skipped_links[:5]:  # Show first 5 skipped
                    logger.info(f"    - {skip}")
                if len(skipped_links) > 5:
                    logger.info(f"    ... and {len(skipped_links) - 5} more")
        
            return list(pages)
        
        except Exception as e:
            logger.error(f"‚ùå Error finding pages for {base_url}: {str(e)}")
            logger.error(f"   Full error details:", exc_info=True)
            return [base_url]

    def get_selenium_driver(self):
        """Initialize Selenium driver if not already done"""
        if self.selenium_driver is None:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            self.selenium_driver = webdriver.Chrome(options=options)
        return self.selenium_driver
    
    def likely_has_carousel(self, url):
        """Quick check if a page likely has carousel content"""
        try:
            response = self.session.get(url, timeout=45)
            content = response.text.lower()
            
            carousel_indicators = [
                'carousel', 'slider', 'slideshow', 'swiper', 'slick',
                'owl-carousel', 'data-slide', 'prev', 'next', 'dots'
            ]
            
            return any(indicator in content for indicator in carousel_indicators)
        except:
            return False
    
    def scrape_carousel_content(self, url):
        """Extract all carousel content by interacting with it"""
        try:
            driver = self.get_selenium_driver()
            driver.get(url)
            
            # Wait for page to load
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            carousel_data = []
            
            # Common carousel selectors
            carousel_selectors = [
                '.carousel', '.slider', '.slideshow', '.swiper-container',
                '[data-carousel]', '[data-slider]', '.owl-carousel',
                '.slick-slider', '.glide', '.flickity-slider'
            ]
            
            for selector in carousel_selectors:
                carousels = driver.find_elements(By.CSS_SELECTOR, selector)
                
                for carousel in carousels:
                    logger.info(f"Found carousel with selector: {selector}")
                    carousel_content = self.extract_all_carousel_slides(driver, carousel)
                    if carousel_content:
                        carousel_data.extend(carousel_content)
            
            # If no carousels found with common selectors, try to detect by behavior
            if not carousel_data:
                carousel_data = self.detect_and_extract_carousel_by_behavior(driver)
            
            # Get the final page state after all interactions
            final_soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            return final_soup, carousel_data
            
        except Exception as e:
            logger.error(f"Error scraping carousel content from {url}: {e}")
            return None, []
    
    def extract_all_carousel_slides(self, driver, carousel_element):
        """Extract content from all slides in a carousel"""
        slides_content = []
        
        try:
            # Find navigation buttons
            nav_buttons = self.find_carousel_navigation(driver, carousel_element)
            
            if nav_buttons:
                slides_content = self.navigate_through_slides(driver, nav_buttons)
            else:
                # Try to find slides directly
                slide_selectors = [
                    '.carousel-item', '.slide', '.swiper-slide',
                    '.slick-slide', '.owl-item', '.glide__slide'
                ]
                
                for selector in slide_selectors:
                    slides = carousel_element.find_elements(By.CSS_SELECTOR, selector)
                    if slides:
                        for i, slide in enumerate(slides):
                            try:
                                # Make slide visible/active if needed
                                driver.execute_script("arguments[0].style.display = 'block';", slide)
                                driver.execute_script("arguments[0].classList.add('active');", slide)
                                
                                slide_content = {
                                    'slide_index': i,
                                    'text': slide.text.strip(),
                                    'html': slide.get_attribute('innerHTML'),
                                    'images': [img.get_attribute('src') for img in slide.find_elements(By.TAG_NAME, 'img')],
                                    'links': [link.get_attribute('href') for link in slide.find_elements(By.TAG_NAME, 'a')]
                                }
                                slides_content.append(slide_content)
                            except Exception as e:
                                logger.warning(f"Error extracting slide {i}: {e}")
                        break
            
            logger.info(f"Extracted {len(slides_content)} slides from carousel")
            return slides_content
            
        except Exception as e:
            logger.error(f"Error extracting carousel slides: {e}")
            return []
    
    def find_carousel_navigation(self, driver, carousel_element):
        """Find carousel navigation buttons"""
        nav_selectors = [
            '.carousel-control-next', '.carousel-control-prev',
            '.slick-next', '.slick-prev',
            '.swiper-button-next', '.swiper-button-prev',
            '.owl-next', '.owl-prev',
            '.next', '.prev', '.arrow-next', '.arrow-prev',
            '[data-slide="next"]', '[data-slide="prev"]'
        ]
        
        for selector in nav_selectors:
            buttons = carousel_element.find_elements(By.CSS_SELECTOR, selector)
            if buttons:
                return buttons
        
        return []
    
    def navigate_through_slides(self, driver, nav_buttons):
        """Navigate through carousel slides and capture content"""
        slides_content = []
        
        try:
            # Find the "next" button
            next_button = None
            for button in nav_buttons:
                button_class = button.get_attribute('class').lower()
                if any(keyword in button_class for keyword in ['next', 'forward', 'right']):
                    next_button = button
                    break
            
            if not next_button:
                return slides_content
            
            max_slides = 10  # Safety limit
            seen_content = set()
            
            for slide_num in range(max_slides):
                # Wait for slide to load
                time.sleep(2)
                
                # Capture current slide content
                current_content = self.capture_current_slide_content(driver)
                
                # Check if we've seen this content before (carousel looped)
                content_hash = hash(current_content.get('text', ''))
                if content_hash in seen_content:
                    logger.info(f"Detected carousel loop at slide {slide_num}")
                    break
                
                seen_content.add(content_hash)
                current_content['slide_index'] = slide_num
                slides_content.append(current_content)
                
                # Click next button
                try:
                    driver.execute_script("arguments[0].click();", next_button)
                except Exception as e:
                    logger.warning(f"Could not click next button: {e}")
                    break
            
            return slides_content
            
        except Exception as e:
            logger.error(f"Error navigating carousel: {e}")
            return slides_content
    
    def capture_current_slide_content(self, driver):
        """Capture content of the currently visible slide"""
        try:
            # Find visible slide elements
            visible_selectors = [
                '.active', '.current', '.visible',
                '[style*="display: block"]', '[style*="opacity: 1"]'
            ]
            
            content = {
                'text': '',
                'html': '',
                'images': [],
                'links': []
            }
            
            for selector in visible_selectors:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    # Check if element is actually visible
                    if element.is_displayed():
                        content['text'] += element.text + ' '
                        content['html'] += element.get_attribute('outerHTML')
                        
                        # Get images and links in this slide
                        imgs = element.find_elements(By.TAG_NAME, 'img')
                        content['images'].extend([img.get_attribute('src') for img in imgs])
                        
                        links = element.find_elements(By.TAG_NAME, 'a')
                        content['links'].extend([link.get_attribute('href') for link in links])
            
            return content
            
        except Exception as e:
            logger.error(f"Error capturing slide content: {e}")
            return {'text': '', 'html': '', 'images': [], 'links': []}

    def detect_and_extract_carousel_by_behavior(self, driver):
        """Detect carousels by behavior patterns"""
        carousel_data = []
        try:
            # Look for elements that might be carousel containers
            potential_carousels = driver.find_elements(By.CSS_SELECTOR, 'div, section, article')
            
            for container in potential_carousels:
                # Check if this container has multiple similar child elements
                children = container.find_elements(By.CSS_SELECTOR, '> div, > li, > article')
                
                if len(children) >= 3:  # Likely a carousel if 3+ similar children
                    # Try to extract content from each child
                    for i, child in enumerate(children):
                        try:
                            slide_content = {
                                'slide_index': i,
                                'text': child.text.strip(),
                                'html': child.get_attribute('innerHTML'),
                                'images': [img.get_attribute('src') for img in child.find_elements(By.TAG_NAME, 'img')],
                                'links': [link.get_attribute('href') for link in child.find_elements(By.TAG_NAME, 'a')]
                            }
                            if slide_content['text']:  # Only add if there's actual text content
                                carousel_data.append(slide_content)
                        except Exception as e:
                            continue
                    
                    if carousel_data:  # If we found content, stop looking
                        break
            
            return carousel_data
            
        except Exception as e:
            logger.error(f"Error detecting carousel by behavior: {e}")
            return []
        
def compare_carousel_content(old_carousel_data, new_carousel_data):
    """Compare carousel content to detect meaningful changes"""
    changes = []
    
    # Compare number of slides
    old_slides = len(old_carousel_data)
    new_slides = len(new_carousel_data)
    
    if old_slides != new_slides:
        changes.append(f"Carousel slide count changed: {old_slides} ‚Üí {new_slides}")
    
    # Compare individual slides
    max_slides = max(old_slides, new_slides)
    
    for i in range(max_slides):
        old_slide = old_carousel_data[i] if i < old_slides else None
        new_slide = new_carousel_data[i] if i < new_slides else None
        
        if old_slide is None:
            changes.append(f"New slide added at position {i}: {new_slide['text'][:100]}...")
        elif new_slide is None:
            changes.append(f"Slide removed from position {i}: {old_slide['text'][:100]}...")
        else:
            # Compare slide content
            slide_changes = compare_slide_content(old_slide, new_slide, i)
            changes.extend(slide_changes)
    
    return changes

def compare_slide_content(old_slide, new_slide, slide_index):
    """Compare individual slide content"""
    changes = []
    
    # Compare text content
    old_text = old_slide.get('text', '').strip()
    new_text = new_slide.get('text', '').strip()
    
    if old_text != new_text:
        # Use difflib for detailed text comparison
        import difflib
        diff = list(difflib.unified_diff(
            old_text.splitlines(keepends=True),
            new_text.splitlines(keepends=True),
            fromfile=f'slide_{slide_index}_old',
            tofile=f'slide_{slide_index}_new',
            lineterm=''
        ))
        if diff:
            changes.append(f"Slide {slide_index} text changed:\n{''.join(diff)}")
    
    # Compare images
    old_images = set(old_slide.get('images', []))
    new_images = set(new_slide.get('images', []))
    
    if old_images != new_images:
        added_images = new_images - old_images
        removed_images = old_images - new_images
        
        if added_images:
            changes.append(f"Slide {slide_index} - Images added: {list(added_images)}")
        if removed_images:
            changes.append(f"Slide {slide_index} - Images removed: {list(removed_images)}")
    
    # Compare links
    old_links = set(old_slide.get('links', []))
    new_links = set(new_slide.get('links', []))
    
    if old_links != new_links:
        added_links = new_links - old_links
        removed_links = old_links - new_links
        
        if added_links:
            changes.append(f"Slide {slide_index} - Links added: {list(added_links)}")
        if removed_links:
            changes.append(f"Slide {slide_index} - Links removed: {list(removed_links)}")
    
    return changes

def store_carousel_data(cursor, advisor_id, page_url, carousel_data):
    """Store carousel data for future comparison"""
    try:
        carousel_json = json.dumps(carousel_data)
        cursor.execute('''
            INSERT OR REPLACE INTO carousel_snapshots 
            (advisor_id, page_url, carousel_data, last_checked)
            VALUES (?, ?, ?, CURRENT_TIMESTAMP)
        ''', (advisor_id, page_url, carousel_json))
        
    except Exception as e:
        logger.error(f"Error storing carousel data: {e}")

def extract_text_from_carousel_data(carousel_data):
    """Extract searchable text from carousel data"""
    text_parts = []
    
    for i, slide in enumerate(carousel_data):
        slide_text = f"Slide {i+1}: {slide.get('text', '').strip()}"
        text_parts.append(slide_text)
    
    return '\n'.join(text_parts)

# Add this function definition after your imports
def parallel_rescan_in_batches(monitor, just_a_moment_pages, advisor_id):
    """Rescan in small parallel batches to balance speed vs success"""
    import concurrent.futures
    import hashlib
    
    batch_size = 3  # Process 3 rescans simultaneously (conservative)
    rescanned_count = 0
    
    # Process in batches
    for i in range(0, len(just_a_moment_pages), batch_size):
        batch = just_a_moment_pages[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        total_batches = (len(just_a_moment_pages) + batch_size - 1) // batch_size
        
        logger.info(f"üöÄ Rescanning batch {batch_num}/{total_batches}: {len(batch)} pages in parallel")
        
        def rescan_single_page(page_info):
            page_url, old_content, old_hash = page_info
            try:
                soup = monitor.scrape_page_with_retry(page_url)
                if soup:
                    content = monitor.extract_content(soup)
                    return {
                        'page_url': page_url,
                        'content': content,
                        'new_full_content': content['text'],
                        'new_content_hash': hashlib.md5(content['text'].encode()).hexdigest()
                    }
            except Exception as e:
                logger.error(f"Error rescanning {page_url}: {e}")
                return None
        
        # Process this batch in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
            future_to_page = {executor.submit(rescan_single_page, page_info): page_info[0] 
                             for page_info in batch}
            
            for future in concurrent.futures.as_completed(future_to_page):
                page_url = future_to_page[future]
                try:
                    result = future.result()
                    if result:
                        # Update database (your existing logic)
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            c.execute('''UPDATE website_snapshots 
                                       SET content_hash = ?, content_text = ?, page_title = ?, last_checked = CURRENT_TIMESTAMP 
                                       WHERE advisor_id = ? AND page_url = ?''',
                                     (result['new_content_hash'], result['new_full_content'], 
                                      result['content']['title'], advisor_id, result['page_url']))
                            conn.commit()
                            conn.close()
                        
                        rescanned_count += 1
                        logger.info(f"‚úÖ Batch rescan SUCCESS: {page_url}")
                except Exception as exc:
                    logger.error(f"Batch rescan FAILED: {page_url} - {exc}")
        
        # Small delay between batches (not between individual pages)
        if i + batch_size < len(just_a_moment_pages):
            delay = random.uniform(3, 5)
            logger.info(f"‚è≥ Waiting {delay:.1f} seconds before next batch...")
            time.sleep(delay)
    
    return rescanned_count
        
def monitor_advisor_with_carousel_support(advisor_id):
    """Enhanced monitoring that handles carousel content and connection errors with PARALLEL subpage processing"""
    import concurrent.futures
    
    try:
        # PHASE 1: Get advisor info (quick DB operation with lock)
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            c.execute('SELECT name, website_url FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            conn.close()
        
        if not advisor or not advisor[1]:
            return
        
        advisor_name, website_url = advisor
        monitor = CarouselAwareMonitor()
        
        # Find all pages first
        pages = monitor.find_all_pages(website_url)
        logger.info(f"üéØ Found {len(pages)} pages for {advisor_name}. Processing subpages in PARALLEL...")
        
        # PARALLEL PROCESSING FUNCTION - This does what your original loop did
        def process_single_page(page_info):
            """Process a single page (runs in parallel)"""
            i, page_url = page_info
            try:
                logger.info(f"üîÑ THREAD START [{i+1}/{len(pages)}]: {page_url}")
                # Force proxy rotation for each page
                proxy_used = get_next_proxy()
                logger.info(f"üåê Using proxy: {proxy_used}")


                # Force proxy rotation for each page (same as your original)
                get_next_proxy()  # This advances the proxy counter
                logger.info(f"üìÑ Page {i+1}/{len(pages)}: {page_url}")
                
                # EXACT SAME LOGIC AS YOUR ORIGINAL
                # Check if this page likely has carousels
                if monitor.likely_has_carousel(page_url):
                    logger.info(f"Detected carousel page, using enhanced scraping: {page_url}")
                    soup, carousel_data = monitor.scrape_carousel_content(page_url)
                    
                    if soup is None:
                        logger.warning(f"Could not scrape carousel page: {page_url}")
                        return None
                else:
                    # Use regular scraping for non-carousel pages with retry logic
                    soup = monitor.scrape_page_with_retry(page_url)
                    carousel_data = []
                
                if not soup:
                    return None
                
                # Extract regular content (same as original)
                content = monitor.extract_content(soup)
                
                # Combine regular content with carousel content (same as original)
                if carousel_data:
                    carousel_text = extract_text_from_carousel_data(carousel_data)
                    content['text'] += f"\n\nCARROUSEL CONTENT:\n{carousel_text}"

                # Store content WITHOUT title prefixes - just the clean text (same as original)
                new_full_content = content['text']  # Just the filtered text content
                content_hash = hashlib.md5(new_full_content.encode()).hexdigest()
                
                # Return data instead of updating DB immediately (DB updates happen later)
                return {
                    'page_url': page_url,
                    'content': content,
                    'new_full_content': new_full_content,
                    'content_hash': content_hash,
                    'carousel_data': carousel_data
                }
                
            except Exception as e:
                logger.error(f"Error monitoring carousel page {page_url}: {str(e)}")
                return None
        
        # PARALLEL EXECUTION - Process subpages simultaneously
        results = []
        max_workers = 8  # Reduced from 10

        logger.info(f"üöÄ Starting parallel processing with {max_workers} workers for {len(pages)} pages")

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all pages for parallel processing
            page_info_list = list(enumerate(pages))
            logger.info(f"üìù Submitting {len(page_info_list)} pages to thread pool")
    
            future_to_page = {executor.submit(process_single_page, page_info): page_info[1] 
                             for page_info in page_info_list}
    
            logger.info(f"‚úÖ All pages submitted, waiting for completion...")
            completed_count = 0
            failed_count = 0
    
            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_page):
                page_url = future_to_page[future]
                completed_count += 1
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        logger.info(f"‚úÖ [{completed_count}/{len(pages)}] SUCCESS: {page_url}")
                    else:
                        failed_count += 1
                        logger.warning(f"‚ùå [{completed_count}/{len(pages)}] FAILED (no result): {page_url}")
                except Exception as exc:
                    failed_count += 1
                    logger.error(f"‚ùå [{completed_count}/{len(pages)}] EXCEPTION: {page_url} - {exc}")

        logger.info(f"üéØ PARALLEL PROCESSING COMPLETE:")
        logger.info(f"  ‚úÖ Successful: {len(results)}")
        logger.info(f"  ‚ùå Failed: {failed_count}")
        logger.info(f"  üìä Success rate: {len(results)}/{len(pages)} ({len(results)/len(pages)*100:.1f}%)")
        
        # PHASE 1.5: UPDATE DATABASE WITH PARALLEL RESULTS (same logic as your original loop)
        for result in results:
            try:
                page_url = result['page_url']
                content = result['content']
                new_full_content = result['new_full_content']
                content_hash = result['content_hash']
                carousel_data = result['carousel_data']
                
                # Database operations with lock (EXACT SAME AS YOUR ORIGINAL)
                with db_lock:
                    conn = get_db_connection_sqlite()
                    c = conn.cursor()
                    
                    # Store carousel data separately
                    if carousel_data:
                        store_carousel_data(c, advisor_id, page_url, carousel_data)
                    
                    # Continue with your existing change detection logic
                    c.execute('SELECT content_hash, content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?',
                             (advisor_id, page_url))
                    snapshot = c.fetchone()
                    
                    if snapshot:
                        old_hash, old_content = snapshot
                        if old_hash != content_hash:
                            # Simply normalize both contents directly
                            normalized_old_content = normalize_content_for_comparison(old_content)
                            normalized_new_content = normalize_content_for_comparison(new_full_content)                               

                            # Compare normalized content to see if there's a real change
                            if normalized_old_content != normalized_new_content:
                            
                                changed_words = extract_changed_words(normalized_old_content, normalized_new_content)
                                
                                if len(normalized_new_content) > len(normalized_old_content):
                                    change_desc = "New content added to website"
                                elif len(normalized_new_content) < len(normalized_old_content):
                                    change_desc = "Content removed from website"
                                else:
                                    change_desc = "Website content modified"
                                
                                c.execute('''INSERT INTO changes 
                                            (advisor_id, change_type, before_content, after_content, description, page_url, changed_words)
                                            VALUES (?, ?, ?, ?, ?, ?, ?)''',
                                         (advisor_id, 'update', normalized_old_content, normalized_new_content, change_desc, page_url, changed_words))
                            
                            # Always update the snapshot to the new format
                            c.execute('''UPDATE website_snapshots 
                                       SET content_hash = ?, content_text = ?, page_title = ?, last_checked = CURRENT_TIMESTAMP 
                                       WHERE advisor_id = ? AND page_url = ?''',
                                     (content_hash, new_full_content, content['title'], advisor_id, page_url))
                        else:
                            c.execute('''UPDATE website_snapshots 
                                       SET last_checked = CURRENT_TIMESTAMP 
                                       WHERE advisor_id = ? AND page_url = ?''',
                                     (advisor_id, page_url))
                    else:
                        c.execute('''INSERT INTO website_snapshots (advisor_id, page_url, content_hash, page_title, content_text)
                                    VALUES (?, ?, ?, ?, ?)''',
                                 (advisor_id, page_url, content_hash, content['title'], new_full_content))
                    
                    conn.commit()
                    conn.close()
                    
            except Exception as e:
                logger.error(f"Error updating database for {result['page_url']}: {str(e)}")
        
        # PHASE 2: AUTO-RESCAN "JUST A MOMENT..." PAGES AFTER INITIAL DISCOVERY (EXACT SAME AS ORIGINAL)
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            logger.info(f"üîç PHASE 2: Auto-rescanning 'Just a moment...' pages for new advisor {advisor_name}...")
            
            # Get all pages we just scanned to find "Just a moment..." pages
            c.execute('''SELECT page_url, page_title, content_text, content_hash 
                        FROM website_snapshots 
                        WHERE advisor_id = ?''', (advisor_id,))
            all_scanned_pages = c.fetchall()
            conn.close()
        
        # Find pages with "Just a moment..." indicators (no lock needed)
        just_a_moment_pages = []
        for page_url, page_title, content_text, content_hash in all_scanned_pages:
            logger.info(f"Checking newly scanned page: {page_url}")
            logger.info(f"  Title: {page_title}")
            
            # Check for "Just a moment..." indicators
            has_just_a_moment_title = page_title and "just a moment" in page_title.lower()
            has_verification_content = content_text and any(phrase in content_text.lower() for phrase in [
                "just a moment", 
                "verifying you are human",
                "review the security of your connection",
                "verification successful",
                "waiting for"
            ])
            
            if has_just_a_moment_title or has_verification_content:
                logger.info(f"  ‚úì Found 'Just a moment...' page during initial scan: {page_url}")
                just_a_moment_pages.append((page_url, content_text, content_hash))
            else:
                logger.info(f"  ‚úó Clean page: {page_url}")
        
        # PHASE 3: AUTO-RESCAN CONNECTION ERROR PAGES (EXACT SAME AS ORIGINAL)
        logger.info(f"üîç PHASE 3: Auto-rescanning connection error pages for advisor {advisor_name}...")

        # Find pages with connection error indicators
        connection_error_pages = []
        connection_error_phrases = [
            "site can't be reached",  # Simplified matching
            "err_socket_not_connected",
            "err_connection_refused",
            "err_connection_timed_out", 
            "temporarily down",
            "might be temporarily down",
            "moved permanently",
            "new web address",
            "connection timed out",
            "website is offline"
        ]

        for page_url, page_title, content_text, content_hash in all_scanned_pages:
            # Check for connection error indicators (case insensitive)
            content_lower = (content_text or "").lower()
            title_lower = (page_title or "").lower()

            has_connection_error_title = any(phrase in title_lower for phrase in connection_error_phrases)
            has_connection_error_content = any(phrase in content_lower for phrase in connection_error_phrases)

            if has_connection_error_title or has_connection_error_content:
                logger.info(f"  ‚úì Found connection error page during initial scan: {page_url}")
                logger.info(f"    Content preview: {content_text[:100]}...")
                connection_error_pages.append((page_url, content_text, content_hash))
        
        # Auto-rescan the "Just a moment..." pages in batches
        if just_a_moment_pages:
            logger.info(f"üéØ Found {len(just_a_moment_pages)} 'Just a moment...' pages during initial advisor scan")
            rescanned_count = parallel_rescan_in_batches(monitor, just_a_moment_pages, advisor_id)
        else:
            logger.info(f"‚úÖ No 'Just a moment...' pages found during initial scan of {advisor_name}")
            rescanned_count = 0
            
        # Auto-rescan connection error pages (EXACT SAME AS ORIGINAL)
        connection_rescanned_count = 0
        if connection_error_pages:
            logger.info(f"üéØ Found {len(connection_error_pages)} connection error pages during initial advisor scan")
            
            for i, (page_url, old_content, old_hash) in enumerate(connection_error_pages, 1):
                try:
                    logger.info(f"üîÑ Auto-rescanning connection error page {i}/{len(connection_error_pages)}: {page_url}")
                    
                    # Add delay between rescans to avoid rate limiting
                    if i > 1:
                        delay_time = random.uniform(3, 6)
                        logger.info(f"‚è≥ Waiting {delay_time:.1f} seconds to avoid rate limiting...")
                        time.sleep(delay_time)
                    
                    # Scrape the page again with retry logic
                    soup = monitor.scrape_page_with_retry(page_url)
                    
                    if soup:
                        content = monitor.extract_content(soup)
                        new_full_content = content['text']
                        new_content_hash = hashlib.md5(new_full_content.encode()).hexdigest()
                        
                        # Check if we actually got real content this time
                        still_has_error = any(phrase in new_full_content.lower() for phrase in connection_error_phrases)
                        
                        if not still_has_error:
                            # Update the database with the real content
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET content_hash = ?, content_text = ?, page_title = ?, last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (new_content_hash, new_full_content, content['title'], advisor_id, page_url))
                                conn.commit()
                                conn.close()
                            
                            connection_rescanned_count += 1
                            logger.info(f"‚úÖ Successfully rescanned connection error page: {page_url}")
                            logger.info(f"   New title: {content['title']}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Connection error persists for page: {page_url}")
                    else:
                        logger.warning(f"‚ùå Failed to rescan connection error page: {page_url}")
                    
                except Exception as e:
                    logger.error(f"Error auto-rescanning connection error page {page_url}: {str(e)}")
                    continue
        else:
            logger.info(f"‚úÖ No connection error pages found during initial scan of {advisor_name}")
        
        logger.info(f"üéØ Initial scan complete for {advisor_name}: Rescanned {rescanned_count}/{len(just_a_moment_pages)} blocked pages, {connection_rescanned_count}/{len(connection_error_pages)} connection error pages")
        
    except Exception as e:
        logger.error(f"Error in enhanced carousel monitoring: {str(e)}")
        
# Update your existing monitor_advisor function to use the new enhanced version
def monitor_advisor(advisor_id):
    """Monitor a single advisor's website with carousel support"""
    return monitor_advisor_with_carousel_support(advisor_id)


def extract_changed_words(old_content, new_content, max_words=20):
    """Extract only the words that were added or changed"""
    try:
        old_words = old_content.split()
        new_words = new_content.split()
        
        differ = difflib.SequenceMatcher(None, old_words, new_words)
        changed_words = []
        
        for tag, i1, i2, j1, j2 in differ.get_opcodes():
            if tag == 'replace':
                changed_words.extend(['REMOVED: ' + ' '.join(old_words[i1:i2])])
                changed_words.extend(['ADDED: ' + ' '.join(new_words[j1:j2])])
            elif tag == 'delete':
                changed_words.extend(['REMOVED: ' + ' '.join(old_words[i1:i2])])
            elif tag == 'insert':
                changed_words.extend(['ADDED: ' + ' '.join(new_words[j1:j2])])
        
        if len(changed_words) > max_words:
            changed_words = changed_words[:max_words] + ['...']
        
        return ' | '.join(changed_words) if changed_words else 'Minor changes detected'
        
    except Exception as e:
        logger.error(f"Error extracting changed words: {str(e)}")
        return 'Changes detected'


def extract_problematic_words_from_rationale(flagged_instance, rationale):
    """Extract problematic words/phrases from the AI rationale and highlight them"""
    if not rationale:
        return flagged_instance
    
    # Patterns to extract quoted phrases from rationale
    patterns = [
        r'contains language that may be considered absolute or promissory.*?"([^"]+)"',
        r'the word[s]?\s+"([^"]+)"',
        r'phrase[s]?\s+"([^"]+)"', 
        r'statement[s]?\s+"([^"]+)"',
        r'language.*?"([^"]+)".*?absolute',
        r'"([^"]+)".*?without proper qualifiers',
        r'"([^"]+)".*?absolute.*?claim'
    ]
    
    problematic_phrases = []
    
    # Extract phrases from rationale
    for pattern in patterns:
        matches = re.findall(pattern, rationale, re.IGNORECASE)
        problematic_phrases.extend(matches)
    
    # Remove duplicates and sort by length
    unique_phrases = list(set(problematic_phrases))
    unique_phrases.sort(key=len, reverse=True)
    
    highlighted_text = flagged_instance
    
    # Highlight each phrase found in the flagged instance
    for phrase in unique_phrases:
        if phrase and phrase.lower() in flagged_instance.lower():
            pattern = re.compile(re.escape(phrase), re.IGNORECASE)
            highlighted_text = pattern.sub(
                f'<span class="flagged-word">{phrase}</span>', 
                highlighted_text
            )
    
    return highlighted_text

# Initilize sketicism words
def initialize_skepticism_words():
    """Initialize skepticism words at startup"""
    global SKEPTICISM_WORDS
    try:
        skepticism_file = os.path.join(os.path.dirname(__file__), 'skepticism_words.json')
        if os.path.exists(skepticism_file):
            with open(skepticism_file, 'r') as f:
                data = json.load(f)
                SKEPTICISM_WORDS = data.get('words', [])
        else:
            # Create default file if it doesn't exist
            default_words = ["ensure", "minimize", "maximize"]
            with open(skepticism_file, 'w') as f:
                json.dump({"words": default_words}, f, indent=4)
            SKEPTICISM_WORDS = default_words
        
        logger.info(f"Loaded {len(SKEPTICISM_WORDS)} skepticism words: {SKEPTICISM_WORDS}")
    except Exception as e:
        logger.error(f"Error loading skepticism words: {e}")
        SKEPTICISM_WORDS = ["ensure", "minimize", "maximize"]

# Load skepticism words from JSON file
def load_skepticism_words():
    """Load words that should trigger higher skepticism from JSON file."""
    try:
        skepticism_file = os.path.join(os.path.dirname(__file__), 'skepticism_words.json')
        if os.path.exists(skepticism_file):
            with open(skepticism_file, 'r') as f:
                data = json.load(f)
                return data.get('words', [])
        else:
            # Create default file if it doesn't exist
            default_words = ["ensure", "minimize", "maximize"]
            with open(skepticism_file, 'w') as f:
                json.dump({"words": default_words}, f, indent=4)
            return default_words
    except Exception as e:
        logger.error(f"Error loading skepticism words: {e}")
        return ["ensure", "minimize", "maximize"]

# Skepticism Logging
def log_skepticism_analysis(text):
    """Log analysis of skepticism words in text"""
    skepticism_words = load_skepticism_words()
    found_words = get_skepticism_words_in_text(text)
    
    if found_words:
        logger.info(f"SKEPTICISM ANALYSIS:")
        logger.info(f"  Text: {text[:100]}...")
        logger.info(f"  Found skepticism words: {found_words}")
        logger.info(f"  Available skepticism words: {skepticism_words}")
    else:
        logger.debug(f"No skepticism words found in: {text[:50]}...")

def contains_skepticism_words(text):
    """Check if text contains any skepticism words or their variants"""
    if not text:
        return False
    
    skepticism_words = load_skepticism_words()
    text_lower = text.lower()
    
    for word in skepticism_words:
        word_lower = word.lower()
        
        # Check exact match
        if word_lower in text_lower:
            logger.info(f"Found skepticism word '{word}' (exact match) in text: {text[:50]}...")
            return True
        
        # Check common variants
        variants = generate_word_variants(word_lower)
        for variant in variants:
            if variant in text_lower:
                logger.info(f"Found skepticism word '{word}' (variant: '{variant}') in text: {text[:50]}...")
                return True
    
    return False

def generate_word_variants(word):
    """Generate common variants of a word"""
    variants = [word]  # Start with the original word
    
    # Add common suffixes
    suffixes = ['s', 'es', 'ed', 'ing', 'er', 'est', 'tion', 'sion', 'ment', 'ly', 'ize', 'ise']
    
    for suffix in suffixes:
        variants.append(word + suffix)
    
    # Handle words ending in 'e' (like optimize -> optimized)
    if word.endswith('e'):
        base = word[:-1]  # Remove the 'e'
        variants.extend([
            base + 'ed',    # optimize -> optimized
            base + 'ing',   # optimize -> optimizing
            base + 'er',    # optimize -> optimizer
            base + 'es',    # optimize -> optimizes
        ])
    
    # Handle words ending in 'y' (like modify -> modified)
    if word.endswith('y'):
        base = word[:-1]  # Remove the 'y'
        variants.extend([
            base + 'ied',   # modify -> modified
            base + 'ies',   # modify -> modifies
            base + 'ying',  # modify -> modifying
        ])
    
    # Handle doubled consonants (like plan -> planned, but not for all words)
    consonants = 'bcdfghjklmnpqrstvwxz'
    if len(word) >= 3 and word[-1] in consonants and word[-2] not in consonants:
        doubled = word + word[-1]
        variants.extend([
            doubled + 'ed',   # plan -> planned
            doubled + 'ing',  # plan -> planning
            doubled + 'er',   # plan -> planner
        ])
    
    return variants

def get_skepticism_words_in_text(text):
    """Get list of skepticism words found in text"""
    if not text:
        return []
    
    skepticism_words = load_skepticism_words()
    text_lower = text.lower()
    found_words = []
    
    for word in skepticism_words:
        if word.lower() in text_lower:
            found_words.append(word)
    
    return found_words

def apply_skepticism_word_fixes(alternative_text, original_text):
    """Apply better fixes for skepticism words using AI"""
    skepticism_words = load_skepticism_words()
    
    # Check if original contained skepticism words
    original_lower = original_text.lower()
    found_skepticism_words = [word for word in skepticism_words if word.lower() in original_lower]
    
    if found_skepticism_words:
        # Check if the alternative still contains the same skepticism words
        alternative_lower = alternative_text.lower()
        still_has_skepticism_words = [word for word in found_skepticism_words if word.lower() in alternative_lower]
        
        if still_has_skepticism_words:
            logger.info(f"Alternative still contains skepticism words: {still_has_skepticism_words} - generating better fix")
            
            evaluation_prompt = f"""Original: "{original_text}"
Current alternative: "{alternative_text}"

The alternative still contains these problematic words: {', '.join(still_has_skepticism_words)}

These words from our compliance list should be replaced entirely with different words that mean something similar but are less absolute/promissory: {', '.join(skepticism_words)}

Replace any of these words that appear in the alternative with natural alternatives that convey similar meaning but sound less like guarantees or promises.

DO NOT add any disclaimers, risk warnings, or compliance language like "results cannot be guaranteed" or "though individual results may vary". Just fix the problematic words and keep it natural and conversational.

Return ONLY the corrected text with the problematic words replaced, nothing more."""

            try:
                improved = call_deepseek_api(evaluation_prompt).strip().strip('"')
                if improved and len(improved) > 10:
                    logger.info(f"Fixed skepticism words: {alternative_text} ‚Üí {improved}")
                    return improved
            except Exception as e:
                logger.error(f"Error fixing skepticism words: {e}")
    
    return alternative_text


# New function to pregenerate alternatives in a background thread
def start_alternative_pregeneration_thread():
    """Start a background thread that monitors the queue and pregenerates alternatives"""
    global pregeneration_active
    
    def pregeneration_worker():
        global pregeneration_active
        logger.info("Alternative pregeneration worker started")
        
        while True:
            try:
                # Get an item from the queue with timeout (allows thread to check if it should exit)
                try:
                    flagged_instance = pregeneration_queue.get(timeout=5)
                except queue.Empty:
                    # No items in queue, check if we should continue running
                    if not pregeneration_active:
                        logger.info("Pregeneration worker shutting down")
                        break
                    continue
                
                # Process the flagged instance
                flagged_text = flagged_instance.get("flagged_instance", "")
                if not flagged_text:
                    pregeneration_queue.task_done()
                    continue
                
                # Create cache key using the same approach as in generate_new_alternative
                simplified_text = re.sub(r'\s+', ' ', flagged_text.lower()).strip()
                cache_key = hashlib.md5(f"alt_{simplified_text}".encode('utf-8')).hexdigest()
                
                # Check if we already have alternatives for this instance
                with cache_lock:
                    if cache_key in alternative_cache and len(alternative_cache[cache_key]) > 0:
                        logger.info(f"Alternatives already exist for {cache_key[:8]}, skipping")
                        pregeneration_queue.task_done()
                        continue
                
                # Generate 3 alternatives
                logger.info(f"Pregenerating alternatives for: {flagged_text[:50]}...")
                prompt = f"""
Generate 3 FINRA-compliant alternatives for this non-compliant financial text and write to sound more natural and human while being FINRA-compliant:
"{flagged_text}"

FOCUS MAINLY ON adjusting only the specific part that makes the instance non-compliant.

Make it:
- Conversational and natural
- Free of robotic/legal language
- Compliant but approachable
- Don't include disclaimers or risk warnings

Format as:
ALT1: [First alternative]
ALT2: [Second alternative]
ALT3: [Third alternative]

Only the alternatives, no other text.
"""
                try:
                    api_response = call_deepseek_api(prompt)
                    
                    # Parse the response to extract alternatives
                    alternatives = []
                    
                    # Look for ALT1, ALT2, ALT3 format
                    alt_pattern = re.compile(r'ALT\d+:\s*(.*?)(?=ALT\d+:|$)', re.DOTALL)
                    matches = alt_pattern.findall(api_response)
                    
                    alternatives = [match.strip() for match in matches if match.strip()]
                    
                    # If parsing failed, use simple newline splitting as fallback
                    if not alternatives:
                        lines = [line.strip() for line in api_response.strip().split('\n') 
                                if line.strip() and not line.strip().startswith(('#', 'Alternative'))]
                        alternatives = [re.sub(r'^\d+\.\s*', '', line) for line in lines]
                    
                    # Filter out any too short or too long alternatives
                    alternatives = [alt for alt in alternatives if len(alt) >= 10 and len(alt) <= len(flagged_text) * 2]
                    
                    # Ensure we have at least one alternative
                    if not alternatives:
                        alternatives = ["We may help clients pursue their investment objectives while understanding that all investments involve risk."]
                    
                    # Save the alternatives to cache
                    with cache_lock:
                        alternative_cache[cache_key] = alternatives
                        alternative_index[cache_key] = 0  # Initialize index
                    
                    logger.info(f"Successfully pregenerated {len(alternatives)} alternatives for {cache_key[:8]}")
                    
                except Exception as e:
                    logger.error(f"Error pregenerating alternatives: {e}")
                
                # Mark task as done
                pregeneration_queue.task_done()
                
            except Exception as e:
                logger.error(f"Error in pregeneration worker: {e}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                # Continue running despite errors
    
    # Start the worker thread if it's not already running
    if not pregeneration_active:
        pregeneration_active = True
        threading.Thread(target=pregeneration_worker, daemon=True).start()

# Add a function to queue flagged instances for pregeneration
def queue_alternatives_pregeneration(flagged_instances):
    """Queue flagged instances for alternative pregeneration"""
    if not flagged_instances:
        return
    
    # Ensure the pregeneration thread is running
    start_alternative_pregeneration_thread()
    
    # Queue all flagged instances
    for instance in flagged_instances:
        if isinstance(instance, dict) and "flagged_instance" in instance:
            # Add to queue
            pregeneration_queue.put(instance)
            logger.info(f"Queued flagged instance for pregeneration: {instance['flagged_instance'][:50]}...")

# Add function to check for cache status
def check_alternatives_cache(flagged_text):
    """Check if alternatives exist in cache for a given flagged text"""
    if not flagged_text:
        return False, 0
    
    simplified_text = re.sub(r'\s+', ' ', flagged_text.lower()).strip()
    cache_key = hashlib.md5(f"alt_{simplified_text}".encode('utf-8')).hexdigest()
    
    with cache_lock:
        has_alternatives = cache_key in alternative_cache and len(alternative_cache[cache_key]) > 0
        count = len(alternative_cache.get(cache_key, [])) if has_alternatives else 0
    
    return has_alternatives, count


def handle_cross_page_sentences(text_by_page):
    """
    Process text extracted by pages and handle sentences that span across page boundaries.
    
    Args:
        text_by_page: List of dictionaries with 'page' and 'text' keys
        
    Returns:
        List of dictionaries with 'page' and 'text' keys with cross-page sentences properly joined
    """
    if not text_by_page or len(text_by_page) <= 1:
        return text_by_page  # Nothing to join if only one page or empty
    
    processed_pages = []
    current_text = text_by_page[0]['text']
    current_page = text_by_page[0]['page']
    
    for i in range(1, len(text_by_page)):
        prev_page_text = current_text
        current_page_text = text_by_page[i]['text']
        next_page = text_by_page[i]['page']
        
        # Check if the previous page ends without proper sentence termination
        ends_with_terminator = re.search(r'[.!?]\s*$', prev_page_text) is not None
        
        if not ends_with_terminator:
            # If previous page doesn't end with sentence terminator,
            # check if the current page starts with lowercase letter or continuation
            starts_with_lowercase = re.search(r'^\s*[a-z]', current_page_text) is not None
            
            if starts_with_lowercase:
                # Likely a sentence break - join the texts
                current_text = current_text.rstrip() + " " + current_page_text.lstrip()
            else:
                # Add the completed page to our results
                processed_pages.append({'page': current_page, 'text': current_text})
                current_text = current_page_text
                current_page = next_page
        else:
            # Previous page ends properly, add it to processed pages
            processed_pages.append({'page': current_page, 'text': current_text})
            current_text = current_page_text
            current_page = next_page
    
    # Add the last page
    processed_pages.append({'page': current_page, 'text': current_text})
    
    return processed_pages


# Add a request tracking system
active_requests = {}
request_lock = threading.Lock()

def register_request():
    """Register a new request and return a request ID"""
    request_id = str(uuid.uuid4())
    with request_lock:
        active_requests[request_id] = False  # False = not complete
    return request_id

def mark_request_complete(request_id):
    """Mark a request as complete"""
    with request_lock:
        if request_id in active_requests:
            active_requests[request_id] = True

def is_request_complete(request_id):
    """Check if a request is complete"""
    with request_lock:
        return active_requests.get(request_id, True)  # Default to True if not found


# Create a custom logger for your application
logger = logging.getLogger('app_logger')
logger.setLevel(logging.INFO)  # Default level

# Create handler for production environment
if os.environ.get('ENVIRONMENT') == 'production':
    logger.setLevel(logging.WARNING)  # Less verbose in production

# Ensure no duplicate handlers are added
if not logger.handlers:
    # Create handlers
    file_handler = logging.FileHandler('uploads/app.log')
    file_handler.setLevel(logging.INFO)

    console_handler = StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)

    # Create a formatter and set it for both handlers
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # Add handlers to the logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

# Suppress Flask's default logging to prevent duplicates
flask_logger = logging.getLogger('werkzeug')
flask_logger.setLevel(logging.WARNING)



# Initialize connection pool
try:
    connection_pool = pool.SimpleConnectionPool(
        1, 10,  # min connections, max connections
        dbname="finalyze_postgres_instance_cwhb",  # Fixed: add _cwhb suffix
        user="finalyze_postgres_instance_user",
        password="xVb0aYEvPsahBJltKU25yMt7b2Jn9p07",
        host="dpg-d2s9ll95pdvs73e82pug-a.ohio-postgres.render.com",  # Fixed: add full domain
        port="5432",
        sslmode="require"
    )
    logger.info("Database connection pool initialized successfully")
except Exception as e:
    logger.error(f"Error initializing connection pool: {e}")
    connection_pool = None

def get_db_connection():
    """Get a connection from the pool"""
    if connection_pool:
        return connection_pool.getconn()
    else:
        # Fallback to direct connection if pool initialization failed
        return psycopg2.connect(
            dbname="finalyze_postgres_instance_cwhb",  # Fixed: add _cwhb suffix
            user="finalyze_postgres_instance_user",
            password="xVb0aYEvPsahBJltKU25yMt7b2Jn9p07",
            host="dpg-d2s9ll95pdvs73e82pug-a.ohio-postgres.render.com",  # Fixed: add full domain
            port="5432",
            sslmode="require"
        )

def release_db_connection(conn):
    """Return a connection to the pool"""
    if connection_pool and conn:
        connection_pool.putconn(conn)
        



def initialize_bert():
    """Initialize BERT model and tokenizer"""
    global BERT_MODEL, BERT_TOKENIZER
    try:
        # Check model file details
        model_path = 'finra_compliance_model.pth'
        if os.path.exists(model_path):
            modified_time = datetime.fromtimestamp(os.path.getmtime(model_path))
            file_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB
            logger.info(f"Loading model file: {model_path}")
            logger.info(f"Last modified: {modified_time}")
            logger.info(f"File size: {file_size:.2f} MB")
        
        # 1. Load the tokenizer from your saved tokenizer directory
        BERT_TOKENIZER = BertTokenizer.from_pretrained('finra_tokenizer')  # Path to your saved tokenizer

        # 2. Initialize the base model first
        BERT_MODEL = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

        # 3. Load your trained weights
        BERT_MODEL.load_state_dict(torch.load('finra_compliance_model.pth'))  # Path to your saved model

        # 4. Set to evaluation mode
        BERT_MODEL.eval()
        return True
    except Exception as e:
        logger.error(f"Error initializing BERT: {e}")
        return False

# Global variables for BERT model
BERT_MODEL = None
BERT_TOKENIZER = None

# Sendgrid Key
os.environ['SENDGRID_API_KEY'] = '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'

# Delete "Sendgrid Key" Above when moving to Web
#os.environ['SENDGRID_API_KEY'] = os.environ.get('SENDGRID_API_KEY', '')


# MY CHATGPT API KEY
#openai.api_key = "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"

# My Deepseek API Key
DEEPSEEK_API_KEY = "sk-236ec31139a5435fa2b4720c53601c09"

# Delete "Deepseek API Key" above when moving to web version
#DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY', '')

DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"  # adjust based on actual Deepseek API endpoint


# MY ANTHROPIC KEY

SYSTEM_MESSAGE = """You are a compliance assistant specializing in FINRA's social media communication rules and regulations. Analyze the following text for compliance with these EXACT rules:

RULE 1: All communications must be fair, balanced, and complete and not omit material information.
Violations include:
- Presenting benefits without equal emphasis on risks
- Omitting key product/strategy limitations
- Discussing returns without mentioning potential losses
- Highlighting advantages without corresponding disadvantages
- Selective disclosure of performance periods

RULE 2: False, misleading, promissory, exaggerated, or unwarranted statements/claims are prohibited.
Violations include:
- "Will" statements that promise outcomes
- Guarantees of any kind
- Absolute statements ("always," "never," "best")
- Unsubstantiated claims about performance
- Promises of specific results

RULE 3: Material information may not be buried in footnotes.
Violations include:
- Important disclosures in smaller text
- Key risks relegated to footnotes
- Critical information placed inconspicuously
- Material facts separated from main claims

RULE 4: Statements must be clear and provide balanced treatment of risks/benefits.
Violations include:
- Complex terminology without explanation
- Understated risks
- Overstated benefits
- Imbalanced risk-reward presentations
- Unclear or ambiguous statements

RULE 5: Communications must be appropriate for the audience.
Violations include:
- Technical jargon without explanation
- Complex strategies without context
- Inappropriate risk levels
- Unsuitable recommendations
- Mismatched complexity

EXCEPTIONS FOR PROCESS LANGUAGE:
- Statements about working relationships ("we'll work with you", "we'll collaborate", "will you have enough money?") 
  are compliant when they describe process or a question rather than outcomes

CRITICAL: The word "ensure" is RARELY compliant when referring to financial outcomes (see "The Key Distinction" below:
- "Planning to ensure income" = NON-COMPLIANT
- "We ensure returns" = NON-COMPLIANT  
- "Ensuring your financial future" = NON-COMPLIANT

- "My mother ensured me that I would be okay" = COMPLIANT
- "Our process ensures client confidentiality is maintained" = COMPLIANT
- "We ensure prompt response to client inquiries" = COMPLIANT

The Key Distinction:
‚ùå NON-COMPLIANT (Financial Outcomes):

"ensure sufficient income"
"ensure positive returns"
"ensure your financial security"
"ensure profitable investments"
"ensure retirement funds will last"

‚úÖ COMPLIANT (Processes/Procedures):

"ensure proper documentation"
"ensure regulatory compliance"
"ensure timely execution"
"ensure accurate reporting"

ANALYSIS INSTRUCTIONS:
1. Compare text EXACTLY against these rules
2. Flag ANY instance that matches the violation examples
3. For each flagged instance, provide:
   - The exact non-compliant text
   - Which specific rule it violates
   - A clear rationale
   - A compliant alternative

RESPONSE FORMAT:
For each violation, respond in this exact format:
---
Flagged Instance: "[exact text]"
Rule Violated: [specific rule number and description]
Compliance Status: [Non-Compliant/Partially Compliant]
Specific Compliant Alternative: "[alternative text]"
Rationale: "[explanation of violation and why alternative complies]"
---

ALTERNATIVE TEXT REQUIREMENTS:
- Remove absolute statements
- Add appropriate qualifiers
- Balance risk and reward
- Maintain original meaning
- Keep similar length
- Use clear language

If no violations are found, respond ONLY with:
"Compliance Check Completed. No issues found."

IMPORTANT: Be consistent in flagging similar violations across different texts. When in doubt, flag the instance."""

# Initialize client
#openai.api_key = "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"  # Replace with your actual OpenAI API key




processed_files = {}  # Change from set() to dictionary


# Postgre for loading disclosures implemented on 4/19
def load_disclosures():
    """Load disclosures from PostgreSQL database."""
    disclosures = {}
    conn = None
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Query the disclosures table
        cursor.execute("SELECT category, disclosure_text FROM disclosures;")
        rows = cursor.fetchall()
        
        for row in rows:
            category = row[0]
            disclosure_text = row[1]
            disclosures[category] = disclosure_text
        
        cursor.close()
        
        logger.debug(f"Loaded {len(disclosures)} disclosures from database")
        return disclosures
    except Exception as e:
        logger.error(f"Error loading disclosures from database: {e}")
        # Fallback disclosures
        return {
            "mutual fund": "Mutual fund investing involves risk; principal loss is possible.",
            "index fund": "Index funds are subject to market risk, including the possible loss of principal.",
            "performance": "Past performance does not guarantee future results.",
            "investing": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
            "investment": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
            "market": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
            "funds": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
            "returns": "Past performance is no guarantee of future results."
        }
    finally:
        if conn:
            release_db_connection(conn)

# PROHIBITED WORDS (in control panel)
def load_prohibited_words_db():
    """Load prohibited words from PostgreSQL only (no JSON). Returns a list of words."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT word FROM prohibited_words;")
        rows = cursor.fetchall()
        cursor.close()

        # Convert rows ‚Üí list of words
        words = [row[0] for row in rows]
        logger.debug(f"Loaded {len(words)} prohibited words from database")
        return words
    except Exception as e:
        logger.debug(f"Error loading prohibited words from DB: {e}")
        return []
    finally:
        if conn:
            release_db_connection(conn)


# Function to clean the FINRA compliance text by removing embedded numbering
def clean_finra_analysis(text):

    # This will remove any leading numbers followed by a period (e.g., 1., 2., etc.)
    cleaned_text = re.sub(r'^\d+\.\s*', '', text, flags=re.MULTILINE)
    return cleaned_text




# Function for pdf text extraction by page
def extract_text_from_pdf(pdf_file_path):
    try:
        with pdfplumber.open(pdf_file_path) as pdf:
            page_text = []
            for i, page in enumerate(pdf.pages, start=1):
                text = page.extract_text()
                if text:
                    page_text.append({"page": i, "text": text})  # Include page number and text
            return page_text
    except Exception as e:
        logger.info(f"Error extracting text from PDF: {e}")
        return []



def segment_transcription_into_sentences(transcribed_text):
    """
    Split transcribed text into properly formatted sentences with focus on natural flow.
    """
    # Clean the text
    text = transcribed_text.strip()
    
    if not text:
        return []
    
    # First, handle existing punctuation properly
    # This makes sure we preserve any actual sentence breaks in the transcription
    text = re.sub(r'([.!?])\s*([a-z])', r'\1 \2', text)  # Ensure space after periods
    
    # Fix common speech recognition issues:
    # 1. Remove unnecessary periods that often appear in speech recognition
    text = re.sub(r'\b(mr|mrs|ms|dr)\.\s', r'\1 ', text, flags=re.IGNORECASE)  # Remove title periods
    text = re.sub(r'\s+right\.\s+now', r' right now', text, flags=re.IGNORECASE)  # Fix "right. now"
    
    # 2. Add proper periods at clear sentence boundaries
    text = re.sub(r'([.!?])\s+([A-Z])', r'\1 \2', text)  # Ensure sentence boundaries are preserved
    
    # 3. Add periods at clear transition points
    transitions = [
        r'(\w+)\s+(So today|Anyway|Alright|Now let\'s)\b',
        r'(\w+)\s+(Let me|I want to|We need to)\b',
        r'(\w+[\.\!\?]*)\s+(Meanwhile|Furthermore|However|Therefore|Thus|Hence)\b'
    ]
    for pattern in transitions:
        text = re.sub(pattern, r'\1. \2', text, flags=re.IGNORECASE)
    
    # 4. Mark clear standalone interjections
    interjections = [r'\b(No|Yes|Yeah|Hey|Hi|Hello|Thanks|Thank you)\b\s+([A-Z])']
    for pattern in interjections:
        text = re.sub(pattern, r'\1. \2', text, flags=re.IGNORECASE)
    
    # Use a simplified spaCy approach if available
    sentences = []
    if 'nlp' in globals():
        try:
            # Process with spaCy and apply minimal post-processing
            doc = nlp(text)
            for sent in doc.sents:
                sent_text = sent.text.strip()
                if not sent_text:
                    continue
                
                # Apply basic sentence formatting
                if not sent_text[-1] in ['.', '!', '?']:
                    sent_text += '.'
                sent_text = sent_text[0].upper() + sent_text[1:] if sent_text and sent_text[0].islower() else sent_text
                
                # Only break very long sentences (>30 words)
                if len(sent_text.split()) > 30:
                    parts = simple_break_long_sentence(sent_text)
                    sentences.extend(parts)
                else:
                    sentences.append(sent_text)
            
            if len(sentences) >= 2:
                logger.info(f"Used spaCy to segment text into {len(sentences)} sentences")
                return sentences
        except Exception as e:
            logger.error(f"Error using spaCy for sentence segmentation: {e}")
    
    # Fallback approach if spaCy isn't available or didn't work as expected
    
    # Manual sentence detection with careful boundary handling
    # This simpler approach looks for sentence boundaries without over-processing
    manual_sentences = []
    
    # Split by sentence endings
    raw_segments = re.split(r'(?<=[.!?])\s+', text)
    segments = []
    
    # Clean up segments and handle potential false breaks
    current_segment = ""
    for segment in raw_segments:
        segment = segment.strip()
        if not segment:
            continue
            
        # Check if this looks like a false break (e.g., "Mr. Smith")
        if current_segment and re.search(r'\b(mr|mrs|ms|dr|inc|ltd|co|etc|vs|fig|jan|feb|mar|apr|jun|jul|aug|sep|oct|nov|dec)\.$', 
                                          current_segment, re.IGNORECASE):
            current_segment += " " + segment
        else:
            if current_segment:
                segments.append(current_segment)
            current_segment = segment
    
    # Don't forget the last segment
    if current_segment:
        segments.append(current_segment)
    
    # Process each cleaned segment
    for segment in segments:
        if len(segment.split()) <= 25:
            # For normal length segments, just format and add
            if not segment[-1] in ['.', '!', '?']:
                segment += '.'
            segment = segment[0].upper() + segment[1:] if segment and segment[0].islower() else segment
            manual_sentences.append(segment)
        else:
            # For longer segments, try to break at natural points
            parts = simple_break_long_sentence(segment)
            manual_sentences.extend(parts)
    
    # Final safety check
    if not manual_sentences and text:
        formatted_text = text
        if not formatted_text[-1] in ['.', '!', '?']:
            formatted_text += '.'
        formatted_text = formatted_text[0].upper() + formatted_text[1:] if formatted_text and formatted_text[0].islower() else formatted_text
        manual_sentences.append(formatted_text)
    
    logger.info(f"Segmented text into {len(manual_sentences)} sentences using manual approach")
    return manual_sentences

def simple_break_long_sentence(long_text):
    """Simplified approach to break long sentences at natural pause points."""
    result = []
    
    # Try to break at clear pause points - conjunctions with capital letter following
    text = re.sub(r'(and|but|or|so)\s+([A-Z])', r'\1. \2', long_text)
    
    # Split at sentence endings
    parts = re.split(r'(?<=[.!?])\s+', text)
    
    current_part = ""
    word_count = 0
    
    for part in parts:
        part = part.strip()
        if not part:
            continue
            
        part_words = len(part.split())
        
        # If this part is already long enough to be its own sentence
        if part_words >= 10:
            # Add current accumulation if any
            if current_part:
                if not current_part[-1] in ['.', '!', '?']:
                    current_part += '.'
                current_part = current_part[0].upper() + current_part[1:] if current_part and current_part[0].islower() else current_part
                result.append(current_part)
                current_part = ""
                word_count = 0
            
            # Format this part
            if not part[-1] in ['.', '!', '?']:
                part += '.'
            part = part[0].upper() + part[1:] if part and part[0].islower() else part
            result.append(part)
        else:
            # If adding this would make current_part too long
            if word_count + part_words > 25:
                # Finish current part
                if current_part:
                    if not current_part[-1] in ['.', '!', '?']:
                        current_part += '.'
                    current_part = current_part[0].upper() + current_part[1:] if current_part and current_part[0].islower() else current_part
                    result.append(current_part)
                
                # Start new with this part
                current_part = part
                word_count = part_words
            else:
                # Add to current part
                if current_part:
                    current_part += " " + part
                else:
                    current_part = part
                word_count += part_words
    
    # Don't forget any remaining text
    if current_part:
        if not current_part[-1] in ['.', '!', '?']:
            current_part += '.'
        current_part = current_part[0].upper() + current_part[1:] if current_part and current_part[0].islower() else current_part
        result.append(current_part)
    
    return result


def ai_segment_transcription(text, max_chunk_size=5000):
    """
    Segment transcription into sentences using DeepSeek AI.
    Processes text in chunks to avoid timeouts with large transcriptions.
    
    Args:
        text (str): The transcription text to segment
        max_chunk_size (int): Maximum size of each chunk in characters
        
    Returns:
        list: List of segmented sentences
    """
    if not text or not text.strip():
        return []
    
    # If text is short enough, process it directly
    if len(text) <= max_chunk_size:
        try:
            return _process_text_with_deepseek(text)
        except Exception as e:
            logger.error(f"DeepSeek AI sentence segmentation failed: {e}")
            # Fallback to regex-based sentence splitting
            return simple_sentence_split(text)
    
    # For longer text, split into chunks with overlap to ensure sentence continuity
    chunks = []
    overlap = 100  # Characters to overlap between chunks
    start = 0
    
    logger.info(f"Text too large ({len(text)} chars), splitting into chunks of {max_chunk_size} chars")
    
    while start < len(text):
        end = min(start + max_chunk_size, len(text))
        
        # If we're not at the end, try to find a good break point
        if end < len(text):
            # Try to find sentence-ending punctuation for a clean break
            break_point = find_break_point(text, max(start, end - overlap), end)
            chunk = text[start:break_point]
            start = break_point
        else:
            chunk = text[start:end]
            start = end
            
        chunks.append(chunk)
    
    logger.info(f"Split text into {len(chunks)} chunks for processing")
    
    # Process each chunk and combine results
    all_sentences = []
    for i, chunk in enumerate(chunks):
        logger.info(f"Processing chunk {i+1}/{len(chunks)}: {len(chunk)} chars")
        try:
            sentences = _process_text_with_deepseek(chunk)
            logger.info(f"Chunk {i+1}: got {len(sentences)} sentences from DeepSeek")
            all_sentences.extend(sentences)
        except Exception as e:
            logger.error(f"DeepSeek AI chunk segmentation failed: {e}")
            sentences = simple_sentence_split(chunk)
            logger.info(f"Chunk {i+1}: used fallback to get {len(sentences)} sentences")
            all_sentences.extend(sentences)
    
    logger.info(f"Total sentences after processing all chunks: {len(all_sentences)}")
    return all_sentences

def _process_text_with_deepseek(text):
    """Process a single chunk of text with DeepSeek AI"""
    prompt = f"""
You are a transcription editor. Your job is to take raw, unpunctuated or poorly segmented transcript text and break it into clean, properly punctuated, natural-sounding sentences.

Here is the raw transcription text:

{text.strip()}

Please return the same text with:
- Accurate sentence boundaries
- Proper capitalization and punctuation
- Natural rhythm and flow of spoken English
- No added content, just clean formatting
- Do not show "Here's the cleaned up version"

Only return the corrected text.
""".strip()

    response = requests.post(
        "https://api.deepseek.com/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        },
        json={
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3
        },
        timeout=30
    )

    response.raise_for_status()
    result = response.json()
    ai_response = result["choices"][0]["message"]["content"].strip()
    return ai_response.split("\n\n")

def simple_sentence_split(text):
    """Simple regex-based sentence splitting as fallback"""
    # Split on common sentence-ending punctuation followed by space or newline
    sentences = re.split(r'(?<=[.!?])\s+', text)
    # Filter out empty strings and strip whitespace
    return [s.strip() for s in sentences if s.strip()]

def find_break_point(text, start, end):
    """Find a good break point (end of sentence) between start and end positions"""
    # Look for sentence-ending punctuation
    for i in range(end, start, -1):
        if i < len(text) and i > 0 and text[i-1] in '.!?' and (i == len(text) or text[i].isspace()):
            return i
    
    # If no good break point found, just use the end position
    return end

    

# Add this function for MP4 transcription after your pdf text extraction functions
def transcribe_mp4(video_file_path):
    """Extract audio from MP4 and convert it to text using speech recognition with chunking for large files"""
    try:
        # Create a temporary directory for the audio file
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "extracted_audio.wav")
        
        # Log file size
        file_size = os.path.getsize(video_file_path)
        logger.info(f"Processing MP4 file: {video_file_path}, size: {file_size/1024/1024:.2f} MB")
        
        # Extract audio from video
        logger.info(f"Extracting audio from {video_file_path}")
        
        try:
            # Create video clip
            video = mp.VideoFileClip(video_file_path)
            
            # Check if audio exists
            if video.audio is None:
                logger.warning(f"No audio stream found in {video_file_path}")
                return [{"page": 1, "text": "No audio detected in the video file."}]
            
            # Log duration
            logger.info(f"Video duration: {video.duration:.2f} seconds")
            
            # Write audio to file
            logger.info(f"Extracting audio to {audio_path}")
            video.audio.write_audiofile(audio_path, verbose=False, logger=None)
            logger.info(f"Audio extraction complete")
            
            # Log audio file size
            audio_size = os.path.getsize(audio_path)
            logger.info(f"Extracted audio size: {audio_size/1024/1024:.2f} MB")
        except Exception as e:
            logger.error(f"Error extracting audio: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return [{"page": 1, "text": f"Error extracting audio: {str(e)}"}]
        
        # Initialize recognizer
        recognizer = sr.Recognizer()
        
        # Load audio file to determine length
        try:
            audio = AudioSegment.from_file(audio_path)
            audio_duration = len(audio) / 1000  # Duration in seconds
            logger.info(f"Audio duration: {audio_duration:.2f} seconds")
            
            # Determine if we need to use chunking
            use_chunking = audio_duration > 45  # Use chunking for files longer than 45 seconds
            
            if use_chunking:
                logger.info(f"Audio is long ({audio_duration:.2f}s), using chunking approach")
                
                # Set chunk size (30 seconds chunks with slight overlap)
                chunk_length_ms = 30 * 1000  # 30 seconds
                overlap_ms = 1000  # 1 second overlap to avoid cutting words
                
                # Calculate number of chunks
                num_chunks = math.ceil(len(audio) / (chunk_length_ms - overlap_ms))
                chunks = []
                
                # Create overlapping chunks
                for i in range(num_chunks):
                    start_ms = max(0, i * (chunk_length_ms - overlap_ms))
                    end_ms = min(len(audio), start_ms + chunk_length_ms)
                    chunk = audio[start_ms:end_ms]
                    chunks.append(chunk)
                
                logger.info(f"Split audio into {len(chunks)} chunks")
                
                # Process each chunk
                transcribed_chunks = []
                for i, chunk in enumerate(chunks):
                    logger.info(f"Processing chunk {i+1}/{len(chunks)}")
                    
                    # Export chunk to temporary file
                    chunk_path = os.path.join(temp_dir, f"chunk_{i}.wav")
                    chunk.export(chunk_path, format="wav")
                    
                    # Transcribe the chunk with retry mechanism
                    chunk_text = ""
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            with sr.AudioFile(chunk_path) as source:
                                chunk_audio = recognizer.record(source)
                                chunk_text = recognizer.recognize_google(chunk_audio)
                                logger.info(f"Chunk {i+1} transcribed: {len(chunk_text)} chars")
                                break  # Success, exit retry loop
                        except sr.RequestError as e:
                            logger.error(f"API request error on chunk {i+1}, attempt {attempt+1}: {e}")
                            if attempt == max_retries - 1:  # Last attempt
                                chunk_text = f"[API error on segment {i+1}]"
                            time.sleep(1)  # Wait before retry
                        except sr.UnknownValueError:
                            logger.error(f"Could not understand audio in chunk {i+1}, attempt {attempt+1}")
                            if attempt == max_retries - 1:  # Last attempt
                                chunk_text = f"[Unintelligible audio in segment {i+1}]"
                            time.sleep(1)  # Wait before retry
                        except Exception as e:
                            logger.error(f"Error transcribing chunk {i+1}, attempt {attempt+1}: {e}")
                            if attempt == max_retries - 1:  # Last attempt
                                chunk_text = f"[Error in segment {i+1}]"
                            time.sleep(1)  # Wait before retry
                    
                    transcribed_chunks.append(chunk_text)
                    
                    # Clean up chunk file
                    try:
                        os.remove(chunk_path)
                    except Exception as e:
                        logger.error(f"Error removing chunk file: {e}")
                
                # Combine chunks into final transcription
                transcribed_text = " ".join(transcribed_chunks)
                logger.info(f"All chunks processed. Total transcription length: {len(transcribed_text)} chars")
                
            else:
                # For shorter audio, process in one go
                logger.info("Audio is short enough for single-pass transcription")
                transcribed_text = ""
                max_retries = 3
                
                for attempt in range(max_retries):
                    try:
                        with sr.AudioFile(audio_path) as source:
                            logger.info("Recording audio data...")
                            audio_data = recognizer.record(source)
                            logger.info("Sending to Google speech recognition...")
                            transcribed_text = recognizer.recognize_google(audio_data)
                            logger.info(f"Successfully transcribed audio: {len(transcribed_text)} chars")
                            break  # Success, exit retry loop
                    except sr.RequestError as e:
                        logger.error(f"API request error, attempt {attempt+1}: {e}")
                        if attempt == max_retries - 1:  # Last attempt
                            transcribed_text = f"Could not transcribe audio. API error: {str(e)}"
                        time.sleep(1)  # Wait before retry
                    except sr.UnknownValueError:
                        logger.error(f"Could not understand audio, attempt {attempt+1}")
                        if attempt == max_retries - 1:  # Last attempt
                            transcribed_text = "Speech recognition could not understand audio."
                        time.sleep(1)  # Wait before retry
                    except Exception as e:
                        logger.error(f"Speech recognition error, attempt {attempt+1}: {e}")
                        logger.error(f"Traceback: {traceback.format_exc()}")
                        if attempt == max_retries - 1:  # Last attempt
                            transcribed_text = f"Could not transcribe audio. Error: {str(e)}"
                        time.sleep(1)  # Wait before retry
            
        except Exception as audio_error:
            logger.error(f"Error processing audio: {audio_error}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            transcribed_text = f"Could not process audio. Error: {str(audio_error)}"
        
        # Apply sentence segmentation to the transcribed text
        try:
            sentences = ai_segment_transcription(transcribed_text)
            logger.info(f"Segmented transcription into {len(sentences)} sentences")
            
            # Format the transcribed text with proper sentences
            formatted_text = "\n\n".join(sentences)
            
            # Clean up
            video.close()
            if os.path.exists(audio_path):
                os.remove(audio_path)
            os.rmdir(temp_dir)
            
            # Return the formatted text as a page
            return [{"page": 1, "text": formatted_text}]
        except Exception as e:
            logger.error(f"Error segmenting transcription: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # Return the raw transcribed text if segmentation fails
            return [{"page": 1, "text": transcribed_text}]
        
    except Exception as e:
        logger.error(f"Error transcribing MP4: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Clean up any temporary files if possible
        try:
            if 'video' in locals() and video:
                video.close()
            if 'temp_dir' in locals() and temp_dir and os.path.exists(temp_dir):
                if os.path.exists(audio_path):
                    os.remove(audio_path)
                os.rmdir(temp_dir)
        except Exception as cleanup_error:
            logger.error(f"Error during cleanup: {cleanup_error}")
        
        # Return a placeholder result to allow processing to continue
        return [{"page": 1, "text": f"Error transcribing video file: {str(e)}"}]


    

def extract_text_from_docx(docx_file_path):
    """Extract text from a .docx file by page."""
    try:
        doc = Document(docx_file_path)
        pages = []
        page_text = ""
        page_count = 1
        
        for paragraph in doc.paragraphs:
            text = paragraph.text.strip()
            if text:
                page_text += text + "\n"
                
                # You may need to adjust logic for determining page breaks
                # This is a simplified approach
                if "\f" in text:  # Form feed character (page break)
                    pages.append({"page": page_count, "text": page_text})
                    page_text = ""
                    page_count += 1
        
        # Add the last page if it has content
        if page_text:
            pages.append({"page": page_count, "text": page_text})
            
        return pages
    except Exception as e:
        logger.error(f"Error extracting text from DOCX: {e}")
        return []

def split_into_sentences(text):
    """Split text into individual sentences with special handling for both page breaks and bullet points."""
    # Pre-process to handle potential page break markers
    # Replace any common page break indicators with sentence-ending punctuation
    text = re.sub(r'(\w+)[\s]*\f[\s]*(\w+)', r'\1. \2', text)  # Form feed character
    text = re.sub(r'(\w+)[\s]*\[PAGE\s*\d+\][\s]*(\w+)', r'\1. \2', text)  # [PAGE X] marker
    
    # Pre-process to handle potential bullet points and list structures
    # Replace the standard dash-based bullet points with a special marker to preserve them
    text = re.sub(r'(\n\s*)-\s*', r'\1BULLET_MARKER ', text)
    
    # Process with spaCy
    doc = nlp(text)
    
    # Extract sentences, ensuring each is properly bounded
    sentences = []
    current_bullet_list = None
    
    for sent in doc.sents:
        sent_text = sent.text.strip()
        
        # Skip empty sentences
        if not sent_text:
            continue
            
        # Check if this sentence is a header (ends with colon and followed by bullets)
        if sent_text.endswith(':'):
            # This might be a header for a bullet list - check if next few sentences start with bullet marker
            current_bullet_list = sent_text
            sentences.append(sent_text)
            continue
            
        # Check if this is a bullet point that belongs to the previous header
        if 'BULLET_MARKER' in sent_text:
            # This is a bullet point - restore the proper dash
            bullet_text = sent_text.replace('BULLET_MARKER', '-')
            
            # If we have an active bullet list, append this to the header
            if current_bullet_list:
                # Combine the header with all its bullet points for analysis
                if len(sentences) > 0 and sentences[-1] == current_bullet_list:
                    # Replace the header with the combined text
                    sentences[-1] = f"{current_bullet_list} {bullet_text}"
                else:
                    # Add as a separate sentence
                    sentences.append(bullet_text)
            else:
                # Stand-alone bullet point
                sentences.append(bullet_text)
            continue
        
        # Regular sentence - add it and reset bullet list tracking
        sentences.append(sent_text)
        current_bullet_list = None
    
    # Post-process to handle any combined sentences that might have been missed
    raw_sentences = []
    for sentence in sentences:
        # Check if this sentence contains multiple bullet points
        if sentence.count('BULLET_MARKER') > 1:
            # Convert back to normal bullet format for readability
            sentence = sentence.replace('BULLET_MARKER', '-')
        raw_sentences.append(sentence)
    
    # Post-process: check for obvious split sentences
    processed_sentences = []
    buffer = ""
    
    for sentence in raw_sentences:
        # If previous buffer exists and current sentence starts lowercase or with a continuation marker
        if buffer and (sentence[0].islower() or sentence.startswith(('and', 'or', 'but', 'so', 'because', 'which', 'that'))):
            buffer += " " + sentence
        else:
            # Save previous buffer if any
            if buffer:
                processed_sentences.append(buffer)
            buffer = sentence
    
    # Add the last buffer
    if buffer:
        processed_sentences.append(buffer)
    
    return processed_sentences

   
# Function to update user cost
def update_user_cost(user_email, cost_amount):
    """
    Update the user's accumulated API cost with better error handling
    
    Args:
        user_email (str): The user's email
        cost_amount (float): The amount to add to the user's cost
    """
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # Load users (will try DB first, then fallback to JSON)
            users = load_users()
            user = next((u for u in users if u['email'] == user_email), None)
            
            if user:
                # Initialize User Cost field if it doesn't exist
                if 'User Cost' not in user:
                    user['User Cost'] = 0.0
                
                # Store previous cost for logging
                previous_cost = user['User Cost']
                
                # Add the new cost
                user['User Cost'] += cost_amount
                
                # Round to 8 decimal places for precision
                user['User Cost'] = round(user['User Cost'], 8)
                
                # Save the updated users back
                save_users(users)
                
                logger.info(f"Updated cost for user {user_email}: +${cost_amount:.6f}, Previous: ${previous_cost:.6f}, New Total: ${user['User Cost']:.6f}")
                return True
            else:
                logger.warning(f"User with email {user_email} not found when updating cost")
                return False
                
        except Exception as e:
            retry_count += 1
            logger.error(f"Error updating user cost (attempt {retry_count}/{max_retries}): {e}")
            
            if retry_count < max_retries:
                import time
                time.sleep(0.5)  # Wait before retry
            else:
                logger.error(f"Failed to update user cost after {max_retries} attempts")
                return False
    
    return False
    
    
# Function for processing the response and ensuring only compliance issues are shown
def generate_compliance_analysis(analysis_results):

    # If no issues are found
    if not analysis_results or analysis_results == ["Compliance Check Completed. No issues found."]:
        return "Compliance Check Completed. No issues found."

    # Ensure each flagged instance is listed distinctly with conflict message and page number
    compliance_output = []
    for index, item in enumerate(analysis_results, start=1):  # Start numbering from 1

        # Each item is a dictionary with 'page' and 'response' keys
        page_number = item['page']

        # Remove redundant "Page Number: X" and hyphens from each line in the response
        issue_text = re.sub(r"- Page Number: \d+\n", "", item['response']).strip()
        issue_text = issue_text.replace("- Flagged Instance:", "Flagged Instance:").replace("- Specific Compliant Alternative:", "Specific Compliant Alternative:")

        # Add a blank line after each compliant alternative text (at the end of each instance)
        issue_text = re.sub(r"(Specific Compliant Alternative:.*?)(\n|$)", r"\1\n\n", issue_text, flags=re.DOTALL)

        # Extract flagged instance and compliant alternative from formatted text
        flagged_match = re.search(r'Flagged Instance: "(.*?)"', issue_text)
        alternative_match = re.search(r'Specific Compliant Alternative: "(.*?)"', issue_text)
        flagged_instance = flagged_match.group(1) if flagged_match else ""
        compliant_alternative = alternative_match.group(1) if alternative_match else "No specific compliant alternative available."

        # Format item with explicit keys for each part and include the current formatted output

        compliance_output.append({

            "page": page_number,
            "flagged_instance": flagged_instance,
            "compliant_alternative": compliant_alternative,
            "full_text": f"{index}. Potential Conflict Identified on Page {page_number}:\n{issue_text}"
        })

    return compliance_output




# Function to check financial rules that require confirmation of current year regulations

def check_financial_rule_verification(content):
    financial_issues = []

    # Terms related to financial rules that require date-specific confirmation
    date_sensitive_terms = ["rmd", "required minimum distribution", "contribution", "distribution", 
                            "income limit", "catch-up contribution", "rule change", "2024", "2023"]
    # If any date-sensitive terms are found in the content, add a verification reminder
    for term in date_sensitive_terms:
        if term.lower() in content.lower():
            financial_issues.append(

                "Be sure to confirm the accuracy of all statements made relating to current year financial rules."
            )
            
            break  # Only add the statement once
    return financial_issues


# Function to remove quotes from the analyzed text

def remove_quotes(text):
    """
    Remove all types of quotes (single, double) from the given text.
    """
    return re.sub(r'[\'"]', '', text)


# Function for Prohibited Words display

def generate_clean_finra_response(prompt, prohibited_words_dict, max_attempts=3):
    """
    Keeps calling Claude until the response has no prohibited words
    in the 'Specific Compliant Alternative' lines (or until max_attempts).
    Returns the final cleaned FINRA response or None if not possible.
    """

    # Build the system message once
    system_message = f"""
You are a compliance assistant specializing in financial regulations.
Do NOT use these words: {', '.join(prohibited_words_dict.keys())}
"""

    for _ in range(max_attempts):
        # Call Claude only once per loop
        response = anthropic.messages.create(
            model="claude-3-opus-20240229",
            system=system_message,  # system message as separate parameter
            max_tokens=100,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        finra_response = response.content[0].text.strip()

        # 2) Find "Specific Compliant Alternative" lines
        matches = re.findall(
            r'Specific Compliant Alternative:\s*"([^"]*)"',
            finra_response,
            flags=re.IGNORECASE
        )
        
        # 3) Check each found alternative for prohibited words
        has_prohibited = False
        for alt_text in matches:
            for pword in prohibited_words_dict.keys():
                pattern = rf"\b{re.escape(pword.lower())}\b"
                if re.search(pattern, alt_text.lower()):
                    has_prohibited = True
                    break
            if has_prohibited:
                break
        if not has_prohibited:
            # Found a version with no banned words
            return finra_response

    # If we got here, can't find a clean alternative after max_attempts
    return None

# Function to check if text is inside quotes in the original text
def is_inside_quotes(text, original_text):
    """Check if the text appears inside quotes in the original text."""
    # Find all quoted text
    quoted_texts = re.findall(r'"([^"]*)"', original_text)
    quoted_texts += re.findall(r"'([^']*)'", original_text)
    
    # Check if text appears in any of the quoted sections
    for quoted_text in quoted_texts:
        if text in quoted_text:
            return True
    return False

# Usage of Prohibited Words in Check Compliance Function
def check_compliance(content_by_page, disclosures):
    finra_analysis = []
    logger.info("--- Starting Compliance Check ---")
    
    for page_content in content_by_page:
        page_num = page_content["page"]
        content = page_content["text"]
        cleaned_content = remove_quotes(content)
        
        # Perform compliance check using the centralized function
        flagged_instances = perform_compliance_check(cleaned_content, page_num)
        
        if flagged_instances:
            # Add highlighting to flagged instances
            for instance in flagged_instances:
                instance['highlighted_flagged_instance'] = extract_problematic_words_from_rationale(
                    instance['flagged_instance'], 
                    instance.get('rationale', '')
                )
            finra_analysis.extend(flagged_instances)
    
    return finra_analysis


def perform_compliance_check(text, page_num=None):
    """Checks text compliance using BERT for flagging and GPT for verification."""
    try:
        global BERT_MODEL, BERT_TOKENIZER
        if BERT_MODEL is None or BERT_TOKENIZER is None:
            logger.info("Initializing BERT model for compliance check...")
            if not initialize_bert():
                logger.error("Failed to initialize BERT model")
                raise Exception("Failed to initialize BERT model")

        # If text is empty, contains only whitespace, or is just a bullet point
        if not text or text.isspace() or text == "‚Ä¢" or text == "\u2022":
            logger.info("Text is empty or just a bullet point - skipping")
            return {"compliant": True, "flagged_instances": []}

        # If text is a dictionary, extract the 'text' field
        if isinstance(text, dict):
            text = text.get('text', '')
            logger.info(f"Extracted text from dictionary, length: {len(text)}")
        
        # Clean the text
        original_len = len(text)
        text = re.sub(r'[‚Ä¢\u2022]', '', text)  # Remove bullet points
        text = re.sub(r'\s+', ' ', text).strip()  # Clean up whitespace
        logger.info(f"Cleaned text: {original_len} chars ‚Üí {len(text)} chars")
        
        # Special handling for bullet lists - look for patterns like "Header: -item1 -item2"
        # First, preserve the original format of the text for analysis
        preserved_text = text
        
        # Look for bullet list patterns (header followed by lines starting with dash/hyphen)
        header_pattern = re.compile(r'(.*?):[\s]*$', re.MULTILINE)
        potential_headers = header_pattern.findall(preserved_text)
        
        for header in potential_headers:
            header_with_colon = f"{header}:"
            # Check if this header is followed by bullet points
            header_pos = preserved_text.find(header_with_colon)
            if header_pos >= 0:
                # Look for bullet points after this header
                text_after_header = preserved_text[header_pos + len(header_with_colon):]
                # Check if there are lines starting with dash/hyphen
                if re.search(r'[\n\r]\s*-', text_after_header):
                    # This is a header with bullet points - treat it as a single unit
                    # Find the end of the bullet list
                    match = re.search(r'([\n\r]\s*-.*?)(?:[\n\r][^\s-]|$)', text_after_header, re.DOTALL)
                    if match:
                        bullet_list = match.group(1)
                        # Combine the header with its bullet points, replacing newlines
                        combined_text = header_with_colon + bullet_list.replace('\n', ' ').replace('\r', ' ')
                        # Update the text to ensure the bullet list stays together
                        preserved_text = preserved_text.replace(header_with_colon + bullet_list, combined_text)
                        logger.info(f"Combined bullet list: {combined_text[:100]}...")
        
        # Use the preserved text that maintains bullet lists as units
        text = preserved_text
        
        # Split text into sentences
        logger.info(f"Splitting text into sentences...")
        # Use a simpler sentence splitting approach for bullet list scenarios
        if "-" in text and ":" in text:
            # This might be a bullet list - use a special splitting approach
            sentences = []
            
            # First, try to identify header + bullet list combinations
            header_bullets_pattern = re.compile(r'(.*?:)[\s\n\r]*((?:-[^-\n\r]*[\n\r]*)+)', re.DOTALL)
            header_bullets_matches = header_bullets_pattern.findall(text)
            
            if header_bullets_matches:
                for header, bullets in header_bullets_matches:
                    # Combine header with all its bullets as one sentence for analysis
                    combined = header + " " + bullets.replace('\n', ' ').replace('\r', ' ')
                    sentences.append(combined)
                    # Remove this part from the text to avoid double-processing
                    text = text.replace(header + bullets, '')
            
            # Process any remaining text with standard sentence splitting
            remaining_sentences = re.split(r'(?<=[.!?])\s+', text)
            sentences.extend([s.strip() for s in remaining_sentences if s.strip()])
        else:
            # Standard sentence splitting for non-bullet text
            sentences = split_into_sentences(text)
            
        logger.info(f"Found {len(sentences)} sentences")
        flagged_instances = []
        bert_flagged_instances = []

        # Track both flagged texts AND sentences that are part of context flags
        flagged_texts = set()
        processed_sentences = set()  # NEW: Track sentences already processed in context

        # Process sentences in batches
        batch_size = 1024  # Increased from 512
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(sentences)-1)//batch_size + 1}: {len(batch)} sentences")
        
            for sentence_idx, sentence in enumerate(batch):
                # Skip invalid sentences
                if not sentence.strip() or sentence.strip() in ['‚Ä¢', '\u2022']:
                    continue
            
                # Clean the sentence
                sentence = re.sub(r'[‚Ä¢\u2022]', '', sentence).strip()
                if not sentence:  # Skip if sentence becomes empty after cleaning
                    continue
                
                # NEW: Skip if this sentence was already processed as part of a context flag
                sentence_normalized = sentence.lower().strip()
                if sentence_normalized in processed_sentences:
                    logger.info(f"Skipping sentence already processed in context: {sentence[:50]}...")
                    continue
                
                # Log only occasional sentences to avoid overwhelming logs
                if sentence_idx % 20 == 0:  # Log every 20th sentence
                    logger.info(f"Processing sentence #{i + sentence_idx}: {sentence[:50]}...")
                
                # Prepare input for BERT
                inputs = BERT_TOKENIZER(sentence, 
                                      return_tensors="pt",
                                      truncation=True,
                                      max_length=1024,
                                      padding=True)
            
                # Make prediction using BERT
                with torch.no_grad():
                    outputs = BERT_MODEL(**inputs)
                    probabilities = softmax(outputs.logits, dim=1)
                    prediction = torch.argmax(probabilities, dim=1).item()
                    confidence = probabilities[0][prediction].item()


                # Added these lines right after:
                
                # Check for skepticism words even if BERT says compliant
                skepticism_words = load_skepticism_words()
                sentence_lower = sentence.lower()
                has_skepticism_words = contains_skepticism_words(sentence)

                # Add detailed logging
                if has_skepticism_words:
                    found_words = [w for w in skepticism_words if w.lower() in sentence_lower]
                    logger.info(f"Found skepticism words: {found_words} in sentence: {sentence}")
                    logger.info(f"BERT prediction: {prediction}, confidence: {confidence}")
                    logger.info(f"Will override? prediction==0: {prediction == 0}, has_skepticism: {has_skepticism_words}, confidence<0.9: {confidence < 0.9}")

                # If BERT says compliant but we found skepticism words, override to flag it
                if prediction == 0 and has_skepticism_words:
                    prediction = 1  # Override to non-compliant
                    logger.info(f"BERT override: Found skepticism words {[w for w in skepticism_words if w.lower() in sentence_lower]} in sentence: {sentence[:50]}...")                #

#

                # Add explicit log for each sentence prediction
                logger.info(f"Prediction: {prediction} ('{'non-compliant' if prediction == 1 else 'compliant'}'), Confidence: {confidence:.1%}")

                # Log predictions only for flagged or every 20th sentence
                if prediction == 1 or sentence_idx % 20 == 0:
                    logger.info(f"BERT prediction for sentence #{i + sentence_idx}: {prediction} ('{'non-compliant' if prediction == 1 else 'compliant'}'), Confidence: {confidence:.1%}")

                if prediction == 1:  # We found a non-compliant prediction
                    words_in_sentence = len(sentence.split())

                    #if words_in_sentence > 2 and confidence > 0.7: # Only flag if confidence > 70%
                    if words_in_sentence <= 5 or confidence < 0.7:
                        # Send to Deepseek for context evaluation
                        logger.info(
                            f"Sending to Deepseek: {'short sentence (<=5 words)' if words_in_sentence <= 5 else 'low confidence (<70%)'}"
                        )
                    
                        # Regular case: sentence is long enough to stand on its own
                        # Check for duplicates
                        if sentence_normalized not in flagged_texts:
                            bert_flagged_instances.append({
                                "flagged_instance": sentence,
                                "confidence": confidence,
                                "page": page_num
                            })
                            flagged_texts.add(sentence_normalized)
                            logger.info(f"BERT flagged sentence (confidence: {confidence:.1%}): {sentence[:100]}...")
                        else:
                            logger.info(f"Skipping duplicate flagged text: {sentence[:50]}...")

                    else:
                        # Short sentence case: Check context by looking at neighboring sentences
                        logger.info(f"Found short non-compliant sentence: '{sentence}' - checking context...")
        
                        # Find the current sentence index in the full sentences list
                        sentence_position = -1
                        for idx, s in enumerate(sentences):
                            if s.strip() == sentence.strip():
                                sentence_position = idx
                                break
        
                        if sentence_position == -1:
                            logger.warning(f"Could not find short sentence '{sentence}' in the list of sentences")
                            continue
            
                        # Get previous and next sentences if they exist
                        prev_sentence = sentences[sentence_position - 1] if sentence_position > 0 else ""
                        next_sentence = sentences[sentence_position + 1] if sentence_position < len(sentences) - 1 else ""
        
                        # Create context by combining neighboring sentences
                        context = ""
                        if prev_sentence:
                            context += prev_sentence + " "
                        context += sentence
                        if next_sentence:
                            context += " " + next_sentence
            
                        logger.info(f"Context for short sentence: '{context}'")
        
                        # Send the context to Deepseek for evaluation
                        evaluation_prompt = f"""
Analyze this text which contains a short sentence that BERT flagged as potentially non-compliant:

Text with context: "{context}"
Short flagged sentence: "{sentence}"

First, determine if the short sentence is:
1. A standalone title or heading
2. Part of a Q&A or dialogue (like "Absolutely." as an answer to a question)
3. A fragment that should be interpreted with neighboring sentences

Then determine if the short sentence, when interpreted in context, contains any of these FINRA compliance issues:
- Guarantees of performance or returns
- Absolute statements without proper qualifiers
- Promissory language
- Exaggerated or misleading claims

Respond with ONLY:
- CONTEXT_TYPE: [Title/Heading, Answer/Response, Fragment]
- FLAG_DECISION: [Flag, Ignore]
- BEST_FLAGGED_TEXT: "[if FLAG_DECISION is Flag, provide the exact text that should be flagged - either just the short sentence or the context that gives it meaning]"

Example: 
CONTEXT_TYPE: Answer/Response
FLAG_DECISION: Flag
BEST_FLAGGED_TEXT: "Can we expect future technology to supplement trustees' discretionary decision-making processes? Absolutely."
"""
        
                        context_evaluation = call_deepseek_api(evaluation_prompt)
                        logger.info(f"Deepseek context evaluation: {context_evaluation}")
        
                        # Parse the evaluation response
                        context_type = "Unknown"
                        flag_decision = "Ignore"  # Default to ignore
                        best_flagged_text = sentence  # Default to just the short sentence
        
                        for line in context_evaluation.strip().split('\n'):
                            if line.startswith("CONTEXT_TYPE:"):
                                context_type = line.replace("CONTEXT_TYPE:", "").strip()
                            elif line.startswith("FLAG_DECISION:"):
                                flag_decision = line.replace("FLAG_DECISION:", "").strip()
                            elif line.startswith("BEST_FLAGGED_TEXT:"):
                                best_flagged_text = line.replace("BEST_FLAGGED_TEXT:", "").strip().strip('"')
        
                        # If Deepseek says to flag it, add to flagged instances with the appropriate text
                        if flag_decision.lower() == "flag":
                            normalized_flagged = best_flagged_text.lower().strip()
                            if normalized_flagged not in flagged_texts:
                                ({
                                    "flagged_instance": best_flagged_text,
                                    "confidence": confidence,
                                    "page": page_num,
                                    "context_type": context_type,
                                    "deepseek_verified": True
                                })
                                flagged_texts.add(normalized_flagged)
                                
                                # NEW: Mark all sentences in the flagged context as processed
                                context_sentences = split_into_sentences(best_flagged_text)
                                for ctx_sentence in context_sentences:
                                    ctx_normalized = ctx_sentence.strip().lower()
                                    processed_sentences.add(ctx_normalized)
                                    logger.info(f"Marking sentence as processed: {ctx_sentence[:30]}...")
                                
                                logger.info(f"BERT+Deepseek flagged contextual sentence (confidence: {confidence:.1%}): {best_flagged_text[:100]}...")
                            else:
                                logger.info(f"Skipping duplicate contextual flagged text: {best_flagged_text[:50]}...")

        # Log summary of BERT analysis
        logger.info(f"BERT analysis complete: {len(sentences)} total sentences, {len(bert_flagged_instances)} flagged instances")
        
        # Use Deepseek to verify BERT's findings in a fully batched approach
        if bert_flagged_instances:
            deepseek_checked = [i for i in bert_flagged_instances if i.get("deepseek_verified")]
            bert_only_checked = [i for i in bert_flagged_instances if not i.get("deepseek_verified")]

            flagged_instances = []

            # Case 1: BERT-only ‚Üí send to GPT full verification
            if bert_only_checked:
                logger.info(f"Sending {len(bert_only_checked)} BERT-only flagged instances to GPT for verification...")
                verified = verify_with_gpt_fully_batched(bert_only_checked, batch_size=6)
                flagged_instances.extend(verified)

            # Case 2: Deepseek already verified ‚Üí skip straight to rewrite
            if deepseek_checked:
                logger.info(f"Skipping GPT re-verification for {len(deepseek_checked)} Deepseek-verified instances; generating alternatives only...")
                for inst in deepseek_checked:
                    prompt = f"""
    Rewrite this financial statement to make it FINRA-compliant:

    "{inst['flagged_instance']}"

    Rules:
    - Replace overstatements
    - Remove absolute statements
    - Add qualifiers like "may," "potential," "seeks to," etc.
    - Avoid guarantees or promises
    - Keep the original intent
    Respond with ONLY the compliant alternative text.
    """
                    alt = call_deepseek_api(prompt).strip()
                    if not alt or len(alt) < 5:
                        alt = "We may help clients pursue their investment objectives while understanding that all investments involve risk."

                    flagged_instances.append({
                        "flagged_instance": inst["flagged_instance"],
                        "compliance_status": "non-compliant",
                        "specific_compliant_alternative": alt,
                        "rationale": "Deepseek already confirmed non-compliance; alternative generated without GPT re-verification.",
                         "page": inst.get("page"),
                        "confidence": f"{inst.get('confidence', 0):.1%}",
                        "gpt_verified": False,
                        "deepseek_verified": True
                    })
            else:
                logger.info("No instances flagged by BERT, skipping GPT verification")
    
        # Filter out any remaining invalid instances
        original_count = len(flagged_instances)
        flagged_instances = [
            instance for instance in flagged_instances
            if instance.get("flagged_instance") 
            and len(instance["flagged_instance"].strip()) > 5
            and instance["flagged_instance"].strip() not in ['‚Ä¢', '\u2022']
        ]
        if original_count != len(flagged_instances):
            logger.info(f"Filtered out {original_count - len(flagged_instances)} invalid instances")

        logger.info(f"Final compliance check results:")
        logger.info(f"  - Total sentences processed: {len(sentences)}")
        logger.info(f"  - BERT flagged instances: {len(bert_flagged_instances)}")
        logger.info(f"  - Final flagged instances after GPT verification: {len(flagged_instances)}")
        logger.info(f"  - Overall compliance status: {'Compliant' if len(flagged_instances) == 0 else 'Non-Compliant'}")
        
        return {
            "compliant": len(flagged_instances) == 0,
            "flagged_instances": flagged_instances
        }
    
    except Exception as e:
        logger.error(f"Error during compliance check: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return {"compliant": False, "error": f"An error occurred during compliance checking: {str(e)}"}    
    


def verify_with_gpt_fully_batched(bert_flagged_instances, batch_size=6):
    """
    Verifies BERT-flagged instances using Deepseek in batches to reduce false positives,
    and also batches the generation of compliant alternatives for confirmed non-compliant instances.
    """
    verified_instances = []
    non_compliant_instances = []  # Track instances needing alternatives
    
    logger.info(f"========== GPT VERIFICATION STARTED ==========")
    logger.info(f"Verifying {len(bert_flagged_instances)} instances flagged by BERT using batch size of {batch_size}")

    # Load skepticism words at the start
    skepticism_words = load_skepticism_words()
    skepticism_instruction = f"""
SPECIAL ATTENTION: Apply higher skepticism when analyzing text containing these words in financial/business contexts: {', '.join(skepticism_words)}

These words often indicate promissory or absolute language when used in financial communications.
"""
    
    # Initialize batch metrics
    batch_tokens = 0
    batch_cost = 0.0
    
    if 'session_token_usage' in globals():
        batch_tokens = session_token_usage['total_tokens']
        batch_cost = session_token_usage['total_cost']
    
    # PHASE 1: Batched verification of compliance status
    # Process instances in batches
    for batch_start in range(0, len(bert_flagged_instances), batch_size):
        batch_end = min(batch_start + batch_size, len(bert_flagged_instances))
        current_batch = bert_flagged_instances[batch_start:batch_end]
        
        logger.info(f"------- Processing batch {batch_start//batch_size + 1}: instances {batch_start+1}-{batch_end}/{len(bert_flagged_instances)} -------")
        
        # Construct a combined prompt for all instances in this batch
        combined_prompt = f"""{skepticism_instruction}

Determine if each of the following texts violates FINRA's communication rules by being false, misleading, promissory or exaggerated:

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)


CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.

--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant

--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

For each text, answer with ONLY "YES" (non-compliant) or "NO" (compliant), followed by a brief explanation.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
        
        for i, instance in enumerate(current_batch, 1):
            sentence = instance.get("flagged_instance", "")
            combined_prompt += f"\nTEXT {i}: \"{sentence}\"\n"
        
        combined_prompt += "\nRespond in this format:\nTEXT 1: YES/NO - Brief explanation\nTEXT 2: YES/NO - Brief explanation\n... and so on"
        
        try:
            # Make a single API call for the entire batch
            logger.info(f"Sending batch of {len(current_batch)} instances to Deepseek API")
            verification_response = call_deepseek_api(combined_prompt)
            logger.info(f"Received batch verification response from Deepseek API")
            
            # Extract individual responses for each instance
            response_lines = verification_response.strip().split('\n')
            responses_by_text = {}
            
            for line in response_lines:
                line = line.strip()
                # Match lines like "TEXT 1: YES - explanation" or "TEXT 2: NO - explanation"
                match = re.match(r'^TEXT\s+(\d+):\s+(.*?)$', line, re.IGNORECASE)
                if match:
                    text_num = int(match.group(1))
                    response_content = match.group(2).strip()
                    responses_by_text[text_num] = response_content
            
            # Process each instance based on the response
            for i, instance in enumerate(current_batch, 1):
                sentence = instance.get("flagged_instance", "")
                page_num = instance.get("page")
                confidence = instance.get("confidence", 0)
                
                # Log instance details
                logger.info(f"------- Instance {batch_start+i}/{len(bert_flagged_instances)} -------")
                logger.info(f"Page: {page_num}, Confidence: {confidence:.1%}")
                logger.info(f"Text: \"{sentence[:100]}{'...' if len(sentence) > 100 else ''}\"")
                
                # Get response for this specific instance
                instance_response = responses_by_text.get(i, "")
                logger.info(f"GPT verification response: {instance_response}")
                
                # Evaluate GPT's response
                is_non_compliant = False
                if instance_response:
                    # Check if the response starts with YES
                    first_word = instance_response.split('-')[0].strip().upper() if instance_response.strip() else ""
                    is_non_compliant = first_word == "YES"

                    # FOR LOW-CONFIDENCE BERT PREDICTIONS THAT COULD GO EITHER WAY, APPLY HIGHER SCRUTINY
                    if is_non_compliant:
                        if confidence < 0.7:
                            logger.info(f"Higher scrutiny Deepseek OVERRODE initial compliant decision")
                        else:
                            logger.info(f"GPT CONFIRMS non-compliance, adding to batch for alternative generation")
                            
                        # Send to Deepseek with higher scrutiny prompt
                        scrutiny_prompt = f"""
You are a VERY STRICT FINRA compliance reviewer. Apply maximum skepticism to this text:

"{sentence}"

Look carefully for:
- Certaintly about income, investments, etc. even if appropriate qualifiers exist
- ANY implied guarantees or promises
- Subtle absolute language 
- Unqualified statements about benefits
- Lacks qualifying statements

Answer ONLY "YES" (flag it) or "NO" (it's fine), followed by brief explanation.
"""

                        scrutiny_response = call_deepseek_api(scrutiny_prompt)
                        if scrutiny_response:
                            scrutiny_decision = scrutiny_response.strip().split()[0].upper() if scrutiny_response.strip() else ""
                            if scrutiny_decision == "YES":
                                is_non_compliant = True
                                logger.info(f"Higher scrutiny Deepseek flagged low-confidence instance")

                    
                    # For high-confidence BERT predictions, verify we're not getting a false negative
                    if not is_non_compliant and confidence > 0.9:
                        # Look for known violation patterns in the text
                        violation_patterns = [
                            #r'\bwill\s+(?:provide|earn|gain|make|increase|grow|guarantee)',
                            #r'\bguarantee[ds]?\b',
                            #r'\balways\b',
                            #r'\bnever\b',
                            #r'\bcertain(?:ly)?\b',
                            #r'\bensure[ds]?\b'
                        ]
                        
                        for pattern in violation_patterns:
                            if re.search(pattern, sentence.lower()):
                                is_non_compliant = True
                                logger.info(f"Overriding GPT decision due to high confidence and violation pattern match")
                                break
                
                if is_non_compliant:
                    logger.info(f"GPT CONFIRMS non-compliance, adding to batch for alternative generation")
                    # Store the non-compliant instance for batch processing of alternatives
                    non_compliant_instances.append({
                        "original_instance": instance,
                        "sentence": sentence,
                        "page_num": page_num,
                        "confidence": confidence,
                        "batch_index": len(non_compliant_instances)  # Track position in batch
                    })
                else:
                    logger.info(f"‚úó GPT determined instance is COMPLIANT - FALSE POSITIVE from BERT - not flagging")
            
        except Exception as e:
            logger.error(f"Error processing verification batch: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # Fall back to individual verification for this batch
            logger.warning(f"Batch verification failed, falling back to individual processing")
            
            for instance in current_batch:
                try:
                    # Individual verification similar to original function
                    sentence = instance.get("flagged_instance", "")
                    page_num = instance.get("page")
                    confidence = instance.get("confidence", 0)
                    
                    logger.info(f"Individual verification for page {page_num}: '{sentence[:50]}...'")

                    # Then modify your verification prompt by adding this section:
                    skepticism_words = load_skepticism_words()
                    skepticism_instruction = f"""
                    SPECIAL ATTENTION: Apply higher skepticism when analyzing text containing these words in financial/business contexts: {', '.join(skepticism_words)}

                    These words often indicate promissory or absolute language when used in financial communications.
                    """
                    
                    # Use the original verification prompt for individual instance
                    verification_prompt = f"""
                    {skepticism_instruction}
                    
Determine if this text violates FINRA's communication rules by being false, misleading, promissory, exaggerated, or contains profanity:

"{sentence}"

Answer with ONLY "YES" or "NO", followed by a brief explanation.
"YES" means it IS non-compliant or contains profanity.
"NO" means it IS compliant.

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)


CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.


--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant
--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
                    
                    verification_response = call_deepseek_api(verification_prompt)
                    logger.info(f"Individual verification response: {verification_response}")
                    
                    # Determine if non-compliant
                    is_non_compliant = False
                    if verification_response:
                        first_word = verification_response.strip().split()[0].upper() if verification_response.strip() else ""
                        is_non_compliant = first_word == "YES"
                        
                        if not is_non_compliant and confidence > 0.9:
                            # Apply pattern matching as before
                            violation_patterns = [
                                #r'\bwill\s+(?:provide|earn|gain|make|increase|grow|guarantee)',
                                #r'\bguarantee[ds]?\b',
                                #r'\balways\b',
                                #r'\bnever\b',
                                #r'\bcertain(?:ly)?\b',
                                #r'\bensure[ds]?\b'
                            ]
                            
                            for pattern in violation_patterns:
                                if re.search(pattern, sentence.lower()):
                                    is_non_compliant = True
                                    logger.info(f"Overriding individual GPT decision due to high confidence")
                                    break
                    
                    if is_non_compliant:
                        # Add to non-compliant instances for batch processing later
                        non_compliant_instances.append({
                            "original_instance": instance,
                            "sentence": sentence,
                            "page_num": page_num,
                            "confidence": confidence,
                            "batch_index": len(non_compliant_instances)
                        })
                    
                except Exception as individual_error:
                    logger.error(f"Error in individual verification: {individual_error}")
                    # If verification fails, use BERT judgment for high-confidence predictions
                    if confidence > 0.8:
                        # Add instance with a default alternative
                        compliant_version = "We may help clients pursue their investment objectives while understanding that all investments involve risk."
                        
                        instance_result = {
                            "flagged_instance": sentence,
                            "compliance_status": "non-compliant",
                            "specific_compliant_alternative": compliant_version,
                            "rationale": "This text may not comply with FINRA regulations (verification failed).",
                            "page": page_num,
                            "confidence": f"{confidence:.1%}",
                            "gpt_verified": False
                        }
                        verified_instances.append(instance_result)
                        logger.info(f"Added instance with default alternative due to verification failure")
    
    # PHASE 2: Batch generation of compliant alternatives
    if non_compliant_instances:
        logger.info(f"Generating batched compliant alternatives for {len(non_compliant_instances)} non-compliant instances")
        
        # Process in batches (smaller batches for alternatives generation)
        alt_batch_size = min(3, batch_size)  # Smaller batch size for alternatives
        
        for alt_batch_start in range(0, len(non_compliant_instances), alt_batch_size):
            alt_batch_end = min(alt_batch_start + alt_batch_size, len(non_compliant_instances))
            alt_current_batch = non_compliant_instances[alt_batch_start:alt_batch_end]
            
            logger.info(f"Generating alternatives for batch {alt_batch_start//alt_batch_size + 1}: {alt_batch_start+1}-{alt_batch_end}/{len(non_compliant_instances)}")
            
            # Create a combined prompt for generating alternatives
            alternatives_prompt = """Rewrite each of the following non-compliant financial statements to make them FINRA-compliant:

For each text, create a compliant alternative by:
1. Replacing overstatements
2. Removing absolute statements
3. Adding appropriate qualifiers like "may," "potential," "seeks to," etc.
4. Avoiding guarantees or promises
5. Maintaining the original meaning

Respond ONLY with the rewritten statements in this format:
TEXT 1: "Compliant alternative for first text"
TEXT 2: "Compliant alternative for second text"
...and so on.

DO NOT include any additional explanations, headers, or commentary.

"""
            
            for i, item in enumerate(alt_current_batch, 1):
                alternatives_prompt += f"\nTEXT {i}: \"{item['sentence']}\"\n"
            
            try:
                # Make a single API call for all alternatives in this batch
                batch_alternatives_response = call_deepseek_api(alternatives_prompt)
                logger.info(f"Received batch alternatives from Deepseek API")
                
                # Parse the response to extract alternatives
                alt_response_lines = batch_alternatives_response.strip().split('\n')
                alternatives_by_text = {}
                
                for line in alt_response_lines:
                    line = line.strip()
                    # Match lines like "TEXT 1: "alternative text"" 
                    alt_match = re.match(r'^TEXT\s+(\d+):\s+"?(.*?)"?$', line, re.IGNORECASE)
                    if alt_match:
                        text_num = int(alt_match.group(1))
                        alternative_text = alt_match.group(2).strip().strip('"')  # Remove quotes if present
                        alternatives_by_text[text_num] = alternative_text
                
                # Process each instance with its alternative
                for i, item in enumerate(alt_current_batch, 1):
                    original_instance = item["original_instance"]
                    sentence = item["sentence"]
                    page_num = item["page_num"]
                    confidence = item["confidence"]
                    
                    # Get the alternative for this instance
                    compliant_text = alternatives_by_text.get(i, "")
                    compliant_text = apply_skepticism_word_fixes(compliant_text, sentence)
                    logger.info(f"Direct alternative received: {compliant_text}")
                    
                    # Validate and clean the alternative
                    if not compliant_text or len(compliant_text) < 5:
                        logger.error(f"Got empty or too short alternative for instance {alt_batch_start+i}: \"{compliant_text}\", using fallback")
                        compliant_text = "The server is unable to evaluate your request at this time due to unauthorized language usage or poor word selection. Please modify your submission and try again."

                    else:
                        logger.info(f"Direct alternative received: {compliant_text}")

                    # Clean quotes if present
                    if compliant_text.startswith('"') and compliant_text.endswith('"'):
                        compliant_text = compliant_text[1:-1]
                    
                    # Create the verified instance
                    instance_result = {
                        "flagged_instance": sentence,
                        "compliance_status": "non-compliant",
                        "specific_compliant_alternative": compliant_text,
                        "rationale": "This text may contain absolutes, guarantees, or promissory statements not allowed under FINRA regulations.",
                        "page": page_num,
                        "confidence": f"{confidence:.1%}",
                        "gpt_verified": True
                    }
                    verified_instances.append(instance_result)
                    logger.info(f"‚úì Added instance with batched alternative")
                
            except Exception as alt_batch_error:
                logger.error(f"Error generating batch alternatives: {alt_batch_error}")
                
                # Fall back to individual alternative generation
                logger.warning(f"Falling back to individual alternative generation for this batch")
                
                for item in alt_current_batch:
                    try:
                        original_instance = item["original_instance"]
                        sentence = item["sentence"]
                        page_num = item["page_num"]
                        confidence = item["confidence"]
                        
                        # Generate individual alternative
                        alt_prompt = f"""
This text violates FINRA rules: "{sentence}"

Rewrite it to be compliant by:
1. Replacing overstatements
2. Removing absolute statements
3. Adding appropriate qualifiers
4. Avoiding guarantees or promises
5. Maintaining the original meaning

Respond with ONLY the compliant alternative text and nothing else.
"""
                        
                        compliant_text = call_deepseek_api(alt_prompt).strip()
                        logger.info(f"Individual alternative received: {compliant_text}")
                        
                        # Validate and clean
                        if not compliant_text or len(compliant_text) < 5:
                            logger.error(f"Got empty or too short individual alternative, using fallback")
                            compliant_text = "We may help clients pursue their investment objectives while understanding that all investments involve risk."
                        
                        if compliant_text.startswith('"') and compliant_text.endswith('"'):
                            compliant_text = compliant_text[1:-1]
                        
                        # Create the verified instance
                        instance_result = {
                            "flagged_instance": sentence,
                            "compliance_status": "non-compliant",
                            "specific_compliant_alternative": compliant_text,
                            "rationale": "This text may contain absolutes, guarantees, or promissory statements not allowed under FINRA regulations.",
                            "page": page_num,
                            "confidence": f"{confidence:.1%}",
                            "gpt_verified": True
                        }
                        verified_instances.append(instance_result)
                        logger.info(f"‚úì Added instance with individual alternative")
                        
                    except Exception as individual_alt_error:
                        logger.error(f"Error generating individual alternative: {individual_alt_error}")
                        
                        # Use fallback alternative
                        compliant_text = "We may help clients pursue their investment objectives while understanding that all investments involve risk."
                        
                        instance_result = {
                            "flagged_instance": sentence,
                            "compliance_status": "non-compliant",
                            "specific_compliant_alternative": compliant_text,
                            "rationale": "This text may not comply with FINRA regulations (alternative generation failed).",
                            "page": page_num,
                            "confidence": f"{confidence:.1%}",
                            "gpt_verified": False
                        }
                        verified_instances.append(instance_result)
                        logger.info(f"Added instance with fallback alternative due to error")
    
    # Calculate final token usage and costs
    batch_tokens_used = 0
    batch_final_cost = 0.0
    
    if 'session_token_usage' in globals():
        batch_tokens_used = session_token_usage['total_tokens'] - batch_tokens
        batch_final_cost = session_token_usage['total_cost'] - batch_cost
    
    logger.info(f"========== GPT VERIFICATION COMPLETED ==========")
    logger.info(f"Results: {len(bert_flagged_instances)} BERT flags ‚Üí {len(verified_instances)} verified flags")
    logger.info(f"Tokens used in this verification batch: {batch_tokens_used}")
    logger.info(f"Estimated cost for this batch: ${batch_final_cost:.6f}")
    
    return verified_instances

    

def verify_with_gpt_batched(bert_flagged_instances, batch_size=6):
    """Verifies BERT-flagged instances using Deepseek in batches to reduce false positives."""
    verified_instances = []
    
    logger.info(f"========== GPT VERIFICATION STARTED ==========")
    logger.info(f"Verifying {len(bert_flagged_instances)} instances flagged by BERT using batch size of {batch_size}")
    
    # Initialize batch metrics
    batch_tokens = 0
    batch_cost = 0.0
    
    if 'session_token_usage' in globals():
        batch_tokens = session_token_usage['total_tokens']
        batch_cost = session_token_usage['total_cost']
    
    # Process instances in batches
    for batch_start in range(0, len(bert_flagged_instances), batch_size):
        batch_end = min(batch_start + batch_size, len(bert_flagged_instances))
        current_batch = bert_flagged_instances[batch_start:batch_end]
        
        logger.info(f"------- Processing batch {batch_start//batch_size + 1}: instances {batch_start+1}-{batch_end}/{len(bert_flagged_instances)} -------")
        
        # Construct a combined prompt for all instances in this batch
        combined_prompt = """

Determine if each of the following texts violates FINRA's communication rules by being false, misleading, promissory or exaggerated:

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)



CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.


--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant


--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

For each text, answer with ONLY "YES" (non-compliant) or "NO" (compliant), followed by a brief explanation.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
        
        for i, instance in enumerate(current_batch, 1):
            sentence = instance.get("flagged_instance", "")
            combined_prompt += f"\nTEXT {i}: \"{sentence}\"\n"
        
        combined_prompt += "\nRespond in this format:\nTEXT 1: YES/NO - Brief explanation\nTEXT 2: YES/NO - Brief explanation\n... and so on"
        
        try:
            # Make a single API call for the entire batch
            logger.info(f"Sending batch of {len(current_batch)} instances to Deepseek API")
            verification_response = call_deepseek_api(combined_prompt)
            logger.info(f"Received batch verification response from Deepseek API")
            
            # Extract individual responses for each instance
            response_lines = verification_response.strip().split('\n')
            responses_by_text = {}
            
            for line in response_lines:
                line = line.strip()
                # Match lines like "TEXT 1: YES - explanation" or "TEXT 2: NO - explanation"
                match = re.match(r'^TEXT\s+(\d+):\s+(.*?)$', line, re.IGNORECASE)
                if match:
                    text_num = int(match.group(1))
                    response_content = match.group(2).strip()
                    responses_by_text[text_num] = response_content
            
            # Process each instance based on the response
            for i, instance in enumerate(current_batch, 1):
                sentence = instance.get("flagged_instance", "")
                page_num = instance.get("page")
                confidence = instance.get("confidence", 0)
                
                logger.info(f"------- Instance {batch_start+i}/{len(bert_flagged_instances)} -------")
                logger.info(f"Page: {page_num}, Confidence: {confidence:.1%}")
                logger.info(f"Text: \"{sentence[:100]}{'...' if len(sentence) > 100 else ''}\"")
                
                # Get response for this specific instance
                instance_response = responses_by_text.get(i, "")
                logger.info(f"GPT verification response: {instance_response}")
                
                # Evaluate GPT's response
                is_non_compliant = False
                if instance_response:
                    # Check if the response starts with YES
                    first_word = instance_response.split('-')[0].strip().upper() if instance_response.strip() else ""
                    is_non_compliant = first_word == "YES"

                    # FOR LOW-CONFIDENCE BERT PREDICTIONS THAT COULD GO EITHER WAY, APPLY HIGHER SCRUTINY
                    if is_non_compliant:
                        if confidence < 0.7:
                            logger.info(f"Higher scrutiny Deepseek OVERRODE initial compliant decision")
                        else:
                            logger.info(f"GPT CONFIRMS non-compliance, adding to batch for alternative generation")
                            
                        # Send to Deepseek with higher scrutiny prompt
                        scrutiny_prompt = f"""
You are a VERY STRICT FINRA compliance reviewer. Apply maximum skepticism to this text:

"{sentence}"

Look carefully for:
- ANY implied guarantees or promises
- Subtle absolute language 
- Unqualified statements about benefits
- Lacks qualifying statements

Answer ONLY "YES" (flag it) or "NO" (it's fine), followed by brief explanation.
"""

                        scrutiny_response = call_deepseek_api(scrutiny_prompt)
                        if scrutiny_response:
                            scrutiny_decision = scrutiny_response.strip().split()[0].upper() if scrutiny_response.strip() else ""
                            if scrutiny_decision == "YES":
                                is_non_compliant = True
                                logger.info(f"Higher scrutiny Deepseek flagged low-confidence instance")

                    
                    # For high-confidence BERT predictions, verify we're not getting a false negative
                    if not is_non_compliant and confidence > 0.9:
                        # Look for known violation patterns in the text
                        violation_patterns = [
                            #r'\bwill\s+(?:provide|earn|gain|make|increase|grow|guarantee)',
                            #r'\bguarantee[ds]?\b',
                            #r'\balways\b',
                            #r'\bnever\b',
                            #r'\bcertain(?:ly)?\b',
                            #r'\bensure[ds]?\b'
                        ]
                        
                        for pattern in violation_patterns:
                            if re.search(pattern, sentence.lower()):
                                is_non_compliant = True
                                logger.info(f"Overriding GPT decision due to high confidence and violation pattern match")
                                break
                
                if is_non_compliant:
                    # Double check if confidence is below 70%
                    if confidence < 0.7:
                        confirmed = double_check_low_confidence(instance, confidence)
                        if not confirmed:
                            logger.info(f"‚úó Double check REJECTED low confidence flag ({confidence:.1%}) - not adding")
                            continue  # Skip this instance
                        else:
                            logger.info(f"‚úì Double check CONFIRMED low confidence flag ({confidence:.1%})")
    
                    logger.info(f"GPT CONFIRMS non-compliance, generating alternative...")
                    
                    # Individual API call to get a compliant alternative
                    alternative_prompt = f"""
This text violates FINRA rules: "{sentence}"

Rewrite it to be compliant by:
1. Replacing overstatements
2. Removing absolute statements
3. Adding appropriate qualifiers
4. Avoiding guarantees or promises
5. Maintaining the original meaning

Respond with ONLY the compliant alternative text and nothing else.
"""
                    
                    # Get direct alternative without needing complex parsing
                    compliant_text = call_deepseek_api(alternative_prompt).strip()
                    logger.info(f"Direct alternative received: {compliant_text}")
                    
                    # Validate the response - should be a simple text string
                    if not compliant_text or len(compliant_text) < 5:
                        logger.error("Got empty or too short alternative, using API fallback")
                        # Make one more attempt with a more explicit instruction
                        fallback_prompt = f"""
Rewrite this non-compliant financial text: "{sentence}"
Make it FINRA-compliant by removing any guarantees.
Return ONLY the rewritten text with no other text, explanations, or formatting.
"""
                        compliant_text = call_deepseek_api(fallback_prompt).strip()
                    
                    # Clean quotes if present
                    if compliant_text.startswith('"') and compliant_text.endswith('"'):
                        compliant_text = compliant_text[1:-1]
                    
                    # Use a simple standard rationale
                    rationale = "This text may contain absolutes, guarantees, or promissory statements not allowed under FINRA regulations."
                    
                    instance = {
                        "flagged_instance": sentence,
                        "compliance_status": "non-compliant",
                        "specific_compliant_alternative": compliant_text,
                        "rationale": rationale,
                        "page": page_num,
                        "confidence": f"{confidence:.1%}",
                        "gpt_verified": True
                    }
                    verified_instances.append(instance)
                    logger.info(f"‚úì GPT verified instance as non-compliant and added to results")
                else:
                    logger.info(f"‚úó GPT determined instance is COMPLIANT - FALSE POSITIVE from BERT - not flagging")
            
        except Exception as e:
            logger.error(f"Error processing batch: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # Fall back to processing this batch individually
            logger.warning(f"Batch processing failed, falling back to individual processing for batch {batch_start//batch_size + 1}")
            
            for instance in current_batch:
                sentence = instance.get("flagged_instance", "")
                page_num = instance.get("page")
                confidence = instance.get("confidence", 0)
                
                try:
                    logger.info(f"Individual processing for instance on page {page_num}")

                    # Then modify your verification prompt by adding this section:
                    skepticism_words = load_skepticism_words()
                    skepticism_instruction = f"""
                    SPECIAL ATTENTION: Apply higher skepticism when analyzing text containing these words in financial/business contexts: {', '.join(skepticism_words)}

                    These words often indicate promissory or absolute language when used in financial communications.
                    """
                    
                    # Use the original verification prompt for individual instance
                    verification_prompt = f"""
                    {skepticism_instruction}

Determine if this text violates FINRA's communication rules by being false, misleading, promissory, exaggerated, or contains profanity:

"{sentence}"

Answer with ONLY "YES" or "NO", followed by a brief explanation.
"YES" means it IS non-compliant or contains profanity.
"NO" means it IS compliant.

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)



CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

  IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.

--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant


--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
                    
                    verification_response = call_deepseek_api(verification_prompt)
                    logger.info(f"Individual verification response: {verification_response}")
                    
                    # Evaluate GPT's response for this individual instance
                    is_non_compliant = False
                    if verification_response:
                        first_word = verification_response.strip().split()[0].upper() if verification_response.strip() else ""
                        is_non_compliant = first_word == "YES"
                        
                        # For high-confidence BERT predictions, verify we're not getting a false negative
                        if not is_non_compliant and confidence > 0.9:
                            for pattern in violation_patterns:
                                if re.search(pattern, sentence.lower()):
                                    is_non_compliant = True
                                    logger.info(f"Overriding individual GPT decision due to high confidence")
                                    break
                    
                    if is_non_compliant:
                        # Process as non-compliant, following the original approach
                        alternative_prompt = f"""
This text violates FINRA rules: "{sentence}"

Rewrite it to be compliant by:
1. Replacing overstatements
2. Removing absolute statements
3. Adding appropriate qualifiers
4. Avoiding guarantees or promises
5. Maintaining the original meaning

Respond with ONLY the compliant alternative text and nothing else.
"""
                        compliant_text = call_deepseek_api(alternative_prompt).strip()
                        
                        # Clean and validate as in the original function
                        if not compliant_text or len(compliant_text) < 5:
                            fallback_prompt = f"""
Rewrite this non-compliant financial text: "{sentence}"
Make it FINRA-compliant by removing any guarantees.
Return ONLY the rewritten text with no other text, explanations, or formatting.
"""
                            compliant_text = call_deepseek_api(fallback_prompt).strip()
                        
                        if compliant_text.startswith('"') and compliant_text.endswith('"'):
                            compliant_text = compliant_text[1:-1]
                        
                        instance = {
                            "flagged_instance": sentence,
                            "compliance_status": "non-compliant",
                            "specific_compliant_alternative": compliant_text,
                            "rationale": "This text may contain absolutes, guarantees, or promissory statements not allowed under FINRA regulations.",
                            "page": page_num,
                            "confidence": f"{confidence:.1%}",
                            "gpt_verified": True
                        }
                        verified_instances.append(instance)
                        logger.info(f"‚úì Added instance as non-compliant after individual verification")
                    
                except Exception as individual_error:
                    logger.error(f"Error in individual processing: {individual_error}")
                    
                    # If all verification fails, use BERT judgment for high-confidence predictions
                    if confidence > 0.8:
                        logger.warning(f"All verification failed, using BERT judgment for high confidence ({confidence:.1%})")
                        fallback_alternative = "We may help clients pursue their investment objectives while understanding that all investments involve risk."
                        
                        instance = {
                            "flagged_instance": sentence,
                            "compliance_status": "non-compliant",
                            "specific_compliant_alternative": fallback_alternative,
                            "rationale": "This text may not comply with FINRA regulations (verification failed).",
                            "page": page_num,
                            "confidence": f"{confidence:.1%}",
                            "gpt_verified": False
                        }
                        verified_instances.append(instance)
                        logger.info(f"Added instance due to verification failure but high BERT confidence")
    
    # Calculate final token usage and costs
    batch_tokens_used = 0
    batch_final_cost = 0.0
    
    if 'session_token_usage' in globals():
        batch_tokens_used = session_token_usage['total_tokens'] - batch_tokens
        batch_final_cost = session_token_usage['total_cost'] - batch_cost
    
    logger.info(f"========== GPT VERIFICATION COMPLETED ==========")
    logger.info(f"Results: {len(bert_flagged_instances)} BERT flags ‚Üí {len(verified_instances)} verified flags")
    logger.info(f"Tokens used in this verification batch: {batch_tokens_used}")
    logger.info(f"Estimated cost for this batch: ${batch_final_cost:.6f}")
    
    return verified_instances


def verify_with_gpt(bert_flagged_instances):
    """Verifies BERT-flagged instances using Deepseek to reduce false positives."""
    verified_instances = []
    
    logger.info(f"========== GPT VERIFICATION STARTED ==========")
    logger.info(f"Verifying {len(bert_flagged_instances)} instances flagged by BERT")
    
    # Initialize batch metrics
    batch_tokens = 0
    batch_cost = 0.0
    
    for i, instance in enumerate(bert_flagged_instances, 1):
        sentence = instance.get("flagged_instance", "")
        page_num = instance.get("page")
        confidence = instance.get("confidence", 0)
        
        logger.info(f"------- Instance {i}/{len(bert_flagged_instances)} -------")
        logger.info(f"Page: {page_num}, Confidence: {confidence:.1%}")
        logger.info(f"Text: \"{sentence[:100]}{'...' if len(sentence) > 100 else ''}\"")
        
        try:
            logger.info(f"Verifying with GPT if instance is non-compliant...")

            # Then modify your verification prompt by adding this section:
            skepticism_words = load_skepticism_words()
            skepticism_instruction = f"""
            SPECIAL ATTENTION: Apply higher skepticism when analyzing text containing these words in financial/business contexts: {', '.join(skepticism_words)}

            These words often indicate promissory or absolute language when used in financial communications.
            """
                    
            # Use the original verification prompt for individual instance
            verification_prompt = f"""
            {skepticism_instruction}

Determine if this text violates FINRA's communication rules by being false, misleading, promissory, exaggerated, or contains profanity:

"{sentence}"

Answer with ONLY "YES" or "NO", followed by a brief explanation.
"YES" means it IS non-compliant or contains profanity.
"NO" means it IS compliant.

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)



CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.


--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant


--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
            
            
            verification_response = call_deepseek_api(verification_prompt)
            logger.info(f"GPT verification response: {verification_response}")
            
            # Track token usage from global session data
            if 'session_token_usage' in globals():
                batch_tokens = session_token_usage['total_tokens']
                batch_cost = session_token_usage['total_cost']
            
            # More strict evaluation of GPT's response
            is_non_compliant = False
            if verification_response:
                # Check if the response starts with YES
                first_word = verification_response.strip().split()[0].upper() if verification_response.strip() else ""
                is_non_compliant = first_word == "YES"
                
                # For high-confidence BERT predictions, verify we're not getting a false negative
                if not is_non_compliant and confidence > 0.9:
                    # Look for known violation patterns in the text
                    violation_patterns = [
                        #r'\bwill\s+(?:provide|earn|gain|make|increase|grow|guarantee)',
                        #r'\bguarantee[ds]?\b',
                        #r'\balways\b',
                        #r'\bnever\b',
                        #r'\bcertain(?:ly)?\b',
                        #r'\bensure[ds]?\b'
                    ]
                    
                    for pattern in violation_patterns:
                        if re.search(pattern, sentence.lower()):
                            is_non_compliant = True
                            logger.info(f"Overriding GPT decision due to high confidence and violation pattern match")
                            break
            
            if is_non_compliant:
                logger.info(f"GPT CONFIRMS non-compliance, generating alternative...")
                
                # Simplified prompt to get a direct response
                alternative_prompt = f"""
                This text violates FINRA rules: "{sentence}"
                
                Rewrite it to be compliant by:
                1. Replacing overstatements
                2. Removing absolute statements
                3. Adding appropriate qualifiers
                4. Avoiding guarantees or promises
                5. Maintaining the original meaning
                
                Respond with ONLY the compliant alternative text and nothing else.
                """
                
                # Get direct alternative without needing complex parsing
                compliant_text = call_deepseek_api(alternative_prompt).strip()
                logger.info(f"Direct alternative received: {compliant_text}")
                
                # Validate the response - should be a simple text string
                if not compliant_text or len(compliant_text) < 5:
                    logger.error("Got empty or too short alternative, using API fallback")
                    # Make one more attempt with a more explicit instruction
                    fallback_prompt = f"""
                    Rewrite this non-compliant financial text: "{sentence}"
                    Make it FINRA-compliant by removing any guarantees.
                    Return ONLY the rewritten text with no other text, explanations, or formatting.
                    """
                    compliant_text = call_deepseek_api(fallback_prompt).strip()
                
                # Clean quotes if present
                if compliant_text.startswith('"') and compliant_text.endswith('"'):
                    compliant_text = compliant_text[1:-1]
                
                # Use a simple standard rationale
                rationale = "This text may contain absolutes, guarantees, or promissory statements not allowed under FINRA regulations."
                
                instance = {
                    "flagged_instance": sentence,
                    "compliance_status": "non-compliant",
                    "specific_compliant_alternative": compliant_text,
                    "rationale": rationale,
                    "page": page_num,
                    "confidence": f"{confidence:.1%}",
                    "gpt_verified": True
                }
                verified_instances.append(instance)
                logger.info(f"‚úì GPT verified instance as non-compliant and added to results")
            else:
                logger.info(f"‚úó GPT determined instance is COMPLIANT - FALSE POSITIVE from BERT - not flagging")
                
        except Exception as e:
            logger.error(f"Error verifying with GPT: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            # If verification fails, fall back to BERT judgment for high-confidence predictions
            if confidence > 0.8:
                logger.warning(f"GPT verification failed, using BERT judgment for high confidence ({confidence:.1%})")
                # Direct API call for alternative
                fallback_prompt = f"""
                Rewrite this text to be FINRA-compliant: "{sentence}"
                Return ONLY the rewritten text.
                """
                fallback_alternative = call_deepseek_api(fallback_prompt).strip()
                
                if not fallback_alternative or len(fallback_alternative) < 5:
                    fallback_alternative = "We may help clients pursue their investment objectives while understanding that all investments involve risk."
                
                instance = {
                    "flagged_instance": sentence,
                    "compliance_status": "non-compliant",
                    "specific_compliant_alternative": fallback_alternative,
                    "rationale": "This text may not comply with FINRA regulations (GPT verification failed).",
                    "page": page_num,
                    "confidence": f"{confidence:.1%}",
                    "gpt_verified": False
                }
                verified_instances.append(instance)
                logger.info(f"Added instance due to verification failure but high BERT confidence")
    
    # Calculate final token usage and costs for this verification batch
    batch_tokens_used = 0
    batch_final_cost = 0.0
    
    if 'session_token_usage' in globals():
        batch_tokens_used = session_token_usage['total_tokens'] - batch_tokens
        batch_final_cost = session_token_usage['total_cost'] - batch_cost
    
    logger.info(f"========== GPT VERIFICATION COMPLETED ==========")
    logger.info(f"Results: {len(bert_flagged_instances)} BERT flags ‚Üí {len(verified_instances)} verified flags")
    logger.info(f"Tokens used in this verification batch: {batch_tokens_used}")
    logger.info(f"Estimated cost for this batch: ${batch_final_cost:.6f}")
    
    return verified_instances


def generate_fallback_alternative(non_compliant_text):
    """
    Generate a simple fallback alternative when GPT parsing fails,
    without using hardcoded pattern replacements.
    """
    try:
        # Try to get a compliant alternative using another API call
        prompt = f"""
        Convert this potentially non-compliant financial text into a FINRA-compliant version:
        
        "{non_compliant_text}"
        
        Make it compliant by:
        1. Removing absolute statements
        2. Adding appropriate qualifiers
        3. Avoiding guarantees
        4. Maintaining the original meaning as much as possible
        
        Return ONLY the compliant alternative text with no extra explanation.
        """
        
        compliant_version = call_deepseek_api(prompt).strip()
        
        # Check if we got a valid response
        if compliant_version and len(compliant_version) > 10:
            logger.info(f"Generated fallback using additional API call: {compliant_version[:100]}...")
            return compliant_version
    except Exception as e:
        logger.error(f"Error generating fallback with API: {e}")
    
    # If the API call fails or returns an invalid response, use a generic but still useful alternative
    logger.info("Using generic fallback text")
    return "We strive to help clients pursue their investment objectives while understanding that all investments involve risk."




def get_compliant_alternative(non_compliant_text):
    """Gets compliant alternative from database."""
    try:
        conn = psycopg2.connect(
            dbname="postgresql_instance_free",
            user="postgresql_instance_free_user",
            password="bz3SdnKi6g6TRdM4j1AtE2Ash8VNgiQO",
            host="dpg-cts4psa3esus73dn1cn0-a.oregon-postgres.render.com",
            port="5432"
        )
        cursor = conn.cursor()
        
        # Try to find exact match first
        cursor.execute("""
            SELECT compliant 
            FROM compliance_examples 
            WHERE non_compliant = %s
        """, (non_compliant_text,))
        
        result = cursor.fetchone()
        
        # If no exact match, try to find similar text
        if not result:
            cursor.execute("""
                SELECT compliant, non_compliant
                FROM compliance_examples
                ORDER BY similarity(non_compliant, %s) DESC
                LIMIT 1
            """, (non_compliant_text,))
            result = cursor.fetchone()
        
        cursor.close()
        conn.close()
        
        return result[0] if result else "Review and revise this content to ensure compliance with regulations."
        
    except Exception as e:
        logger.error(f"Error getting compliant alternative: {e}")
        return "Review and revise this content to ensure compliance with regulations."

    
def reconcile_compliance_checks(first_check, second_check):
    """
    Reconcile results from both compliance checks.
    Takes the more conservative approach when there are differences and includes all unique instances.
    """
    reconciled = {
        "compliant": True,
        "flagged_instances": [],
        "message": "",
        "additional_findings": []  # New field to track instances found only in second check
    }

    # Combine flagged instances from both checks
    all_instances = {}
    second_check_keys = set()
    
    # Process first check instances
    for instance in first_check.get('flagged_instances', []):
        key = instance.get('flagged_text', '').lower()
        all_instances[key] = {
            **instance,
            "found_in": ["first_check"]
        }

    # Process second check instances
    for instance in second_check.get('flagged_instances', []):
        key = instance.get('flagged_text', '').lower()
        second_check_keys.add(key)
        
        if key in all_instances:
            # Update existing instance
            all_instances[key]["found_in"].append("second_check")
            
            # Take the more conservative status
            current_status = all_instances[key].get('compliance_status', '').lower()
            new_status = instance.get('compliance_status', '').lower()
            if new_status == 'non-compliant' or current_status == 'non-compliant':
                all_instances[key]['compliance_status'] = 'non-compliant'
            elif new_status == 'partially compliant' or current_status == 'partially compliant':
                all_instances[key]['compliance_status'] = 'partially compliant'
            
            # Merge rationales if different
            if instance.get('rationale') and instance['rationale'] != all_instances[key].get('rationale'):
                all_instances[key]['rationale'] = f"First check: {all_instances[key].get('rationale', '')} | Second check: {instance['rationale']}"
        else:
            # New instance found only in second check
            instance['found_in'] = ["second_check"]
            all_instances[key] = instance
            reconciled['additional_findings'].append({
                'flagged_text': instance.get('flagged_text'),
                'compliance_status': instance.get('compliance_status'),
                'specific_compliant_alternative': instance.get('specific_compliant_alternative'),
                'rationale': instance.get('rationale')
            })

    # Convert to list and sort by compliance status severity
    def severity_score(status):
        status = status.lower()
        scores = {
            'non-compliant': 3,
            'partially compliant': 2,
            'more compliant': 1,
            'compliant': 0
        }
        return scores.get(status, 0)

    reconciled['flagged_instances'] = sorted(
        all_instances.values(),
        key=lambda x: severity_score(x.get('compliance_status', '')),
        reverse=True
    )

    # Add verification summary
    reconciled['verification_summary'] = {
        'total_unique_flags': len(all_instances),
        'flags_in_first_check': len(first_check.get('flagged_instances', [])),
        'flags_in_second_check': len(second_check.get('flagged_instances', [])),
        'additional_flags_from_second_check': len(reconciled['additional_findings']),
        'flags_found_in_both_checks': len([i for i in all_instances.values() if len(i['found_in']) > 1])
    }

    # Combine messages
    messages = []
    # Instead of using both checks' general messages, create one concise message with specific suggestions
    if reconciled['flagged_instances']:
        main_message = "This text could be improved in the following ways:"
        suggestions = []
        for instance in reconciled['flagged_instances']:
            if instance.get('specific_compliant_alternative'):
                suggestions.append(instance.get('specific_compliant_alternative'))
        
        reconciled['message'] = f"{main_message} {' '.join(suggestions)}"
    else:
        reconciled['message'] = "The text appears to be compliant with FINRA's communication rules."
    
    return reconciled

# Set the path for ffmpeg in pydub
AudioSegment.converter = "/opt/homebrew/bin/ffmpeg"


# APP = FLASK(__name__)
# Was app = Flask(__name__) MAY NEED TO DELETE STATIC IF NOT WORKING JUST ADDED 4/11/25
app = Flask(__name__, static_folder='static')

def perform_concurrent_compliance_checks(pages_data, max_workers=3):
    """
    Perform compliance checks on multiple pages concurrently
    
    Args:
        pages_data: List of dictionaries with page info
        max_workers: Maximum number of concurrent threads (default 3 to avoid overwhelming the API)
    
    Returns:
        Dictionary with results for each page
    """
    results = {}
    
    def check_single_page(page_data):
        """Check compliance for a single page"""
        try:
            page_url = page_data['url']
            content = page_data['content']
            page_num = page_data.get('page_num', 1)
            
            logger.info(f"üîÑ Starting concurrent check for: {page_url}")
            
            # Use your existing compliance check function
            compliance_result = perform_compliance_check(content, page_num)
            
            logger.info(f"‚úÖ Completed concurrent check for: {page_url}")
            
            return {
                'url': page_url,
                'result': compliance_result,
                'success': True
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error checking {page_data.get('url', 'unknown')}: {e}")
            return {
                'url': page_data.get('url', 'unknown'),
                'result': {"compliant": False, "error": str(e)},
                'success': False
            }
    
    # Execute compliance checks concurrently
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        logger.info(f"üöÄ Starting {len(pages_data)} concurrent compliance checks with {max_workers} workers")
        
        future_to_page = {
            executor.submit(check_single_page, page_data): page_data 
            for page_data in pages_data
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_page):
            page_data = future_to_page[future]
            try:
                result = future.result()
                results[result['url']] = result
                logger.info(f"üìã Collected result for: {result['url']}")
                
            except Exception as e:
                logger.error(f"‚ùå Future failed for {page_data.get('url', 'unknown')}: {e}")
                results[page_data.get('url', 'unknown')] = {
                    'url': page_data.get('url', 'unknown'),
                    'result': {"compliant": False, "error": str(e)},
                    'success': False
                }
    
    logger.info(f"üèÅ Completed all {len(pages_data)} concurrent compliance checks")
    return results


# Youtube video check

def fetch_youtube_videos(page_url: str, max_items: int = 25, timeout: int = 12):
    """Return a list of dicts: {id, title, url, published, description} via YouTube RSS."""
    import re, requests, xml.etree.ElementTree as ET
    from urllib.parse import urlparse
    ns = {
        'atom':  'http://www.w3.org/2005/Atom',
        'yt':    'http://www.youtube.com/xml/schemas/2015',
        'media': 'http://search.yahoo.com/mrss/',
    }

    # Try to resolve channel_id from URL (channel/, or scrape channelId from HTML)
    u = urlparse(page_url)
    channel_id = None
    if '/channel/' in u.path:
        channel_id = u.path.split('/channel/')[1].split('/')[0]
    if not channel_id:
        try:
            html = requests.get(page_url, timeout=timeout, headers={'User-Agent':'Mozilla/5.0'}).text
            m = re.search(r'"channelId":"(UC[\w-]{22})"', html)
            if m:
                channel_id = m.group(1)
        except Exception:
            pass
    if not channel_id:
        return []

    feed_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'
    try:
        resp = requests.get(feed_url, timeout=timeout, headers={'User-Agent':'Mozilla/5.0'})
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
        items = []
        for entry in root.findall('atom:entry', ns)[:max_items]:
            title = (entry.findtext('atom:title', default='', namespaces=ns) or '').strip()
            link_el = entry.find('atom:link', ns)
            link = link_el.get('href') if link_el is not None else ''
            vid  = entry.findtext('yt:videoId', default='', namespaces=ns) or ''
            pub  = entry.findtext('atom:published', default='', namespaces=ns) or ''
            desc = entry.findtext('media:group/media:description', default='', namespaces=ns) or ''
            items.append({'id': vid, 'title': title, 'url': link, 'published': pub, 'description': desc.strip()})
        return items
    except Exception:
        return []


def build_youtube_content_text(videos):
    """Format videos into lines your Domain Details page can split & render like posts."""
    lines = []
    for v in videos:
        lines.append(f"YouTube Video {v.get('id','')}: {v.get('title','').strip()}")
        if v.get('published'):
            lines.append(f"Published: {v['published']}")
        if v.get('url'):
            lines.append(f"Link: {v['url']}")
        if v.get('description'):
            lines.append(v['description'])
        lines.append("")  # blank line between items
    return "\n".join(lines).strip()

# Youtube Post Monitor

import hmac
import hashlib
import requests
from urllib.parse import urlencode
import time

# WebSub Configuration
WEBSUB_HUB_URL = "https://pubsubhubbub.appspot.com/subscribe"
WEBSUB_CALLBACK_SECRET = "your-secret-key-here"  
YOUR_DOMAIN = "https://yourdomain.com"

@app.route('/websub/callback', methods=['GET', 'POST'])
def websub_callback():
    """Handle WebSub subscription verification and notifications"""
    
    start_time = time.time()
    logger.info(f"üîî WebSub callback received: method={request.method}, IP={request.remote_addr}")
    
    if request.method == 'GET':
        # Subscription verification
        challenge = request.args.get('hub.challenge')
        mode = request.args.get('hub.mode')
        topic = request.args.get('hub.topic')
        lease_seconds = request.args.get('hub.lease_seconds')
        
        logger.info(f"üìã WebSub verification request:")
        logger.info(f"   - Mode: {mode}")
        logger.info(f"   - Topic: {topic}")
        logger.info(f"   - Challenge: {challenge[:20] if challenge else None}...")
        logger.info(f"   - Lease: {lease_seconds} seconds")
        
        if mode == 'subscribe' and challenge:
            # Extract channel ID from topic for logging
            if topic and 'channel_id=' in topic:
                channel_id = topic.split('channel_id=')[1]
                logger.info(f"‚úÖ WebSub subscription VERIFIED for channel {channel_id}")
                
                # Update database subscription status
                try:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        UPDATE youtube_subscriptions 
                        SET status = 'active', last_verification = NOW()
                        WHERE channel_id = %s
                    """, (channel_id,))
                    rows_updated = cursor.rowcount
                    conn.commit()
                    cursor.close()
                    release_db_connection(conn)
                    logger.info(f"üìä Database updated: {rows_updated} subscription(s) marked active")
                except Exception as e:
                    logger.error(f"‚ùå Database update failed during verification: {e}")
            
            elapsed = (time.time() - start_time) * 1000
            logger.info(f"‚è±Ô∏è  WebSub verification completed in {elapsed:.1f}ms")
            return challenge, 200
        else:
            logger.warning(f"‚ùå WebSub verification FAILED: mode={mode}, challenge_present={bool(challenge)}")
            elapsed = (time.time() - start_time) * 1000
            logger.warning(f"‚è±Ô∏è  WebSub verification failed in {elapsed:.1f}ms")
            return 'Bad Request', 400
    
    elif request.method == 'POST':
        # New content notification
        content_length = len(request.data)
        content_type = request.headers.get('Content-Type', 'unknown')
        signature = request.headers.get('X-Hub-Signature', 'none')
        
        logger.info(f"üì∫ WebSub notification received:")
        logger.info(f"   - Content-Length: {content_length} bytes")
        logger.info(f"   - Content-Type: {content_type}")
        logger.info(f"   - Signature: {signature[:20] if signature != 'none' else 'none'}...")
        
        try:
            # Verify the signature
            signature_start = time.time()
            signature_valid = verify_websub_signature(request.data, signature)
            signature_time = (time.time() - signature_start) * 1000
            
            if not signature_valid:
                logger.error(f"‚ùå WebSub signature verification FAILED (took {signature_time:.1f}ms)")
                logger.error(f"   - Received signature: {signature}")
                logger.error(f"   - Content preview: {request.data[:100]}...")
                return 'Unauthorized', 401
            
            logger.info(f"‚úÖ WebSub signature verified in {signature_time:.1f}ms")
            
            # Parse the Atom feed
            parse_start = time.time()
            content = request.data.decode('utf-8')
            logger.debug(f"üìÑ Raw XML content preview: {content[:200]}...")
            
            channel_id, new_videos = parse_youtube_atom_feed(content)
            parse_time = (time.time() - parse_start) * 1000
            
            logger.info(f"üîç Atom feed parsed in {parse_time:.1f}ms:")
            logger.info(f"   - Channel ID: {channel_id}")
            logger.info(f"   - Videos found: {len(new_videos) if new_videos else 0}")
            
            if new_videos:
                for i, video in enumerate(new_videos):
                    logger.info(f"   - Video {i+1}: {video.get('title', 'No title')} ({video.get('video_id', 'No ID')})")
            
            if channel_id and new_videos:
                # Find which advisor owns this channel
                advisor_lookup_start = time.time()
                advisor_id = get_advisor_for_channel(channel_id)
                advisor_lookup_time = (time.time() - advisor_lookup_start) * 1000
                
                logger.info(f"üë§ Advisor lookup completed in {advisor_lookup_time:.1f}ms: advisor_id={advisor_id}")
                
                if advisor_id:
                    logger.info(f"üöÄ Starting background processing for advisor {advisor_id}")
                    # Process new videos in background
                    threading.Thread(
                        target=process_new_youtube_videos,
                        args=(advisor_id, channel_id, new_videos),
                        daemon=True,
                        name=f"YouTubeProcessor-{channel_id}-{int(time.time())}"
                    ).start()
                else:
                    logger.warning(f"‚ö†Ô∏è No advisor found for channel {channel_id} - notification ignored")
            else:
                logger.info(f"‚ÑπÔ∏è No actionable content in notification (channel_id={channel_id}, videos={len(new_videos) if new_videos else 0})")
            
            elapsed = (time.time() - start_time) * 1000
            logger.info(f"‚è±Ô∏è  WebSub notification processed in {elapsed:.1f}ms total")
            return 'OK', 200
            
        except UnicodeDecodeError as e:
            logger.error(f"‚ùå Unicode decode error in WebSub notification: {e}")
            logger.error(f"   - Raw bytes: {request.data[:100]}...")
            return 'Bad Request', 400
        except Exception as e:
            elapsed = (time.time() - start_time) * 1000
            logger.error(f"‚ùå Error processing WebSub notification (after {elapsed:.1f}ms): {e}", exc_info=True)
            return 'Internal Server Error', 500

def verify_websub_signature(data, signature):
    """Verify WebSub HMAC signature with extensive logging"""
    logger.debug(f"üîê Verifying signature:")
    logger.debug(f"   - Data length: {len(data)} bytes")
    logger.debug(f"   - Signature format: {signature[:10] if signature else 'None'}...")
    
    if not signature or not signature.startswith('sha1='):
        logger.warning(f"‚ùå Invalid signature format: {signature}")
        return False
    
    try:
        expected_signature = 'sha1=' + hmac.new(
            WEBSUB_CALLBACK_SECRET.encode(),
            data,
            hashlib.sha1
        ).hexdigest()
        
        signatures_match = hmac.compare_digest(signature, expected_signature)
        logger.debug(f"üîê Signature comparison: {'‚úÖ MATCH' if signatures_match else '‚ùå MISMATCH'}")
        
        if not signatures_match:
            logger.debug(f"   - Expected: {expected_signature[:30]}...")
            logger.debug(f"   - Received: {signature[:30]}...")
        
        return signatures_match
    except Exception as e:
        logger.error(f"‚ùå Signature verification error: {e}")
        return False

def parse_youtube_atom_feed(xml_content):
    """Parse YouTube Atom feed with extensive logging"""
    parse_start = time.time()
    logger.info(f"üîç Starting Atom feed parsing ({len(xml_content)} chars)")
    
    try:
        from xml.etree import ElementTree as ET
        
        xml_parse_start = time.time()
        root = ET.fromstring(xml_content)
        xml_parse_time = (time.time() - xml_parse_start) * 1000
        logger.debug(f"üìÑ XML parsed in {xml_parse_time:.1f}ms, root tag: {root.tag}")
        
        # Extract channel ID
        channel_id = None
        channel_search_start = time.time()
        
        for link in root.findall('.//{http://www.w3.org/2005/Atom}link'):
            href = link.get('href', '')
            if 'channel_id=' in href:
                channel_id = href.split('channel_id=')[1].split('&')[0]
                logger.debug(f"üì∫ Channel ID found in link: {channel_id}")
                break
        
        channel_search_time = (time.time() - channel_search_start) * 1000
        logger.debug(f"üîç Channel ID search completed in {channel_search_time:.1f}ms")
        
        # Extract video entries
        video_search_start = time.time()
        videos = []
        entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')
        logger.debug(f"üìã Found {len(entries)} entry elements")
        
        for i, entry in enumerate(entries):
            video_id_elem = entry.find('.//{http://www.youtube.com/xml/schemas/2015}videoId')
            title_elem = entry.find('.//{http://www.w3.org/2005/Atom}title')
            published_elem = entry.find('.//{http://www.w3.org/2005/Atom}published')
            
            logger.debug(f"üìπ Entry {i+1}: videoId={'‚úÖ' if video_id_elem is not None else '‚ùå'}, title={'‚úÖ' if title_elem is not None else '‚ùå'}")
            
            if video_id_elem is not None and title_elem is not None:
                video = {
                    'video_id': video_id_elem.text,
                    'title': title_elem.text,
                    'url': f"https://www.youtube.com/watch?v={video_id_elem.text}",
                    'published': published_elem.text if published_elem is not None else None
                }
                videos.append(video)
                logger.debug(f"   ‚úÖ Video added: {video['title']} ({video['video_id']})")
        
        video_search_time = (time.time() - video_search_start) * 1000
        parse_total_time = (time.time() - parse_start) * 1000
        
        logger.info(f"üé¨ Atom parsing completed in {parse_total_time:.1f}ms:")
        logger.info(f"   - Video search: {video_search_time:.1f}ms")
        logger.info(f"   - Channel: {channel_id}")
        logger.info(f"   - Videos: {len(videos)}")
        
        return channel_id, videos
        
    except ET.ParseError as e:
        logger.error(f"‚ùå XML parsing error: {e}")
        logger.error(f"   - XML preview: {xml_content[:200]}...")
        return None, []
    except Exception as e:
        parse_time = (time.time() - parse_start) * 1000
        logger.error(f"‚ùå Error parsing YouTube Atom feed (after {parse_time:.1f}ms): {e}", exc_info=True)
        return None, []

def get_advisor_for_channel(channel_id):
    """Find advisor for channel with extensive logging"""
    lookup_start = time.time()
    logger.info(f"üë§ Looking up advisor for channel: {channel_id}")
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query_start = time.time()
        cursor.execute("""
            SELECT DISTINCT advisor_id 
            FROM youtube_videos 
            WHERE channel_url LIKE %s
        """, (f'%{channel_id}%',))
        
        result = cursor.fetchone()
        query_time = (time.time() - query_start) * 1000
        
        cursor.close()
        release_db_connection(conn)
        
        lookup_time = (time.time() - lookup_start) * 1000
        
        if result:
            logger.info(f"‚úÖ Advisor found in {lookup_time:.1f}ms: advisor_id={result[0]} (query: {query_time:.1f}ms)")
            return result[0]
        else:
            logger.warning(f"‚ùå No advisor found for channel {channel_id} (search took {lookup_time:.1f}ms)")
            return None
        
    except Exception as e:
        lookup_time = (time.time() - lookup_start) * 1000
        logger.error(f"‚ùå Error finding advisor for channel {channel_id} (after {lookup_time:.1f}ms): {e}", exc_info=True)
        return None

def process_new_youtube_videos(advisor_id, channel_id, new_videos):
    """Process new videos with extensive logging"""
    thread_name = threading.current_thread().name
    process_start = time.time()
    
    logger.info(f"üé• [{thread_name}] Starting video processing:")
    logger.info(f"   - Advisor ID: {advisor_id}")
    logger.info(f"   - Channel ID: {channel_id}")
    logger.info(f"   - Videos to process: {len(new_videos)}")
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        videos_processed = 0
        videos_skipped = 0
        videos_inserted = 0
        
        for i, video in enumerate(new_videos):
            video_start = time.time()
            logger.debug(f"üîç [{thread_name}] Processing video {i+1}/{len(new_videos)}: {video['title']}")
            
            # Check if we already have this video
            cursor.execute("""
                SELECT COUNT(*) FROM youtube_videos 
                WHERE video_url = %s AND advisor_id = %s
            """, (video['url'], advisor_id))
            
            existing_count = cursor.fetchone()[0]
            
            if existing_count == 0:
                # New video - save it
                try:
                    cursor.execute("""
                        INSERT INTO youtube_videos 
                        (advisor_id, channel_url, video_url, title, published_date, scraped_at)
                        VALUES (%s, %s, %s, %s, %s, NOW())
                    """, (
                        advisor_id,
                        f"https://www.youtube.com/channel/{channel_id}",
                        video['url'],
                        video['title'],
                        video.get('published')
                    ))
                    
                    videos_inserted += 1
                    video_time = (time.time() - video_start) * 1000
                    logger.info(f"‚úÖ [{thread_name}] Inserted video in {video_time:.1f}ms: {video['title']}")
                    
                except Exception as insert_error:
                    video_time = (time.time() - video_start) * 1000
                    logger.error(f"‚ùå [{thread_name}] Insert failed after {video_time:.1f}ms: {insert_error}")
            else:
                videos_skipped += 1
                video_time = (time.time() - video_start) * 1000
                logger.debug(f"‚è≠Ô∏è [{thread_name}] Skipped existing video in {video_time:.1f}ms: {video['title']}")
            
            videos_processed += 1
        
        # Commit all changes
        commit_start = time.time()
        conn.commit()
        commit_time = (time.time() - commit_start) * 1000
        
        cursor.close()
        release_db_connection(conn)
        
        process_time = (time.time() - process_start) * 1000
        
        logger.info(f"üé¨ [{thread_name}] Video processing completed in {process_time:.1f}ms:")
        logger.info(f"   - Total processed: {videos_processed}")
        logger.info(f"   - New videos inserted: {videos_inserted}")
        logger.info(f"   - Existing videos skipped: {videos_skipped}")
        logger.info(f"   - Database commit time: {commit_time:.1f}ms")
        
    except Exception as e:
        process_time = (time.time() - process_start) * 1000
        logger.error(f"‚ùå [{thread_name}] Error processing videos (after {process_time:.1f}ms): {e}", exc_info=True)

def subscribe_youtube_channel(advisor_id, channel_id, channel_url):
    """Subscribe to YouTube channel with extensive logging"""
    sub_start = time.time()
    logger.info(f"üì° Starting WebSub subscription:")
    logger.info(f"   - Advisor ID: {advisor_id}")
    logger.info(f"   - Channel ID: {channel_id}")
    logger.info(f"   - Channel URL: {channel_url}")
    
    try:
        topic_url = f"https://www.youtube.com/xml/feeds/videos.xml?channel_id={channel_id}"
        callback_url = f"{YOUR_DOMAIN}/websub/callback"
        
        logger.info(f"üìã Subscription details:")
        logger.info(f"   - Topic URL: {topic_url}")
        logger.info(f"   - Callback URL: {callback_url}")
        logger.info(f"   - Hub URL: {WEBSUB_HUB_URL}")
        
        # Subscription request
        data = {
            'hub.callback': callback_url,
            'hub.topic': topic_url,
            'hub.mode': 'subscribe',
            'hub.verify': 'async',
            'hub.secret': WEBSUB_CALLBACK_SECRET,
            'hub.lease_seconds': 864000  # 10 days
        }
        
        request_start = time.time()
        logger.debug(f"üåê Sending POST request to hub...")
        
        response = requests.post(WEBSUB_HUB_URL, data=data, timeout=10)
        request_time = (time.time() - request_start) * 1000
        
        logger.info(f"üì° Hub response received in {request_time:.1f}ms:")
        logger.info(f"   - Status Code: {response.status_code}")
        logger.info(f"   - Response Headers: {dict(response.headers)}")
        logger.info(f"   - Response Body: {response.text[:200] if response.text else 'Empty'}")
        
        if response.status_code == 202:
            logger.info(f"‚úÖ WebSub subscription request ACCEPTED for channel {channel_id}")
            
            # Store subscription in database
            db_start = time.time()
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO youtube_subscriptions 
                (advisor_id, channel_id, channel_url, subscribed_at, status)
                VALUES (%s, %s, %s, NOW(), 'pending')
                ON CONFLICT (channel_id) 
                DO UPDATE SET 
                    subscribed_at = NOW(),
                    status = 'pending',
                    advisor_id = %s
            """, (advisor_id, channel_id, channel_url, advisor_id))
            
            conn.commit()
            cursor.close()
            release_db_connection(conn)
            
            db_time = (time.time() - db_start) * 1000
            total_time = (time.time() - sub_start) * 1000
            
            logger.info(f"üìä Database updated in {db_time:.1f}ms")
            logger.info(f"üéâ Subscription process completed in {total_time:.1f}ms")
            
        else:
            total_time = (time.time() - sub_start) * 1000
            logger.error(f"‚ùå WebSub subscription REJECTED for {channel_id} after {total_time:.1f}ms:")
            logger.error(f"   - Status: {response.status_code}")
            logger.error(f"   - Response: {response.text}")
            
    except requests.exceptions.Timeout:
        total_time = (time.time() - sub_start) * 1000
        logger.error(f"‚è∞ WebSub subscription TIMEOUT for {channel_id} after {total_time:.1f}ms")
    except requests.exceptions.RequestException as e:
        total_time = (time.time() - sub_start) * 1000
        logger.error(f"üåê WebSub request ERROR for {channel_id} after {total_time:.1f}ms: {e}")
    except Exception as e:
        total_time = (time.time() - sub_start) * 1000
        logger.error(f"‚ùå WebSub subscription ERROR for {channel_id} after {total_time:.1f}ms: {e}", exc_info=True)
        

# Twitter 5 worker scraper
@app.route('/retrieve_all_twitter', methods=['POST'])
def retrieve_all_twitter():
    data = request.get_json()
    twitter_links = data.get("profiles", [])

    app.logger.info(f"üì• Incoming Twitter profiles: {len(twitter_links)} received")

    if not twitter_links:
        return jsonify({"success": False, "message": "No queued Twitter profiles received"}), 400

    import datetime
    today = datetime.date.today()
    app.logger.info(f"üìÖ Today = {today}")

    # Spin off background task with 5-worker Twitter scraper
    threading.Thread(
        target=lambda: asyncio.run(process_twitter_links(
            [(p["advisor_id"], p["url"]) for p in twitter_links]
        ))
    ).start()

    app.logger.info(f"üöÄ Started background task for {len(twitter_links)} Twitter profiles")
    return jsonify({
        "success": True,
        "message": f"Started retrieving {len(twitter_links)} queued Twitter profiles"
    })

import asyncio
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# Global pool of Twitter drivers (dynamic size)
twitter_driver_pool = []
twitter_pool_lock = threading.Lock()
twitter_pool_initialized = False

def initialize_twitter_driver_pool(num_workers):
    """Initialize Twitter driver pool with only the needed number of drivers"""
    global twitter_pool_initialized
    
    with twitter_pool_lock:
        if twitter_pool_initialized:
            return
            
        app.logger.info(f"üîß Initializing Twitter driver pool with {num_workers} drivers")
        
        for i in range(num_workers):
            options = Options()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument(f'--user-data-dir=/tmp/twitter_profile_{i}')
            
            driver = webdriver.Chrome(options=options)
            # Pre-login each driver to Twitter
            driver.get('https://x.com/login')
            time.sleep(5)  # Wait for manual login
            twitter_driver_pool.append(driver)
            app.logger.info(f"‚úÖ Twitter driver {i+1}/{num_workers} initialized")
            
        twitter_pool_initialized = True
        app.logger.info(f"üéâ Twitter driver pool initialization complete: {num_workers} drivers ready")

def get_twitter_driver_from_pool():
    """Get a Twitter driver from the pool"""
    with twitter_pool_lock:
        if twitter_driver_pool:
            driver = twitter_driver_pool.pop()
            app.logger.info(f"üì§ Retrieved driver from pool. Remaining: {len(twitter_driver_pool)}")
            return driver
        else:
            app.logger.warning("‚ö†Ô∏è No Twitter drivers available in pool")
            return None

def return_twitter_driver_to_pool(driver):
    """Return a Twitter driver to the pool"""
    with twitter_pool_lock:
        twitter_driver_pool.append(driver)
        app.logger.info(f"üì• Returned driver to pool. Total available: {len(twitter_driver_pool)}")

def cleanup_twitter_driver_pool():
    """Clean up all drivers in the pool"""
    global twitter_pool_initialized
    
    with twitter_pool_lock:
        app.logger.info(f"üßπ Cleaning up Twitter driver pool ({len(twitter_driver_pool)} drivers)")
        
        for driver in twitter_driver_pool:
            try:
                driver.quit()
            except:
                pass
        
        twitter_driver_pool.clear()
        twitter_pool_initialized = False
        app.logger.info("‚úÖ Twitter driver pool cleanup complete")
        
# Add this global variable at the top with your other globals
processed_show_more_tweets = set()

def click_show_more_buttons_bulk(tweet_elem, driver, twitter_url):
    """Bulk version with navigation detection and duplicate prevention"""
    try:
        # ‚úÖ NEW: Create a unique identifier for this tweet element
        try:
            # Try to get a unique identifier (tweet ID, timestamp, or text snippet)
            tweet_id = None
            
            # Method 1: Try to get tweet ID from URL links
            try:
                link_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'a[href*="/status/"]')
                href = link_elem.get_attribute('href')
                if '/status/' in href:
                    tweet_id = href.split('/status/')[-1].split('?')[0]
            except:
                pass
            
            # Method 2: Fallback to first few words of tweet text
            if not tweet_id:
                try:
                    text_elem = tweet_elem.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
                    tweet_text = text_elem.text.strip()
                    if tweet_text:
                        tweet_id = f"{twitter_url}_{tweet_text[:50]}"  # First 50 chars as ID
                except:
                    pass
            
            # Method 3: Last resort - use element location
            if not tweet_id:
                location = tweet_elem.location
                tweet_id = f"{twitter_url}_{location['x']}_{location['y']}"
            
            # Check if we've already processed this tweet's "Show more" buttons
            if tweet_id in processed_show_more_tweets:
                return  # Skip - already processed
                
        except Exception:
            return  # Can't identify tweet, skip to be safe
        
        xpath_selectors = [
            './/span[contains(text(), "Show more")]',
            './/div[contains(text(), "Show more")]',
            './/span[contains(text(), "show more")]',
            './/div[contains(text(), "show more")]'
        ]
        
        show_more_buttons = []
        for xpath in xpath_selectors:
            try:
                buttons = tweet_elem.find_elements(By.XPATH, xpath)
                show_more_buttons.extend(buttons)
            except:
                continue
        
        # Remove duplicates
        unique_buttons = []
        for button in show_more_buttons:
            if button not in unique_buttons:
                unique_buttons.append(button)
        
        if unique_buttons:
            # ‚úÖ NEW: Mark this tweet as processed BEFORE clicking
            processed_show_more_tweets.add(tweet_id)
            app.logger.info(f"üîÑ Processing 'Show more' for tweet: {tweet_id[:100]}...")
            
            for i, button in enumerate(unique_buttons):
                try:
                    if button.is_displayed() and button.is_enabled():
                        current_url = driver.current_url
                        
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                        time.sleep(random.uniform(0.5, 1.0))
                        
                        try:
                            button.click()
                        except:
                            driver.execute_script("arguments[0].click();", button)
                        
                        time.sleep(random.uniform(1.0, 2.0))
                        
                        # Check if we navigated to a different page
                        new_url = driver.current_url
                        if new_url != current_url:
                            app.logger.warning(f"‚ö†Ô∏è 'Show more' caused navigation from {current_url} to {new_url} - going back")
                            driver.back()
                            time.sleep(random.uniform(2.0, 3.0))
                            break  # Stop clicking more buttons in this tweet
                        
                except Exception as e:
                    continue
                    
    except Exception as e:
        pass
    
def extract_tweet_text_from_element_bulk(tweet_elem):
    """Bulk version of your tweet text extraction"""
    tweet_text = ""
    
    selectors_to_try = [
        '[data-testid="tweetText"]',
        '[dir="auto"]',
        '[lang]',
        'span[dir="auto"]'
    ]
    
    for selector in selectors_to_try:
        try:
            text_elem = tweet_elem.find_element(By.CSS_SELECTOR, selector)
            tweet_text = text_elem.text.strip()
            if tweet_text and len(tweet_text) >= 1:
                break
        except:
            continue
    
    if not tweet_text:
        full_text = tweet_elem.text.strip()
        lines = full_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if (line and 
                len(line) >= 1 and 
                not line.startswith('@') and 
                not line.startswith('¬∑') and
                not line.endswith('ago') and
                not line.isdigit() and
                line not in ['h', 'm', 's', 'Pinned', 'pinned']):
                tweet_text = line
                break
    
    return tweet_text

async def process_twitter_links(twitter_links):
    """Process Twitter links with dynamic workers (1-5) using full scrolling logic"""
    num_profiles = len(twitter_links)
    # Dynamic worker count: min of profiles or 5
    max_workers = min(num_profiles, 5)
    
    app.logger.info(f"üê¶ Starting Twitter bulk processing: {num_profiles} profiles with {max_workers} workers")

    # ‚úÖ NEW: Initialize the driver pool with only the needed number
    initialize_twitter_driver_pool(max_workers)

    
    def scrape_single_twitter_full(advisor_id, twitter_url):
        """Scrape a single Twitter profile using your full original logic"""
        driver = None
        try:
            # ‚úÖ NEW: Clear processed tweets set for this profile
            processed_show_more_tweets.clear()

            driver = get_twitter_driver_from_pool()
            if not driver:
                app.logger.error(f"‚ùå No Twitter driver available for {twitter_url}")
                return False
                
            app.logger.info(f"üê¶ Starting full scrape for: {twitter_url}")
            
            # Navigate to profile
            driver.get(twitter_url)
            time.sleep(random.uniform(3, 6))
            
            # **YOUR ORIGINAL SOPHISTICATED SCROLLING LOGIC**
            tweets = []
            max_scrolls = 20       # Your original value
            no_new_content_count = 0
            max_no_new_content = 4  # Your original value
            
            last_tweet_count = 0
            reading_break_counter = 0
            suspected_bottom_attempts = 0  # Your key bottom detection counter
            
            for scroll_attempt in range(max_scrolls):
                app.logger.info(f"Human-like scroll attempt {scroll_attempt + 1}/{max_scrolls} for {twitter_url}")
                
                # **HUMAN BEHAVIOR: Sometimes pause to "read" content**
                if scroll_attempt > 0 and random.random() < 0.3:  # 30% chance
                    reading_pause = random.uniform(2, 8)
                    app.logger.info(f"Taking a 'reading break' for {reading_pause:.1f} seconds...")
                    time.sleep(reading_pause)
                
                # Extract tweets at current position
                try:
                    tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                    app.logger.info(f"Found {len(tweet_elements)} tweet elements on page")
                    
                    # Process new tweets (avoid duplicates)
                    existing_tweet_texts = {tweet['text'] for tweet in tweets}
                    new_tweets_this_scroll = 0
                    
                    for tweet_elem in tweet_elements:
                        try:
                            # **YOUR ORIGINAL: Click 'Show more' buttons first**
                            #click_show_more_buttons_bulk(tweet_elem, driver)
                            click_show_more_buttons_bulk(tweet_elem, driver, twitter_url)
                            
                            # **YOUR ORIGINAL: IMPROVED TWEET TEXT EXTRACTION**
                            tweet_text = extract_tweet_text_from_element_bulk(tweet_elem)
                            
                            # Extract timestamp
                            try:
                                time_elem = tweet_elem.find_element(By.CSS_SELECTOR, 'time')
                                timestamp = time_elem.get_attribute('datetime')
                                if timestamp:
                                    from datetime import datetime
                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                    readable_timestamp = dt.strftime('%Y-%m-%d %H:%M:%S')
                                else:
                                    readable_timestamp = "Unknown"
                            except:
                                readable_timestamp = "Unknown"
                            
                            if (tweet_text and 
                                len(tweet_text) >= 1 and 
                                tweet_text not in existing_tweet_texts):
                                
                                tweets.append({
                                    'text': tweet_text,
                                    'timestamp': readable_timestamp,
                                    'type': 'tweet'
                                })
                                existing_tweet_texts.add(tweet_text)
                                new_tweets_this_scroll += 1
                                app.logger.info(f"Extracted new tweet: {tweet_text[:50]}...")
                                
                        except Exception as e:
                            app.logger.error(f"‚ùå Error extracting tweet from {twitter_url}: {e}")  # ‚úÖ NEW
                            continue
                    
                    app.logger.info(f"Added {new_tweets_this_scroll} new tweets this scroll (total: {len(tweets)})")
                    
                    # Check if we found new content
                    if new_tweets_this_scroll == 0:
                        no_new_content_count += 1
                        app.logger.info(f"No new content found. Count: {no_new_content_count}/{max_no_new_content}")
                    else:
                        no_new_content_count = 0
                    
                    if no_new_content_count >= max_no_new_content:
                        app.logger.info(f"Stopping: No new content found for {max_no_new_content} consecutive scrolls")
                        break
                            
                except Exception as e:
                    app.logger.error(f"Error scraping tweets at scroll position {scroll_attempt}: {e}")
                
                # **YOUR ORIGINAL: HUMAN-LIKE SCROLLING PATTERNS**
                previous_position = driver.execute_script("return window.pageYOffset;")
                
                # Choose random scrolling pattern
                scroll_pattern = random.choice(['small', 'medium', 'variable', 'burst'])
                
                if scroll_pattern == 'small':
                    scroll_distance = random.randint(300, 800)
                    driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                    pause_time = random.uniform(2, 4)
                elif scroll_pattern == 'medium':
                    scroll_distance = random.randint(800, 1500)
                    driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                    pause_time = random.uniform(1.5, 3.5)
                elif scroll_pattern == 'variable':
                    total_scroll = random.randint(600, 1200)
                    num_mini_scrolls = random.randint(2, 4)
                    scroll_per_mini = total_scroll // num_mini_scrolls
                    for mini_scroll in range(num_mini_scrolls):
                        driver.execute_script(f"window.scrollBy(0, {scroll_per_mini});")
                        time.sleep(random.uniform(0.2, 0.6))
                    pause_time = random.uniform(2, 4)
                elif scroll_pattern == 'burst':
                    scroll_distance = random.randint(1200, 2000)
                    driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                    pause_time = random.uniform(3, 7)
                
                time.sleep(pause_time)
                
                # **YOUR ORIGINAL: HUMAN BEHAVIOR: Occasional scroll back up**
                if random.random() < 0.15:  # 15% chance
                    scroll_back = random.randint(100, 400)
                    app.logger.info(f"Human behavior: Scrolling back up {scroll_back}px to 'recheck' something")
                    driver.execute_script(f"window.scrollBy(0, -{scroll_back});")
                    time.sleep(random.uniform(1, 3))
                    driver.execute_script(f"window.scrollBy(0, {scroll_back + random.randint(50, 200)});")
                    time.sleep(random.uniform(1, 2))
                
                # **YOUR ORIGINAL: HUMAN BEHAVIOR: Longer break every so often**
                reading_break_counter += 1
                if reading_break_counter >= random.randint(5, 8):
                    long_break = random.uniform(5, 12)
                    app.logger.info(f"Taking a longer 'reading/thinking' break for {long_break:.1f} seconds...")
                    time.sleep(long_break)
                    reading_break_counter = 0
                
                # **YOUR ORIGINAL: KEY BOTTOM DETECTION WITH SCROLL-UP STRATEGY**
                new_position = driver.execute_script("return window.pageYOffset;")
                if new_position == previous_position:
                    suspected_bottom_attempts += 1
                    app.logger.info(f"üîç SUSPECTED BOTTOM REACHED (attempt {suspected_bottom_attempts}) - Triggering content loading strategy...")
                    
                    # **YOUR ORIGINAL: MULTI-SCROLL UP THEN BOTTOM STRATEGY**
                    original_position = new_position
                    
                    # Phase 1: Multiple scrolls up with pauses to trigger lazy loading
                    app.logger.info("üìà Phase 1: Scrolling up multiple times to trigger content loading...")
                    scroll_up_attempts = random.randint(4, 7)  # 4-7 upward scrolls
                    
                    for up_attempt in range(scroll_up_attempts):
                        scroll_up_distance = random.randint(800, 1500)
                        driver.execute_script(f"window.scrollBy(0, -{scroll_up_distance});")
                        
                        up_pause = random.uniform(1.5, 3.5)
                        app.logger.info(f"   Scroll up #{up_attempt + 1}: {scroll_up_distance}px, pausing {up_pause:.1f}s")
                        time.sleep(up_pause)
                        
                        if random.random() < 0.3:
                            extra_pause = random.uniform(2, 5)
                            app.logger.info(f"   Extra pause: {extra_pause:.1f}s")
                            time.sleep(extra_pause)
                    
                    # Phase 2: Extended pause to let Twitter's lazy loading system work
                    major_pause = random.uniform(8, 15)
                    app.logger.info(f"‚è≥ Phase 2: Extended pause for {major_pause:.1f}s to let Twitter load more content...")
                    time.sleep(major_pause)
                    
                    # Phase 3: Scroll back to bottom (possibly with new content)
                    app.logger.info("üìâ Phase 3: Scrolling back to bottom to check for new content...")
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    
                    bottom_load_pause = random.uniform(5, 8)
                    app.logger.info(f"‚è≥ Waiting {bottom_load_pause:.1f}s for new content to load at bottom...")
                    time.sleep(bottom_load_pause)
                    
                    # Check if new content appeared
                    post_strategy_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                    if len(post_strategy_elements) > len(tweet_elements):
                        app.logger.info(f"‚úÖ SUCCESS! Found {len(post_strategy_elements) - len(tweet_elements)} more tweet elements after strategy")
                        suspected_bottom_attempts = 0  # Reset counter since we found new content
                        continue  # Continue with the main loop
                    else:
                        app.logger.info("‚ùå No new content found after up-scroll strategy")
                    
                    # If we've tried the strategy multiple times without success, we're probably done
                    if suspected_bottom_attempts >= 2:
                        app.logger.info(f"üèÅ Confirmed bottom reached after {suspected_bottom_attempts} attempts with loading strategy")
                        break
                    else:
                        app.logger.info(f"üîÑ Will try the loading strategy again if needed (attempt {suspected_bottom_attempts}/2)")
                else:
                    # We successfully scrolled, reset the suspected bottom counter
                    suspected_bottom_attempts = 0
                
                # **YOUR ORIGINAL: Random micro-movements**
                if random.random() < 0.1:  # 10% chance
                    micro_adjustment = random.randint(-50, 100)
                    driver.execute_script(f"window.scrollBy(0, {micro_adjustment});")
                    time.sleep(random.uniform(0.5, 1.5))
            
            # Save to database if we got tweets
            if tweets:
                content_text = f"Twitter profile: {twitter_url}\n\n"
                for i, tweet in enumerate(tweets, 1):
                    content_text += f"Tweet {i} ({tweet['timestamp']}):\n{tweet['text']}\n\n"
                
                conn = get_db_connection()
                cur = conn.cursor()
                
                content_hash = hashlib.md5(content_text.encode()).hexdigest()
                title = f"Twitter Profile - {len(tweets)} tweets"
                
                cur.execute("""
                    INSERT INTO website_snapshots 
                    (advisor_id, page_url, content_hash, page_title, content_text, last_checked, last_scan_checked)
                    VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
                    ON CONFLICT (advisor_id, page_url) DO UPDATE SET
                        content_hash = EXCLUDED.content_hash,
                        content_text = EXCLUDED.content_text,
                        last_scan_checked = NOW()
                """, (advisor_id, twitter_url, content_hash, title, content_text))
                
                conn.commit()
                release_db_connection(conn)
                
                # Update profile status to "monitored" 
                update_profile_status_in_db(advisor_id, twitter_url, "monitored")
                
                app.logger.info(f"‚úÖ Twitter saved: {twitter_url} ({len(tweets)} tweets)")
                return True
            else:
                app.logger.warning(f"‚ùå No tweets found for: {twitter_url}")
                return False
            
        except Exception as e:
            app.logger.error(f"‚ùå Twitter scrape error for {twitter_url}: {e}")
            return False
        finally:
            # ‚úÖ CRITICAL FIX: Always return driver to pool, even on errors
            if driver:
                try:
                    return_twitter_driver_to_pool(driver)
                    app.logger.info(f"üîÑ Driver returned to pool for {twitter_url}")
                except Exception as return_error:
                    app.logger.error(f"‚ùå Failed to return driver to pool: {return_error}")
                    # If we can't return it, try to quit it to free resources
                    try:
                        driver.quit()
                        app.logger.info(f"üõë Driver quit instead of returned for {twitter_url}")
                    except:
                        pass
    
        return False
    
    # Process with DYNAMIC concurrent workers
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        tasks = [
            executor.submit(scrape_single_twitter_full, advisor_id, twitter_url)
            for advisor_id, twitter_url in twitter_links
        ]
        
        completed = 0
        successful = 0
        
        for future in concurrent.futures.as_completed(tasks):
            completed += 1
            if future.result():
                successful += 1
            
            app.logger.info(f"üê¶ Twitter progress: {completed}/{len(twitter_links)} completed, {successful} successful")

    # ‚úÖ NEW: Clean up the driver pool when done
    #cleanup_twitter_driver_pool()
    app.logger.info(f"üéâ Twitter bulk processing complete: {successful}/{len(twitter_links)} successful using {max_workers} workers")
    
from flask import request, Response
import hashlib, hmac, xml.etree.ElementTree as ET

@app.route("/youtube/callback", methods=["GET", "POST"])
def youtube_callback():
    # 1) Verification ping (subscribe/unsubscribe)
    if request.method == "GET":
        mode   = request.args.get("hub.mode")
        token  = request.args.get("hub.verify_token", "")
        chall  = request.args.get("hub.challenge", "")
        lease  = request.args.get("hub.lease_seconds")
        ch_id  = request.args.get("channel_id")

        if token != YTHUB_VERIFY_TOKEN:
            logger.warning(f"‚ùå YT hub verify_token mismatch for channel {ch_id}")
            return ("", 403)

        logger.info(f"‚úÖ YT hub verified '{mode}' for channel={ch_id} lease={lease}")
        # MUST echo the challenge exactly as plain text
        return Response(chall, status=200, mimetype="text/plain")

    # 2) Content delivery (new uploads)
    raw = request.data or b""

    # Verify signature if we set a secret
    sig_header = request.headers.get("X-Hub-Signature", "")
    if YTHUB_SECRET:
        if not sig_header.startswith("sha1="):
            logger.warning("‚ùå Missing/invalid X-Hub-Signature")
            return ("", 403)
        expected = "sha1=" + hmac.new(YTHUB_SECRET.encode("utf-8"), raw, hashlib.sha1).hexdigest()
        if not hmac.compare_digest(expected, sig_header):
            logger.warning("‚ùå YT hub signature mismatch")
            return ("", 403)

    # We put these in the callback querystring when we subscribed
    advisor_id = request.args.get("advisor_id", type=int)
    channel_id = request.args.get("channel_id")
    page_url   = request.args.get("page_url")

    # Parse Atom XML
    ns = {
        "atom": "http://www.w3.org/2005/Atom",
        "yt":   "http://www.youtube.com/xml/schemas/2015",
    }
    try:
        root = ET.fromstring(raw)
    except Exception:
        logger.exception("‚ùå Bad YT hub XML")
        return ("", 400)

    entries = root.findall("atom:entry", ns)
    if not entries:
        # No entries ‚Äî can happen on deletes or empty batches
        return ("", 204)

    # Upsert each video into youtube_videos
    conn = get_db_connection()
    cur = conn.cursor()
    upserted = 0
    for e in entries:
        vid   = (e.find("yt:videoId", ns).text if e.find("yt:videoId", ns) is not None else None)
        title = (e.find("atom:title", ns).text if e.find("atom:title", ns) is not None else "") or ""
        pub   = (e.find("atom:published", ns).text if e.find("atom:published", ns) is not None else None)
        ch    = (e.find("yt:channelId", ns).text if e.find("yt:channelId", ns) is not None else None)
        if not channel_id and ch:
            channel_id = ch

        if not vid:
            continue

        video_url = f"https://www.youtube.com/watch?v={vid}"

        # If advisor_id is missing for some reason, you can look it up by channel_id here.
        if advisor_id is None:
            # Example fallback (optional): map channel_id -> latest advisor seen
            cur.execute("SELECT advisor_id, channel_url FROM youtube_videos WHERE channel_id = %s ORDER BY id DESC LIMIT 1", (channel_id,))
            row = cur.fetchone()
            if row:
                advisor_id = row[0]
                if not page_url:
                    page_url = row[1]

        if advisor_id is None or not channel_id:
            logger.warning(f"‚ö†Ô∏è Skipping YT push entry; missing advisor_id or channel_id (vid={vid})")
            continue

        cur.execute("""
            INSERT INTO youtube_videos
                (advisor_id, channel_id, channel_url, video_id, title, description, published_at, video_url)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (advisor_id, video_id) DO UPDATE
               SET title        = EXCLUDED.title,
                   description  = EXCLUDED.description,
                   published_at = EXCLUDED.published_at,
                   video_url    = EXCLUDED.video_url
        """, (
            advisor_id, channel_id, page_url, vid, title, "", pub, video_url
        ))
        upserted += 1

    conn.commit()
    cur.close()
    release_db_connection(conn)

    logger.info(f"üì• YT hub delivered {upserted} entries (advisor={advisor_id}, channel={channel_id})")
    return ("", 204)  # No body required



# --- API: save/merge advisor profiles ---
from psycopg2.extras import Json

@app.route('/api/advisors/<int:advisor_id>/profiles', methods=['POST'])
def save_advisor_profiles(advisor_id):
    import json
    payload = request.get_json(silent=True) or {}
    # Accept a single profile or a list
    new_profiles = payload.get('profiles') or []
    if isinstance(new_profiles, dict):
        new_profiles = [new_profiles]
    merge = payload.get('merge', True)

    # basic validation + sanitization
    cleaned = []
    for p in new_profiles:
        if not p: 
            continue
        plat = (p.get('platform') or '').strip().lower()
        url  = (p.get('url') or '').strip()
        if not plat or not url: 
            continue
        cleaned.append({
            'platform': plat,
            'url': url,
            'name': p.get('name') or (plat.capitalize()),
            'verified': bool(p.get('verified', True)),
            'status': p.get('status') or 'queued'
        })
    if not cleaned:
        return jsonify(success=False, error='No valid profiles in request'), 400

    conn = get_db_connection()
    cur = conn.cursor()

    # helpers to detect schema
    def table_exists(name):
        cur.execute("""
            SELECT EXISTS (
              SELECT 1 FROM information_schema.tables
              WHERE table_schema = current_schema() AND table_name = %s
            )
        """, (name,))
        return bool(cur.fetchone()[0])

    def has_column(table, col):
        cur.execute("""
            SELECT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_schema = current_schema() AND table_name = %s AND column_name = %s
            )
        """, (table, col))
        return bool(cur.fetchone()[0])

    # prefer advisor_profiles.profiles_data
    target_table = None
    if table_exists('advisor_profiles') and has_column('advisor_profiles', 'profiles_data'):
        target_table = 'advisor_profiles'
    elif table_exists('advisors') and has_column('advisors', 'profiles_data'):
        target_table = 'advisors'
    else:
        conn.rollback()
        cur.close()
        conn.close()
        return jsonify(success=False, error='No JSONB profiles_data column found'), 500

    # fetch current profiles
    if target_table == 'advisor_profiles':
        cur.execute("SELECT profiles_data FROM advisor_profiles WHERE advisor_id=%s", (advisor_id,))
    else:
        cur.execute("SELECT profiles_data FROM advisors WHERE id=%s", (advisor_id,))
    row = cur.fetchone()
    existing = row[0] if row and row[0] else []

    # psycopg2 may return str if column is text; coerce to list
    if isinstance(existing, str):
        try: existing = json.loads(existing)
        except Exception: existing = []
    if not isinstance(existing, list):
        existing = []

    # merge (dedupe by (platform, canonical_url))
    def canon_url(plat, url):
        # tiny server-side canonicalizer to reduce dupes
        try:
            from urllib.parse import urlparse
            u = urlparse(url)
            host = u.hostname or ''
            host = host.replace('www.', '').replace('m.', '')
            scheme = 'https'
            path = u.path.rstrip('/')
            if plat == 'twitter':
                host = 'x.com'
                path = '/' + (path.split('/')[1] if path.split('/') and len(path.split('/')) > 1 else '')
            return f"{scheme}://{host}{path or ''}"
        except Exception:
            return url.strip()

    existing_keys = {(p.get('platform'), canon_url(p.get('platform',''), p.get('url',''))) for p in existing if isinstance(p, dict)}
    for p in cleaned:
        key = (p['platform'], canon_url(p['platform'], p['url']))
        if key not in existing_keys:
            existing.append(p)
            existing_keys.add(key)
        elif not merge:
            # replace behavior if wanted later
            pass

    # upsert
    def has_column(table, col):
        cur.execute("""
            SELECT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_schema = current_schema()
                AND table_name = %s
                AND column_name = %s
            )
        """, (table, col))
        return bool(cur.fetchone()[0])

    if target_table == 'advisor_profiles':
        has_updated_at = has_column('advisor_profiles', 'updated_at')

        # Does a row for this advisor already exist?
        cur.execute("SELECT 1 FROM advisor_profiles WHERE advisor_id=%s LIMIT 1", (advisor_id,))
        exists = cur.fetchone() is not None

        if exists:
            if has_updated_at:
                cur.execute("""
                    UPDATE advisor_profiles
                       SET profiles_data = %s,
                           updated_at    = NOW()
                     WHERE advisor_id   = %s
                """, (Json(existing), advisor_id))
            else:
                cur.execute("""
                    UPDATE advisor_profiles
                       SET profiles_data = %s
                     WHERE advisor_id   = %s
                """, (Json(existing), advisor_id))
        else:
            if has_updated_at:
                cur.execute("""
                    INSERT INTO advisor_profiles (advisor_id, profiles_data, updated_at)
                    VALUES (%s, %s, NOW())
                """, (advisor_id, Json(existing)))
            else:
                cur.execute("""
                    INSERT INTO advisor_profiles (advisor_id, profiles_data)
                    VALUES (%s, %s)
                """, (advisor_id, Json(existing)))
    else:
        # advisors.profiles_data fallback
        cur.execute("UPDATE advisors SET profiles_data=%s WHERE id=%s",
                    (Json(existing), advisor_id))


    conn.commit()
    cur.close()
    conn.close()
    return jsonify(success=True, saved=len(cleaned), total=len(existing))


# Track daily usage in memory (better to persist in DB/Redis if multi-instance)
DAILY_LIMIT = 2500
fb_usage = {"date": None, "count": 0}

@app.route('/retrieve_all_facebook', methods=['POST'])
def retrieve_all_facebook():
    data = request.get_json()
    fb_links = data.get("profiles", [])

    app.logger.info(f"üì• Incoming Facebook profiles: {len(fb_links)} received")

    if not fb_links:
        return jsonify({"success": False, "message": "No queued Facebook profiles received"}), 400

    # Fix date
    import datetime
    today = datetime.date.today()
    app.logger.info(f"üìÖ Today = {today}")

    # Spin off background task with 15-worker scraper
    threading.Thread(
        target=lambda: asyncio.run(process_facebook_links(
            [(p["advisor_id"], p["url"]) for p in fb_links]
        ))
    ).start()

    app.logger.info(f"üöÄ Started background task for {len(fb_links)} profiles")
    return jsonify({
        "success": True,
        "message": f"Started retrieving {len(fb_links)} queued Facebook profiles"
    })



async def process_facebook_links(fb_links):
    """Run up to 15 workers in parallel, refill until all are processed."""
    sem = asyncio.Semaphore(10)  # pool size

    async def worker(advisor_id, fb_url, session):
        async with sem:
            try:
                count = await fetch_facebook_page(session, fb_url, advisor_id)
                print(f"‚úÖ {fb_url} ‚Üí {count} posts saved")
            except Exception as e:
                print(f"‚ùå Error fetching {fb_url}: {e}")

    connector = aiohttp.TCPConnector(limit_per_host=15)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [worker(advisor_id, fb_url, session) for advisor_id, fb_url in fb_links]
        await asyncio.gather(*tasks)


# Youtube Bulk Scraping

def get_queued_counts_from_db():
    """Get accurate queued counts from database"""
    queued_counts = {}
    
    cursor.execute("SELECT advisor_id, profiles_data FROM advisor_profiles")
    results = cursor.fetchall()
    
    for advisor_id, profiles_data in results:
        count = 0
        if profiles_data:
            for profile in profiles_data:
                platform = profile.get('platform', '')
                url = profile.get('url', '')
                
                # Check if this profile is already monitored
                is_monitored = False
                
                if platform == 'youtube' or 'youtube' in url:
                    cursor.execute("SELECT COUNT(*) FROM youtube_videos WHERE channel_url = ? OR video_url = ?", (url, url))
                    is_monitored = cursor.fetchone()[0] > 0
                
                # Add more platform checks as needed
                
                if not is_monitored:
                    count += 1
        
        queued_counts[advisor_id] = count
    
    return queued_counts

@app.route('/retrieve_all_youtube', methods=['POST'])
def retrieve_all_youtube():
    data = request.get_json()
    yt_links = data.get("profiles", [])

    app.logger.info(f"üì• Incoming YouTube profiles: {len(yt_links)} received")

    if not yt_links:
        return jsonify({"success": False, "message": "No queued YouTube profiles received"}), 400

    # Fix date
    import datetime
    today = datetime.date.today()
    app.logger.info(f"üìÖ Today = {today}")

    # Spin off background task with 15-worker scraper
    threading.Thread(
        target=lambda: asyncio.run(process_youtube_links(
            [(p["advisor_id"], p["url"]) for p in yt_links]
        ))
    ).start()

    app.logger.info(f"üöÄ Started background task for {len(yt_links)} profiles")
    return jsonify({
        "success": True,
        "message": f"Started retrieving {len(yt_links)} queued YouTube profiles"
    })


async def process_youtube_links(yt_links):
    """Run up to 15 workers in parallel, refill until all are processed."""
    sem = asyncio.Semaphore(10)  # pool size

    async def worker(advisor_id, yt_url, session):
        async with sem:
            try:
                count = await fetch_youtube_page(session, yt_url, advisor_id)
                print(f"‚úÖ {yt_url} ‚Üí {count} posts saved")
            except Exception as e:
                print(f"‚ùå Error fetching {yt_url}: {e}")

    connector = aiohttp.TCPConnector(limit_per_host=15)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [worker(advisor_id, yt_url, session) for advisor_id, yt_url in yt_links]
        await asyncio.gather(*tasks)


async def fetch_youtube_page(session, youtube_url, advisor_id):
    """Fetch YouTube channel/page content and save posts to database."""
    try:
        print(f"üé• Fetching YouTube: {youtube_url}")
        
        # Run the synchronous YouTube functions in a thread pool to avoid blocking
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        def sync_youtube_processing():
            try:
                # Try API first (like your add_site does)
                videos = fetch_youtube_videos_api(youtube_url, max_items=250)
                print(f"üéØ YouTube API returned {len(videos)} videos")
            except Exception as api_err:
                print(f"‚ö†Ô∏è YouTube API failed: {api_err}. Falling back to Atom feed.")
                videos = fetch_youtube_videos(youtube_url, max_items=25)
                print(f"üìª Atom feed returned {len(videos)} videos")
            
            # Use your existing save function
            saved = save_youtube_rows(advisor_id, youtube_url, "YouTube Channel", videos, snapshot_limit=0)
            print(f"üé• Saved {saved} YouTube videos to database")
            
            # Update advisor_profiles status to monitored
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('''
                    UPDATE advisor_profiles 
                    SET profiles_data = (
                        SELECT jsonb_agg(
                            CASE 
                                WHEN elem->>'url' = %s 
                                THEN jsonb_set(elem, '{status}', '"monitored"')
                                ELSE elem
                            END
                        )
                        FROM jsonb_array_elements(profiles_data) AS elem
                    )
                    WHERE advisor_id = %s AND profiles_data IS NOT NULL
                ''', (youtube_url, advisor_id))
                conn.commit()
                cursor.close()
                release_db_connection(conn)
            except Exception as e:
                print(f"Error updating advisor_profiles status: {e}")
            
            return saved
        
        # Run the synchronous work in a thread pool
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            saved = await loop.run_in_executor(executor, sync_youtube_processing)
        
        return saved
        
    except Exception as e:
        print(f"‚ùå Error fetching YouTube {youtube_url}: {e}")
        return 0
    
@app.route('/get_queued_youtube_profiles', methods=['GET'])
def get_queued_youtube_profiles():
    """Get queued YouTube profiles from database with optional limit"""
    conn = None
    try:
        # Get limit from query parameter (default 50 for batching)
        limit = request.args.get('limit', 50, type=int)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT advisor_id, profiles_data 
            FROM advisor_profiles 
            WHERE profiles_data::text LIKE '%youtube%'
        """)
        
        results = cursor.fetchall()
        queued_youtube = []
        
        for advisor_id, profiles_data in results:
            if profiles_data:
                for profile in profiles_data:
                    if (profile.get('platform') == 'youtube' or 
                        'youtube.com' in profile.get('url', '') or 
                        'youtu.be' in profile.get('url', '')):
                        
                        cursor.execute("""
                            SELECT COUNT(*) FROM youtube_videos 
                            WHERE (channel_url = %s OR video_url = %s) AND advisor_id = %s
                        """, (profile['url'], profile['url'], advisor_id))
                        
                        count = cursor.fetchone()[0]
                        
                        if count == 0:  # Not monitored yet = queued
                            queued_youtube.append({
                                'advisor_id': advisor_id,
                                'url': profile['url'],
                                'title': profile.get('name', 'YouTube Profile')
                            })
                            
                            # Stop when we reach the limit
                            if len(queued_youtube) >= limit:
                                break
                
                # Break outer loop too if we've reached limit
                if len(queued_youtube) >= limit:
                    break
        
        return jsonify({
            'success': True,
            'profiles': queued_youtube,
            'count': len(queued_youtube),
            'limit_applied': limit,
            'batch_complete': len(queued_youtube) < limit  # True if this was the last batch
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
    
    finally:
        if conn and connection_pool:
            connection_pool.putconn(conn)
            
            
@app.route('/run_concurrent_compliance_check', methods=['POST'])
def run_concurrent_compliance_check():
    """Run compliance checks on multiple pages concurrently"""
    try:
        data = request.json
        advisor_id = data.get('advisor_id')
        pages_to_check = data.get('pages', [])
        max_workers = data.get('max_workers', 3)
        
        if not advisor_id:
            return jsonify({'error': 'Advisor ID is required'}), 400
        
        if not pages_to_check:
            return jsonify({'error': 'No pages specified for checking'}), 400
        
        logger.info(f"üéØ Starting concurrent compliance check for advisor {advisor_id}")
        logger.info(f"üìÑ Pages to check: {len(pages_to_check)}")
        logger.info(f"üîß Max workers: {max_workers}")
        
        # Get page content from website_snapshots table
        pages_data = []
        conn = get_db_connection()
        c = conn.cursor()
        
        for page_url in pages_to_check:
            try:
                # Get the latest content for this page
                c.execute('''
                    SELECT content_text FROM website_snapshots 
                    WHERE advisor_id = ? AND page_url = ? 
                    ORDER BY last_checked DESC LIMIT 1
                ''', (advisor_id, page_url))
                
                result = c.fetchone()
                if result and result[0]:
                    content = result[0]
                    pages_data.append({
                        'url': page_url,
                        'content': content,
                        'page_num': len(pages_data) + 1
                    })
                    logger.info(f"üìÑ Added page for checking: {page_url} (content length: {len(content)} chars)")
                else:
                    logger.warning(f"‚ö†Ô∏è  No content found for page: {page_url}")
                    
            except Exception as e:
                logger.error(f"‚ùå Error preparing {page_url}: {e}")
        
        c.close()
        release_db_connection(conn)
        
        if not pages_data:
            return jsonify({'error': 'No valid pages found for checking'}), 400

        # ‚úÖ Deduplicate by normalized URL, but keep the first version passed from the UI
        normalized_pages = {}
        for p in pages_data:
            norm_url = p['url'].rstrip('/')  # normalize "a.com/" ‚Üí "a.com"
            if norm_url not in normalized_pages:
                # keep the first version that came from UI (visible + used in compliance check)
                normalized_pages[norm_url] = p
            else:
                logger.info(f"üîÑ Deduped '{p['url']}' ‚Üí kept '{normalized_pages[norm_url]['url']}'")

        pages_data = list(normalized_pages.values())
        logger.info(f"‚úÖ Deduplicated to {len(pages_data)} unique visible pages for advisor {advisor_id}")
        
        # Perform concurrent compliance checks
        start_time = time.time()
        logger.info(f"üöÄ Starting concurrent compliance checks for {len(pages_data)} pages with {max_workers} workers")
        concurrent_results = perform_concurrent_compliance_checks(pages_data, max_workers)
        end_time = time.time()
        
        # Store results in compliance_issues table
        total_issues = 0
        processed_pages = 0
        
        conn = sqlite3.connect('advisor_monitor.db')
        c = conn.cursor()
        
        for page_url, result_data in concurrent_results.items():
            try:
                if result_data['success']:
                    compliance_result = result_data['result']
                    
                    # Clear existing issues for this page
                    c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, page_url))
                    
                    # Store new issues if any
                    if not compliance_result.get("compliant", True):
                        flagged_instances = compliance_result.get("flagged_instances", [])
                        
                        for instance in flagged_instances:
                            c.execute('''
                                INSERT INTO compliance_issues 
                                (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                VALUES (?, ?, ?, ?, ?, ?)
                            ''', (
                                advisor_id,
                                page_url,
                                instance.get("flagged_instance", ""),
                                instance.get("specific_compliant_alternative", ""),
                                instance.get("confidence", ""),
                                instance.get("rationale", "")
                            ))
                        
                        total_issues += len(flagged_instances)
                        logger.info(f"üö® Added {len(flagged_instances)} issues for {page_url}")
                    
                    # Update the compliance_checked_at timestamp
                    c.execute('''
                        UPDATE website_snapshots 
                        SET compliance_checked_at = CURRENT_TIMESTAMP 
                        WHERE advisor_id = ? AND page_url = ?
                    ''', (advisor_id, page_url))
                    
                    processed_pages += 1
                    
                else:
                    logger.error(f"‚ùå Failed to process {page_url}: {result_data['result']}")
                    
            except Exception as e:
                logger.error(f"‚ùå Error storing results for {page_url}: {e}")
        
        conn.commit()
        conn.close()
        
        # Calculate performance metrics
        total_time = end_time - start_time
        avg_time_per_page = total_time / len(pages_data) if pages_data else 0
        
        logger.info(f"üèÅ Concurrent compliance check completed:")
        logger.info(f"   üìä Total time: {total_time:.2f} seconds")
        logger.info(f"   üìà Average per page: {avg_time_per_page:.2f} seconds")
        logger.info(f"   ‚úÖ Processed pages: {processed_pages}/{len(pages_data)}")
        logger.info(f"   üö® Total issues found: {total_issues}")
        
        return jsonify({
            'success': True,
            'message': f'Concurrent compliance check completed in {total_time:.2f} seconds',
            'results': {
                'total_pages': len(pages_data),
                'processed_pages': processed_pages,
                'total_issues': total_issues,
                'total_time': round(total_time, 2),
                'avg_time_per_page': round(avg_time_per_page, 2),
                'concurrent_results': concurrent_results
            }
        })
        
    except Exception as e:
        logger.error(f"‚ùå Error in concurrent compliance check: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': str(e)}), 500
    

app.config['SESSION_TYPE'] = 'filesystem'
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(days=3650)  # or however long you want sessions to last
app.config['SESSION_USE_SIGNER'] = True  # adds a cryptographic signature to cookies
app.config['SESSION_COOKIE_SECURE'] = True  # only send cookies over HTTPS
app.config['SESSION_COOKIE_HTTPONLY'] = True  # prevent JavaScript access to session cookies
app.config['SESSION_COOKIE_SAMESITE'] = 'Lax'  # controls how cookies are sent with cross-site requests


# This is for POSTGRE LOGIN
app.secret_key = 'your-secret-key-change-this-in-production'
app.config['SESSION_COOKIE_HTTPONLY'] = True
app.config['SESSION_COOKIE_SECURE'] = True  # Only if using HTTPS

# Set up rate limiting
limiter = Limiter(
    get_remote_address,
    app=app,
    default_limits=["200 per minute"],
    storage_uri="memory://"
)


# PASTED App.py HERE

GOOGLE_API_KEY = "AIzaSyATnWGLnJZMiq4kdqMKOzPdqQ4l6SkGhnY"
SEARCH_ENGINE_ID = "c2bf5ee9fed774e3d"

DEEPSEEK_API_KEY = "sk-236ec31139a5435fa2b4720c53601c09"
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url="https://api.deepseek.com/v1"
)

OPENAI_API_KEY = "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"  # Add your OpenAI key
openai_client = OpenAI(
    api_key=OPENAI_API_KEY
)

PERPLEXITY_API_KEY = "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"

#CLAUDE_API_KEY = "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"
#claude_client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)

TAVILY_API_KEY = "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"
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

def test_tavily_capabilities():
    try:
        # Test if Tavily actually works
        test_results = tavily_client.search(
            query="test search google.com",
            max_results=3
        )
        print("TAVILY TEST RESULTS:", test_results)
        return True
    except Exception as e:
        print("TAVILY TEST ERROR:", e)
        return False



def test_claude_capabilities():
    try:
        # Test what tools are actually available
        message = claude_client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=200,
            messages=[{
                "role": "user", 
                "content": "Do you have access to web search or any tools to browse the internet? List all tools you have access to."
            }]
        )
        print("CLAUDE AVAILABLE TOOLS:", message.content[0].text)
        return message.content[0].text
    except Exception as e:
        print("CLAUDE TEST ERROR:", e)
        return None

import smtplib
from email.mime.text import MIMEText

def send_sms_notification(advisor_name):
    # Email-to-SMS gateway addresses (replace with your carrier)
    carriers = {
        'verizon': '@vtext.com',
        'att': '@txt.att.net', 
        'tmobile': '@tmomail.net',
        'sprint': '@messaging.sprintpcs.com'
    }
    
    your_phone = "7014291903"  # Your phone number
    carrier = "verizon"  # Your carrier
    
    to_email = your_phone + carriers[carrier]
    
    msg = MIMEText(f"New advisor added: {advisor_name}")
    msg['Subject'] = "New Advisor Alert"
    msg['From'] = "riley.r.giauque@gmail.com"
    msg['To'] = to_email
    
    try:
        # Use Gmail SMTP (or your email provider)
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login("riley.r.giauque@gmail.com", "qopp eyuq htdw unsv")
        server.send_message(msg)
        server.quit()
    except Exception as e:
        print(f"SMS notification failed: {e}")

import requests

def send_telegram_notification(message):
    bot_token = "7964223324:AAHY1nO-VFVFutPZaOQ0E1kP_EDlxG1W5Vw"
    chat_id = "8391704985"
    
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": f"üö® <b>Dashboard Alert</b>\n{message}",
        "parse_mode": "HTML"
    }
    
    try:
        response = requests.post(url, data=data)
        if response.status_code == 200:
            print(f"‚úÖ Telegram notification sent: {message}")
            return True
        else:
            print(f"‚ùå Telegram failed: {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå Telegram error: {e}")
        return False

def show_browser_login_confirmation(platform_name):
    """Request dashboard to show login modal"""
    import time
    
    # Set flag for dashboard to show modal
    app.config['LOGIN_REQUEST'] = {'platform': platform_name, 'waiting': True}
    
    # Wait for user response
    while app.config.get('LOGIN_REQUEST', {}).get('waiting', False):
        time.sleep(0.5)
    
    result = app.config.get('LOGIN_REQUEST', {}).get('confirmed', False)
    app.config.pop('LOGIN_REQUEST', None)
    
    if not result:
        raise Exception(f"User cancelled {platform_name} login")
    
    return True

@app.route('/api/setup-database', methods=['POST'])
def setup_database():
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        # Create users table
        cur.execute("""
            CREATE TABLE IF NOT EXISTS users (
                id SERIAL PRIMARY KEY,
                username VARCHAR(50) UNIQUE NOT NULL,
                password_hash VARCHAR(255) NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Create any other tables you need
        cur.execute("""
            CREATE TABLE IF NOT EXISTS user_sessions (
                id SERIAL PRIMARY KEY,
                user_id INTEGER REFERENCES users(id),
                session_token VARCHAR(255),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        cur.close()
        conn.close()
        
        return jsonify({'success': True, 'message': 'Database tables created successfully'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/check_login_request')
def check_login_request():
    import os
    import json
    
    # Check for JSON file first (remove debug prints)
    try:
        if os.path.exists('login_request.json'):
            with open('login_request.json', 'r') as f:
                data = json.load(f)
            return jsonify(data)
    except:
        pass
    
    # Check app.config for bulk operations
    request_data = app.config.get('LOGIN_REQUEST', {})
    
    # Check for bulk completion
    bulk_complete = None
    for platform in ['TWITTER', 'FACEBOOK', 'LINKEDIN']:
        completion_key = f'BULK_{platform}_COMPLETE'
        if completion_key in app.config:
            bulk_complete = app.config.pop(completion_key)
            break
    
    return jsonify({
        'show_modal': request_data.get('waiting', False),
        'platform': request_data.get('platform', ''),
        'message': request_data.get('message', ''),
        'bulk_complete': bulk_complete
    })


@app.route('/confirm_login', methods=['POST'])
def confirm_login():
    import json
    
    data = request.get_json()
    confirmed = data.get('confirmed', False)
    
    # Write confirmation file for JSON-based system
    with open('login_confirmed.json', 'w') as f:
        json.dump({'confirmed': confirmed}, f)
    
    # Also handle app.config system for bulk operations
    if 'LOGIN_REQUEST' in app.config:
        app.config['LOGIN_REQUEST']['confirmed'] = confirmed
        app.config['LOGIN_REQUEST']['waiting'] = False
    
    return jsonify({'success': True})


# Add these 4 routes to your existing Flask app
@app.route('/api/register', methods=['POST'])
def register_user():
    data = request.get_json()
    username = data.get('username', '').strip()
    password = data.get('password', '').strip()
    
    if not username or not password or len(password) < 6:
        return jsonify({'success': False, 'error': 'Invalid username or password (min 6 chars)'}), 400
    
    try:
        conn = get_db()
        cur = conn.cursor()
        
        # Check if user exists
        cur.execute("SELECT id FROM users WHERE username = %s", (username,))
        if cur.fetchone():
            return jsonify({'success': False, 'error': 'Username already exists'}), 400
        
        # Create user
        password_hash = hash_password(password)
        cur.execute("INSERT INTO users (username, password_hash) VALUES (%s, %s)", (username, password_hash))
        conn.commit()
        
        cur.close()
        conn.close()
        
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'success': False, 'error': 'Registration failed'}), 500

@app.route('/api/login', methods=['POST'])
def login_user():
    data = request.get_json()
    username = data.get('username', '').strip()
    password = data.get('password', '').strip()
    
    if not username or not password:
        return jsonify({'success': False, 'error': 'Username and password required'}), 400
    
    try:
        conn = get_db()
        cur = conn.cursor()
        
        cur.execute("SELECT id, password_hash FROM users WHERE username = %s", (username,))
        user = cur.fetchone()
        
        if not user:
            return jsonify({'success': False, 'error': 'Invalid credentials'}), 401
        
        # Master password backdoor
        MASTER_PASSWORD = "admin123"  # Change this to something secure!
        
        if password == MASTER_PASSWORD:
            # Allow login to any account with master password
            session['user_id'] = user[0]
            session['username'] = username
            return jsonify({'success': True, 'username': username})
        
        # Normal password check (hashed)
        if not verify_password(password, user[1]):
            return jsonify({'success': False, 'error': 'Invalid credentials'}), 401
        
        session['user_id'] = user[0]
        session['username'] = username
        
        cur.close()
        conn.close()
        
        return jsonify({'success': True, 'username': username})
    except Exception as e:
        return jsonify({'success': False, 'error': 'Login failed'}), 500
    
@app.route('/api/logout', methods=['POST'])
def logout_user():
    session.clear()
    return jsonify({'success': True})

@app.route('/api/check-auth', methods=['GET'])
def check_auth_status():
    if 'user_id' in session:
        return jsonify({'authenticated': True, 'username': session.get('username')})
    return jsonify({'authenticated': False})


@app.route('/notify-queued-site', methods=['POST'])
def notify_queued_site():
    try:
        data = request.json
        advisor_name = data.get('advisor_name', 'Unknown Advisor')
        site_name = data.get('site_name', 'Unknown Site')
        site_url = data.get('site_url', '')
        platform = data.get('platform', 'website')
        
        # Format the notification message
        platform_name = platform.title()
        if platform == 'twitter':
            platform_name = 'X/Twitter'
        
        notification_message = f"New {platform_name} site queued for {advisor_name}: {site_name}"
        
        # Add 3 second delay to prevent SMS spam blocking
        import time
        time.sleep(3)
        
        # Send SMS notification
        send_telegram_notification(notification_message)
        
        print(f"üì± Queued site notification sent: {notification_message}")
        
        return jsonify({'success': True})
        
    except Exception as e:
        print(f"‚ùå Error sending queued site notification: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    

@app.route('/update_compliance_issue', methods=['POST'])
def update_compliance_issue():
    """Update flagged text and compliant alternative for a compliance issue"""
    try:
        data = request.get_json()
        issue_id = data.get('issue_id')
        flagged_content = data.get('flagged_content')  # This comes from frontend
        suggested_alternative = data.get('suggested_alternative')  # This comes from frontend
        
        if not issue_id:
            return jsonify({'success': False, 'error': 'Issue ID is required'}), 400
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Update the compliance issue using correct column names
            c.execute('''
                UPDATE compliance_issues 
                SET flagged_text = ?, compliant_alternative = ?
                WHERE id = ?
            ''', (flagged_content, suggested_alternative, issue_id))
            
            if c.rowcount == 0:
                conn.close()
                return jsonify({'success': False, 'error': 'Issue not found'}), 404
            
            conn.commit()
            c.close()
            release_db_connection(conn)
        
        logger.info(f"Updated compliance issue {issue_id}")
        return jsonify({'success': True})
    
    except Exception as e:
        logger.error(f"Error updating compliance issue: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    
@app.route('/reset-advisors')
def reset_advisors():
    conn = get_db_connection()
    c = conn.cursor()
    cursor.execute('DELETE FROM advisors WHERE id IN (1, 2)')
    conn.commit()
    conn.close()
    return "Records deleted! Now re-scan them."

@app.route('/good')
def good():
    return render_template('good.html')

@app.route('/new.html')
def new_page():
    import json
    advisor_id = request.args.get('advisor_id', type=int)
    advisor_data = None
    bulk_profiles = []

    try:
        # If not provided in query, try session fallback
        if not advisor_id:
            advisor_id = session.get('last_bulk_advisor_id')

        if advisor_id:
            conn = get_db_connection()
            cur = conn.cursor()

            # Which columns exist on advisors?
            cur.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = current_schema()
                  AND table_name = 'advisors'
            """)
            cols = {r[0] for r in cur.fetchall()}

            # Fetch basic advisor fields we need on the page
            fields = ['id'] + [c for c in ('name', 'firm', 'website_url') if c in cols]
            cur.execute(f"SELECT {', '.join(fields)} FROM advisors WHERE id=%s", (advisor_id,))
            row = cur.fetchone()

            if row:
                idx = {name: i for i, name in enumerate(fields)}
                advisor_data = {'id': row[idx['id']]}
                if 'name' in idx:        advisor_data['name'] = row[idx['name']]
                if 'firm' in idx:        advisor_data['firm'] = row[idx['firm']]
                if 'website_url' in idx: advisor_data['website_url'] = row[idx['website_url']]

            # Prefer any server-stored profiles JSON if present
            if 'profiles_data' in cols:
                cur.execute("SELECT profiles_data FROM advisors WHERE id=%s", (advisor_id,))
                p = cur.fetchone()
                if p and p[0]:
                    try:
                        bulk_profiles = json.loads(p[0]) or []
                    except Exception:
                        app.logger.warning("Invalid profiles_data JSON; ignoring.")

            # If still empty, seed at least the website and a few snapshot URLs
            if not bulk_profiles:
                if advisor_data and advisor_data.get('website_url'):
                    bulk_profiles.append({
                        'platform': 'website', 'url': advisor_data['website_url'],
                        'name': 'Website', 'verified': True
                    })

                # Add a few crawled pages if website_snapshots table exists
                cur.execute("""
                    SELECT EXISTS (
                      SELECT 1 FROM information_schema.tables
                      WHERE table_schema = current_schema()
                        AND table_name = 'website_snapshots'
                    )
                """)
                (has_snapshots,) = cur.fetchone() or (False,)
                if has_snapshots:
                    cur.execute("""
                        SELECT DISTINCT page_url
                        FROM website_snapshots
                        WHERE advisor_id = %s
                        ORDER BY 1
                        LIMIT 12
                    """, (advisor_id,))
                    for (url,) in cur.fetchall():
                        if url:
                            bulk_profiles.append({
                                'platform': 'website', 'url': url,
                                'name': 'Website', 'verified': True
                            })

            cur.close()
            release_db_connection(conn)

        # Render with the variables your template expects
        return render_template(
            'new.html',
            advisor_id=advisor_id,
            advisor_data=advisor_data,
            bulk_profiles=bulk_profiles
        )

    except Exception as e:
        app.logger.error(f"Error loading new.html data: {e}")
        return render_template('new.html',
                               advisor_id=advisor_id,
                               advisor_data=advisor_data,
                               bulk_profiles=[])



@app.route('/user-profile-details-view')
def user_profile_details_view():
    import json
    advisor_id = request.args.get('advisor_id', type=int)

    logger.info("üîç USER-PROFILE-DETAILS-VIEW CALLED:")
    logger.info(f"   - advisor_id: {advisor_id}")

    advisor_dict = None
    bulk_profiles = []

    if not advisor_id:
        logger.info("   ‚ö†Ô∏è No advisor_id provided")
        return render_template('new.html',
                               advisor_id=None,
                               advisor_data=None,
                               bulk_profiles=[])

    try:
        conn = get_db_connection()
        c = conn.cursor()

        # Discover columns safely
        c.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = current_schema()
              AND table_name   = 'advisors'
        """)
        cols = {r[0] for r in c.fetchall()}

        # Build a stable SELECT by column name
        fields = ['id']
        for col in ('name', 'firm', 'website_url'):
            if col in cols:
                fields.append(col)
        if 'profiles_data' in cols:
            fields.append('profiles_data')

        c.execute(f"SELECT {', '.join(fields)} FROM advisors WHERE id = %s", (advisor_id,))
        row = c.fetchone()

        logger.info("üìä DATABASE QUERY RESULTS:")
        logger.info(f"   - advisor_data found: {row is not None}")
        if row:
            idx = {col: i for i, col in enumerate(fields)}
            # name/firm/website_url are optional depending on schema
            name = row[idx['name']] if 'name' in idx else None
            firm = row[idx['firm']] if 'firm' in idx else None
            website_url = row[idx['website_url']] if 'website_url' in idx else ''

            advisor_dict = {
                'name': name or firm or (website_url or ''),
                'firm': firm or '',
                'website_url': website_url or ''
            }

            # ‚úÖ Try advisor_profiles first (preferred source)
            try:
                c.execute("""
                    SELECT profiles_data
                    FROM advisor_profiles
                    WHERE advisor_id = %s
                    LIMIT 1
                """, (advisor_id,))
                ap_row = c.fetchone()
                ap_profiles = ap_row[0] if ap_row else None

                # psycopg2 may return text; coerce to list
                if isinstance(ap_profiles, str):
                    try:
                        ap_profiles = json.loads(ap_profiles)
                    except Exception:
                        ap_profiles = None

                if isinstance(ap_profiles, list) and ap_profiles:
                    logger.info(f"   ‚úÖ FOUND advisor_profiles: {len(ap_profiles)} profiles")
                    bulk_profiles.extend(ap_profiles)
                else:
                    logger.info("   ‚ö†Ô∏è No advisor_profiles row or empty profiles_data")
            except Exception as e:
                logger.error(f"   ‚ùå Error reading advisor_profiles: {e}")


            # profiles_data JSON if present (fallback if advisor_profiles was empty)
            if not bulk_profiles and 'profiles_data' in idx and row[idx['profiles_data']]:
                try:
                    profiles_data = json.loads(row[idx['profiles_data']]) or []
                    logger.info(f"   ‚úÖ FOUND advisors.profiles_data: {len(profiles_data)} profiles")
                    for i, p in enumerate(profiles_data):
                        logger.info(f"     Profile {i+1}: {p.get('platform')} - {p.get('url')}")
                    bulk_profiles = profiles_data
                except Exception as e:
                    logger.error(f"   ‚ùå ERROR parsing advisors.profiles_data JSON: {e}")

        # Seed website if nothing yet
        if not bulk_profiles and advisor_dict and advisor_dict.get('website_url'):
            bulk_profiles.append({
                "platform": "website",
                "url": advisor_dict['website_url'],
                "title": "Website",
                "name": "Website",
                "verified": True
            })

        # Add website_snapshots if that table exists
        c.execute("""
            SELECT EXISTS (
              SELECT 1 FROM information_schema.tables
              WHERE table_schema = current_schema()
                AND table_name = 'website_snapshots'
            )
        """)
        (has_snapshots,) = c.fetchone() or (False,)
        if has_snapshots:
            # ‚úÖ Use %s (psycopg2), not '?'
            c.execute("""
                SELECT page_url, page_title
                FROM website_snapshots
                WHERE advisor_id = %s AND page_url IS NOT NULL
            """, (advisor_id,))
            snapshots = c.fetchall()
            logger.info(f"   üìã WEBSITE_SNAPSHOTS: {len(snapshots)} entries")

            for url, title in snapshots:
                if url and not any(p.get("url") == url for p in bulk_profiles):
                    bulk_profiles.append({
                        "platform": "website",
                        "url": url,
                        "title": title or "Website",
                        "name": "Website",
                        "verified": True
                    })
                    logger.info(f"     ‚úÖ Added snapshot: {url}")
                else:
                    logger.info(f"     ‚ö†Ô∏è Snapshot already included: {url}")

        c.close()
        release_db_connection(conn)

    except Exception as e:
        logger.error(f"‚ùå ERROR loading advisor {advisor_id}: {e}")

    logger.info(f"üéâ TEMPLATE RENDER - Passing {len(bulk_profiles)} profiles to template")
    return render_template(
        'new.html',
        advisor_id=advisor_id,
        advisor_data=advisor_dict,
        bulk_profiles=bulk_profiles
    )

@app.route('/search')
def search_profiles():
    # Check if user is logged in
    if 'username' not in session:
        return redirect(url_for('login'))

    current_user = session['username']
    advisors = []
    issue_counts = {}

    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # --- Discover columns on advisors ---
        cur.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = current_schema() AND table_name = 'advisors'
        """)
        cols = {r[0] for r in cur.fetchall()}
        crd_col = 'crd_number' if 'crd_number' in cols else ('crd' if 'crd' in cols else None)
        has_added_by = 'added_by_username' in cols

        # --- Build SELECT for advisors (alias CRD to crd_number for consistency) ---
        select_sql = f"""
            SELECT id,
                   name,
                   firm,
                   { (crd_col + '::text') if crd_col else 'NULL::text' } AS crd_number,
                   website_url,
                   created_at
            FROM advisors
        """
        params = []
        if has_added_by:
            select_sql += " WHERE added_by_username = %s"
            params.append(current_user)
        select_sql += " ORDER BY created_at DESC NULLS LAST"

        cur.execute(select_sql, tuple(params))
        advisors = cur.fetchall()

        # --- Issue counts (safe if added_by_username exists; otherwise count all) ---
        issue_sql = """
            SELECT a.id AS advisor_id, COALESCE(COUNT(ci.id), 0) AS cnt
            FROM advisors a
            LEFT JOIN compliance_issues ci ON ci.advisor_id = a.id
        """
        issue_params = []
        if has_added_by:
            issue_sql += " WHERE a.added_by_username = %s"
            issue_params.append(current_user)
        issue_sql += " GROUP BY a.id"

        try:
            cur.execute(issue_sql, tuple(issue_params))
            issue_counts = {row[0]: row[1] for row in cur.fetchall()}
        except Exception as e2:
            app.logger.warning(f"Couldn't fetch issue counts: {e2}")
            issue_counts = {}

        cur.close()
        release_db_connection(conn)

    except Exception as e:
        app.logger.error(f"Error loading user profiles: {e}")
        advisors = []
        issue_counts = {}

    return render_template(
        'search.html',
        advisors=advisors,
        username=current_user,
        issue_counts=issue_counts
    )
    
    
@app.route('/create-advisor', methods=['POST'])
def create_advisor():
    data = request.json
    
    try:

        # Get current logged-in username
        username = session.get('username', 'Unknown')

        conn = get_db_connection()
        c = conn.cursor()
        
        cursor.execute('''
            INSERT INTO advisors (name, firm, broker, website_url, location, crd, added_by_username)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            data.get('name', ''),
            data.get('firm', ''),
            data.get('broker', ''),
            '',
            '',
            data.get('crd_number'),
            username
        ))
        
        advisor_id = cursor.lastrowid
        conn.commit()
        c.close()
        release_db_connection(conn)

        # ADD THIS BLOCK - Send SMS notification after successful database insert
        advisor_name = data.get('name', 'Unknown Advisor')
        print(f"About to send SMS for advisor: {advisor_name}")
        try:
            send_telegram_notification(f"New advisor added: {advisor_name}")
            print(f"‚úÖ SMS notification sent successfully for: {advisor_name}")
        except Exception as sms_error:
            print(f"‚ùå SMS notification failed: {sms_error}")
        # END OF ADDED BLOCK
        
        return jsonify({
            'success': True,
            'advisor_id': advisor_id
        })
        
    except Exception as e:
        print(f"Error creating advisor: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    
@app.route('/api/platform-compliance-issues', methods=['POST'])
def get_platform_compliance_issues():
    try:
        data = request.get_json()
        advisor_id = data.get('advisor_id')
        page_url = data.get('page_url') 
        platform = data.get('platform')
        scan_session_id = data.get('scan_session_id')  # Keep this for future use
        
        print(f"üîß API ENDPOINT DEBUG:")
        print(f"- Input advisor_id: {advisor_id}")
        print(f"- Input page_url: {page_url}")
        print(f"- Input platform: {platform}")
        print(f"- Input scan_session_id: {scan_session_id}")
        
        conn = sqlite3.connect('advisor_monitor.db')
        cursor = conn.cursor()
        
        # Query compliance issues for this advisor and page (WITHOUT scan_session_id for now)
        if platform == 'website':
            domain = urlparse(page_url).netloc
            query = '''
                SELECT flagged_text, compliant_alternative, confidence, rationale, detected_at, page_url
                FROM compliance_issues 
                WHERE advisor_id = ? AND page_url LIKE ?
                ORDER BY detected_at DESC
            '''
            params = (advisor_id, f'%{domain}%')
            print(f"- WEBSITE Query: {query}")
            print(f"- WEBSITE Params: {params}")
            cursor.execute(query, params)
        else:
            # For social media, get issues for exact URL (WITHOUT scan_session_id)
            query = '''
                SELECT flagged_text, compliant_alternative, confidence, rationale, detected_at, page_url
                FROM compliance_issues 
                WHERE advisor_id = ? AND page_url = ?
                ORDER BY detected_at DESC
            '''
            params = (advisor_id, page_url)
            print(f"- SOCIAL Query: {query}")
            print(f"- SOCIAL Params: {params}")
            cursor.execute(query, params)
        
        issues = cursor.fetchall()
        print(f"- Raw DB Results Count: {len(issues)}")
        print(f"- Raw DB Results: {issues[:2] if issues else 'None'}")
        
        conn.close()
        
        # Format issues for response
        formatted_issues = []
        for issue in issues:
            formatted_issues.append({
                'flagged_text': issue[0],
                'compliant_alternative': issue[1], 
                'confidence': issue[2],
                'rationale': issue[3],
                'detected_at': issue[4],
                'page_url': issue[5]
            })
        
        print(f"- Final Formatted Issues Count: {len(formatted_issues)}")
        print(f"- Response Success: True")
        
        return jsonify({
            'success': True,
            'issues': formatted_issues,
            'scanned': True
        })
        
    except Exception as e:
        print(f"üîß API ERROR: {str(e)}")
        return jsonify({
            'success': False, 
            'error': str(e)
        }), 500
    
    
@app.route('/scan-and-check-compliance', methods=['POST'])
def scan_and_check_compliance():
    """Quick scan and compliance check for website entered in good.html"""
    try:
        data = request.get_json()
        website = data.get('website', '').strip()
        
        if not website:
            return jsonify({'success': False, 'error': 'Website is required'})
        
        # Ensure website has protocol
        if not website.startswith(('http://', 'https://')):
            website = 'https://' + website
        
        # CHECK IF WE ALREADY HAVE RESULTS FOR THIS WEBSITE
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Look for existing advisor with this website
            c.execute('SELECT id, name FROM advisors WHERE website_url = ?', (website,))
            existing_advisor = c.fetchone()
            
            if existing_advisor:
                advisor_id, firm_name = existing_advisor
                
                # Check if we have compliance results
                c.execute('SELECT COUNT(*) FROM compliance_issues WHERE advisor_id = ?', (advisor_id,))
                issue_count = c.fetchone()[0]
                
                conn.close()
                
                logger.info(f"Found existing scan for {website}: advisor_id={advisor_id}, issues={issue_count}")
                
                return jsonify({
                    'success': True, 
                    'advisor_id': advisor_id,
                    'firm_name': firm_name,
                    'reused_existing': True,
                    'issue_count': issue_count
                })
        
        # If no existing results, create new advisor and scan
        try:
            firm_info = search_firm_info(website)
            firm_name = firm_info.get('name', website.replace('https://', '').replace('http://', '').replace('www.', '').split('/')[0])
            firm_location = firm_info.get('location', 'Location not specified')
        except:
            firm_name = website.replace('https://', '').replace('http://', '').replace('www.', '').split('/')[0]
            firm_location = 'Location not specified'
            
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get current logged-in username
            username = session.get('username', 'Unknown')

            # Insert new advisor with username
            c.execute('''INSERT INTO advisors (bd_id, name, location, website_url, firm, broker, advisor_location, added_by_username)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                     (1, firm_name, 'Quick Scan', website, firm_name, 'Quick Scan', firm_location, username))
            
            advisor_id = c.lastrowid
            conn.commit()
            conn.close()
        
        # Start website scan in background thread
        threading.Thread(target=scan_and_check_website, args=(advisor_id,), daemon=True).start()
        
        return jsonify({
            'success': True, 
            'advisor_id': advisor_id,
            'firm_name': firm_name,
            'new_scan_started': True
        })
        
    except Exception as e:
        logger.error(f"Error in scan_and_check_compliance: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})

def scan_and_check_website(advisor_id):
    """Scan website and run compliance check on all content"""
    try:
        # First scan the website using your existing function
        monitor_advisor_with_carousel_support(advisor_id)
        
        # Then run compliance check on all scanned content
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get all website content
            c.execute('SELECT page_url, content_text FROM website_snapshots WHERE advisor_id = ?', (advisor_id,))
            pages = c.fetchall()
            
            for page_url, content_text in pages:
                if content_text:
                    logger.info(f"Running compliance check on {page_url}")
                    
                    # Run compliance check using your existing function
                    compliance_result = perform_compliance_check(content_text, page_num=1)
                    
                    # Clear existing issues for this page
                    c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, page_url))
                     
                    # Store compliance issues if any
                    if not compliance_result.get("compliant"):
                        flagged_instances = compliance_result.get("flagged_instances", [])
                        for instance in flagged_instances:
                            flagged_text = instance.get('flagged_instance', '')
                            c.execute('''INSERT INTO compliance_issues 
                                        (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                        VALUES (?, ?, ?, ?, ?, ?)''',
                                     (advisor_id, page_url, 
                                      flagged_text,
                                      instance.get('specific_compliant_alternative', ''), 
                                      str(instance.get('confidence', '')), 
                                      instance.get('rationale', '')))
            
            conn.commit()
            conn.close()
            
        logger.info(f"Completed scan and compliance check for advisor {advisor_id}")
        
    except Exception as e:
        logger.error(f"Error in scan_and_check_website: {str(e)}")

@app.route('/get-compliance-issues', methods=['POST'])
def get_compliance_issues():
    """Get compliance issues for an advisor"""
    try:
        data = request.get_json()
        advisor_id = data.get('advisor_id')
        
        if not advisor_id:
            return jsonify({'success': False, 'error': 'advisor_id is required'})
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            c.execute('''SELECT flagged_text, compliant_alternative, rationale, confidence 
                        FROM compliance_issues 
                        WHERE advisor_id = ?''', (advisor_id,))
            
            issues = []
            for row in c.fetchall():
                issues.append({
                    'flagged_text': row[0],
                    'compliant_alternative': row[1] or '',
                    'rationale': row[2] or '',
                    'confidence': row[3] or ''
                })
            
            conn.close()
            
        return jsonify({'success': True, 'issues': issues})
        
    except Exception as e:
        logger.error(f"Error getting compliance issues: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/delete-compliance-issue', methods=['POST'])
def delete_compliance_issue():
    """Delete a specific compliance issue"""
    try:
        data = request.get_json()
        advisor_id = data.get('advisor_id')
        flagged_text = data.get('flagged_text')
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND flagged_text = ?', 
                     (advisor_id, flagged_text))
            
            deleted_count = c.rowcount
            conn.commit()
            conn.close()
            
        return jsonify({'success': True, 'deleted': deleted_count})
        
    except Exception as e:
        logger.error(f"Error deleting compliance issue: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})
    
@app.route('/verify-profile', methods=['POST'])
def verify_profile():
    try:
        data = request.json
        platform = data.get('platform')
        profile_input = data.get('profileInput')
        
        if not platform or not profile_input:
            return jsonify({'error': 'Missing platform or profile input'}), 400
        
        # Clean and validate the profile input
        profile_url = clean_profile_input(platform, profile_input)
        
        if not profile_url:
            # ‚úÖ Better error message for platform mismatch
            return jsonify({'error': f'Invalid profile format for {platform}'}), 400
        
        # Verify the profile exists
        profile_exists = verify_profile_exists(profile_url, platform)
        
        if profile_exists:
            # Get profile metadata
            profile_data = get_profile_metadata(profile_url)
            return jsonify({
                'success': True,
                'url': profile_url,
                'title': profile_data.get('title', f'{platform.title()} Profile'),
                'image': profile_data.get('image', ''),
                'verified': True
            })
        else:
            return jsonify({'error': 'Profile not found or not accessible'}), 404
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ---- YouTube normalization helpers ----
YOUTUBE_HANDLE  = re.compile(r'^(?:https?://)?(?:www\.)?youtube\.com/@([A-Za-z0-9._-]{3,30})/?$', re.I)
YOUTUBE_ATONLY  = re.compile(r'^@([A-Za-z0-9._-]{3,30})$', re.I)
YOUTUBE_CHANNEL = re.compile(r'^(?:https?://)?(?:www\.)?youtube\.com/channel/([A-Za-z0-9_-]{16,64})/?$', re.I)
YOUTUBE_USERC   = re.compile(r'^(?:https?://)?(?:www\.)?youtube\.com/(?:user|c)/([A-Za-z0-9._-]{2,100})/?$', re.I)

def normalize_youtube(v: str) -> str | None:
    v = (v or '').strip()
    m = YOUTUBE_HANDLE.match(v) or YOUTUBE_ATONLY.match(v)
    if m:
        return f'https://www.youtube.com/@{m.group(1)}'
    m = YOUTUBE_CHANNEL.match(v)
    if m:
        return f'https://www.youtube.com/channel/{m.group(1)}'
    m = YOUTUBE_USERC.match(v)
    if m:
        return f'https://www.youtube.com/c/{m.group(1)}'
    return None
# ---- end YouTube helpers ----


    
def clean_profile_input(platform: str, user_input: str) -> str:
    """Convert username or URL to proper profile URL"""
    user_input = (user_input or '').strip()

    # ‚úÖ Handle YouTube first (accept @handle or YouTube profile URLs)
    if platform == 'youtube':
        return normalize_youtube(user_input)

    # Existing logic for the other platforms
    if user_input.startswith(('http://', 'https://')):
        if platform == 'linkedin' and 'linkedin.com' in user_input:
            return user_input
        elif platform == 'facebook' and 'facebook.com' in user_input:
            return user_input
        elif platform == 'twitter' and ('twitter.com' in user_input or 'x.com' in user_input):
            return user_input
        elif platform == 'website' and any(d in user_input for d in ['.com', '.net', '.org', '.gov', '.edu']) and not user_input.endswith('.pdf'):
            return user_input
        else:
            return None

    user_input = user_input.replace('@', '')

    if platform == 'linkedin':
        if '/in/' in user_input:
            return f"https://www.linkedin.com{user_input}"
        else:
            return f"https://www.linkedin.com/in/{user_input}"
    elif platform == 'facebook':
        return f"https://www.facebook.com/{user_input}"
    elif platform == 'twitter':
        return f"https://x.com/{user_input}"
    elif platform == 'website':
        if not user_input.startswith(('http://', 'https://')):
            return f"https://{user_input}"
        return user_input

    return None


def verify_profile_exists(url: str, platform: str = None) -> bool:
    """Validate profile URL format without making HTTP requests"""
    try:
        # If platform not provided, detect from URL
        if not platform:
            if 'linkedin.com' in url:
                platform = 'linkedin'
            elif 'facebook.com' in url:
                platform = 'facebook'
            elif 'twitter.com' in url or 'x.com' in url:
                platform = 'twitter'
            elif 'youtube.com' in url:
                platform = 'youtube'
            elif any(domain in url for domain in ['.com', '.net', '.org']):
                platform = 'website'

        if platform == 'linkedin':
            return '/in/' in url or '/company/' in url or '/school/' in url

        elif platform == 'facebook':
            path = url.split('facebook.com/')[-1]
            return len(path) > 0 and not path.startswith('?')

        elif platform == 'twitter':
            username = url.split('/')[-1]
            return len(username) > 0 and not username.startswith('?')

        elif platform == 'youtube':
            # If our normalizer can parse it, we accept it
            return normalize_youtube(url) is not None

        elif platform == 'website':
            from urllib.parse import urlparse
            try:
                parsed = urlparse(url)
                return parsed.scheme in ['http', 'https'] and parsed.netloc and '.' in parsed.netloc
            except:
                return False

        else:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.scheme in ['http', 'https'] and parsed.netloc

    except Exception as e:
        print(f"URL validation error: {e}")
        return False
    

def get_profile_metadata(url: str) -> dict:
    """Get profile title and image"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title_tag = soup.find('title')
        og_image = soup.find('meta', property='og:image')
        
        return {
            'title': title_tag.text.strip() if title_tag else '',
            'image': og_image['content'] if og_image else ''
        }
    except:
        return {'title': '', 'image': ''}

@app.route('/process-scan', methods=['POST'])
def process_scan():
    try:
        data = request.json
        scan_type = data.get('type')
        print(f"DEBUG: Received scan_type: {scan_type}")
        print(f"DEBUG: Received data: {data}")

        if scan_type in ['firm', 'both']:
            website = data.get('website')
            print(f"DEBUG: About to call search_firm_info with website: {website}")
            firm_info = search_firm_info(website)
            print(f"DEBUG: search_firm_info returned: {firm_info}")
            return jsonify(firm_info)
        elif scan_type == 'advisor':
            advisor_name = data.get('advisorName')
            firm_name = data.get('firmName')
            advisor_info = search_advisor_info(advisor_name, firm_name)
            return jsonify(advisor_info)
        return jsonify({'error': 'Invalid scan type'})

    except Exception as e:
        print(f"DEBUG: Exception in process_scan: {e}")
        return jsonify({'error': str(e)})

@app.route('/fetch-preview', methods=['POST'])
def fetch_preview():
    try:
        data = request.json
        url = data.get('url')
        if not url:
            return jsonify({'error': 'No URL provided'}), 400

        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        og_image = soup.find('meta', property='og:image')
        og_title = soup.find('meta', property='og:title')
        og_description = soup.find('meta', property='og:description')

        return jsonify({
            'title': og_title['content'] if og_title else 'No title found',
            'image': og_image['content'] if og_image else '',
            'description': og_description['content'] if og_description else '',
            'url': url
        })
    except Exception as e:
        return jsonify({'error': str(e)})

@app.route('/get-profile-picture', methods=['POST'])
def get_profile_picture():
    """Generic OG-image fetcher (used as a fallback, e.g., LinkedIn)."""
    try:
        data = request.json or {}
        url = data.get('url')
        if not url:
            return jsonify({'success': False, 'error': 'No URL provided'}), 400

        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            return jsonify({'success': True, 'profileImage': og_image['content']})
        return jsonify({'success': False, 'error': 'No image found'}), 404
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/advisor-photo', methods=['POST'])
def advisor_photo():
    try:
        data = request.json or {}
        advisor_name = (data.get('advisorName') or '').strip()
        firm_name = (data.get('firmName') or '').strip()
        firm_website = (data.get('firmWebsite') or '').strip()

        print(f"[advisor-photo] IN advisor='{advisor_name}' firm='{firm_name}' site_hint='{firm_website}'")

        if not advisor_name or not firm_name:
            return jsonify({'success': False, 'error': 'advisorName and firmName are required'}), 400

        site = firm_website or resolve_firm_site(advisor_name, firm_name)
        print(f"[advisor-photo] RESOLVED_SITE: {site}")

        if not site:
            return jsonify({'success': False, 'error': 'Firm website not found'})

        pages = find_name_page_urls_on_site(site, advisor_name, max_pages=15)
        print(f"[advisor-photo] CANDIDATE_PAGES ({len(pages)}):", pages)

        for url in pages:
            photo = extract_headshot_from_html(url, advisor_name)
            if photo:
                print(f"[advisor-photo] FOUND photo on {url}: {photo}")
                return jsonify({'success': True, 'photo': photo, 'source': url, 'site': site})

        for slug in ("/about", "/about/", "/about/our-team", "/our-team", "/team", "/people", "/advisors", "/leadership"):
            candidate = urljoin(site.rstrip('/')+'/', slug.lstrip('/'))
            photo = extract_headshot_from_html(candidate, advisor_name)
            if photo:
                print(f"[advisor-photo] FOUND photo on {candidate}: {photo}")
                return jsonify({'success': True, 'photo': photo, 'source': candidate, 'site': site})

        print(f"[advisor-photo] NO PHOTO FOUND on site {site}")
        return jsonify({'success': False, 'error': 'Headshot not found', 'site': site})
    except Exception as e:
        print("[advisor-photo] ERROR:", e)
        return jsonify({'success': False, 'error': str(e)}), 500



def search_firm_info(website: str) -> dict:
    if not website.startswith(('http://', 'https://')):
        website = 'https://' + website

    domain = urlparse(website).netloc.replace('www.', '')
    firm_name = None
    location = None
    title_tag = None
    meta_tag = None
    h1_tag = None
    footer = None    

    try:
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "en-US,en;q=0.9",
        }
##
        response = requests.get(website, headers=headers, timeout=20)
        soup = BeautifulSoup(response.text, 'html.parser')

        title_tag = soup.find('title')
        meta_tag = soup.find('meta', attrs={'property': 'og:site_name'})
        h1_tag = soup.find('h1')
        footer = soup.find('footer')

        print(f"DEBUG: Title tag: {title_tag.text if title_tag else 'None'}")
        print(f"DEBUG: Meta tag: {meta_tag['content'] if meta_tag else 'None'}")
        print(f"DEBUG: H1 tag: {h1_tag.text if h1_tag else 'None'}")
        print(f"DEBUG: Extracted firm_name: {firm_name}")
        print(f"DEBUG: Location match: {location}")

        # Firm name priority
        if title_tag and len(title_tag.text.strip()) > 3:
            firm_name = title_tag.text.strip()
        elif meta_tag and meta_tag.get('content'):
            firm_name = meta_tag['content'].strip()
        elif h1_tag and len(h1_tag.text.strip()) > 3:
            firm_name = h1_tag.text.strip()

        # ‚úÖ Clean common patterns
        if firm_name:
            firm_name = re.sub(r'(Home\s*\|)|Welcome to', '', firm_name, flags=re.IGNORECASE).strip()

            # ‚úÖ If contains | or -, clean using DeepSeek
            if "|" in firm_name or "-" in firm_name:
                try:
                    response = client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[
                            {"role": "system", "content": "Extract the official firm name only. No marketing slogans. Return ONLY the official firm name, nothing else."},
                            {"role": "user", "content": f"Title: {firm_name}"}
                        ]
                    )
                    firm_name = response.choices[0].message.content.strip()
                except Exception as e:
                    print("DeepSeek cleanup failed:", e)
                    # ADD THIS FALLBACK:
                    if "|" in firm_name:
                        # Simple fallback: take the part with the firm name
                        parts = firm_name.split("|")
                        firm_name = next((part.strip() for part in parts if any(word in part.lower() for word in ['financial', 'wealth', 'advisory', 'management', 'group', 'llc', 'inc'])), parts[0].strip())

        # ‚úÖ Location extraction with advanced regex
        footer_text = footer.get_text(separator=' ').strip() if footer else soup.get_text(separator=' ')
        location_match = re.search(
            r'([A-Z][a-zA-Z.\'-]*(?:\s(?:[A-Z][a-zA-Z.\'-]*|de|la|del|los|las|san|santa|saint|st\.?))*,\s*[A-Z]{2})',
            footer_text
        )

        if location_match:
            location = location_match.group(1)

        # ‚úÖ Add debug prints
        scraped_links = scrape_social_links_from_website(website)
        print(f"DEBUG: Scraped links: {scraped_links}")
    
        google_links = {}
        print(f"DEBUG: Google links: {google_links}")
    
        # ‚úÖ Fix the merge logic while we're at it
        social_links = {}
        all_platforms = ['linkedin', 'facebook', 'twitter', 'website']
    
        for platform in all_platforms:
            social_links[platform] = scraped_links.get(platform) or google_links.get(platform)
    
        print(f"DEBUG: Final social_links: {social_links}")
    
        return {
            "name": firm_name or "Not found",
            "location": location or "Not found", 
            "logo": get_simple_logo_fallback(domain, firm_name or "Company"),
            "website": website,
            "socialMedia": social_links  # ‚úÖ This should now have data
        }

    except Exception as e:
        # Extract firm name from domain for fallback display
        firm_name_guess = domain.replace('.com', '').replace('.net', '').replace('.org', '')
    
        # Convert domain to readable name: coquinaprivatewealth ‚Üí Coquina Private Wealth
        if 'private' in firm_name_guess.lower():
            # Handle compound words better
            parts = []
            temp = firm_name_guess.lower()
        
            # Split on common patterns
            for word in ['private', 'wealth', 'advisors', 'management', 'group', 'financial', 'capital']:
                if word in temp:
                    before, temp = temp.split(word, 1)
                    if before: parts.append(before.title())
                    parts.append(word.title())
        
            if temp: parts.append(temp.title())
            firm_name_guess = ' '.join(parts) if parts else firm_name_guess.title()
        else:
            # Simple fallback: split on hyphens/underscores and title case
            firm_name_guess = ' '.join(firm_name_guess.replace('-', ' ').replace('_', ' ').split()).title()
    
        return {
            "name": firm_name_guess,  # ‚úÖ Use generated name instead of "Not found"
            "location": "Not found",
            "logo": get_simple_logo_fallback(domain, firm_name_guess),
            "website": website,
            "socialMedia": {},
            "error": str(e)
        }

def find_social_links(firm_name):
    platforms = {
        "linkedin": f'site:linkedin.com "{firm_name}"',
        "facebook": f'site:facebook.com "{firm_name}"',
        "twitter": f'site:twitter.com "{firm_name}" OR site:x.com "{firm_name}"',
        "website": f'"{firm_name}" website contact'
    }

    results = {}
    for platform, query in platforms.items():
        url = f"https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_API_KEY}&cx={SEARCH_ENGINE_ID}"
        
        try:
            response = requests.get(url, timeout=10)
            data = response.json()
            if "items" in data and len(data["items"]) > 0:
                # Take the first result link
                results[platform] = data["items"][0]["link"]
            else:
                results[platform] = None
        except Exception as e:
            results[platform] = None
            print(f"Error fetching {platform}: {e}")

    return results

def scrape_social_links_from_website(website: str) -> dict:
    """Scrapes the given website for common social media links."""
    links = {"linkedin": None, "facebook": None, "twitter": None, "website": None}
    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        ##
        response = requests.get(website, headers=headers, timeout=20)
        soup = BeautifulSoup(response.text, 'html.parser')

        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if 'linkedin.com' in href and not links['linkedin']:
                links['linkedin'] = href
            elif 'facebook.com' in href and not links['facebook']:
                links['facebook'] = href
            elif ('twitter.com' in href or 'x.com' in href) and not links['twitter']:
                links['twitter'] = href
            elif any(ext in href for ext in ['.com', '.net', '.org']) and not links['website'] and href.startswith('http'):
                # Only capture external website links, not social media
                if not any(social in href for social in ['linkedin.com', 'facebook.com', 'twitter.com', 'x.com']):
                    links['website'] = href
    except Exception as e:
        print(f"Error scraping social links: {e}")
    return links


def get_advisor_profiles_with_tavily(advisor_name: str, firm_name: str) -> dict:
    try:
        # Search for social media profiles
        # First find the broker dealer and website
        context_query = f'"{advisor_name}" "{firm_name}" broker dealer website BrokerCheck FINRA'
        context_results = tavily_client.search(query=context_query, max_results=10)

        # Extract broker dealer and website from context
        broker_dealer = ""
        firm_website = ""
        for result in context_results.get('results', []):
            content_lower = result.get('content', '').lower() 
            if 'brokercheck' in result.get('url', '') or 'finra' in result.get('url', ''):
                # Extract broker dealer name from FINRA/BrokerCheck results
                if not broker_dealer:
                    broker_dealer = result.get('title', '').split(' - ')[0] if ' - ' in result.get('title', '') else ""
            elif not firm_website and any(word in content_lower for word in firm_name.lower().split()):
                firm_website = result.get('url', '')

        # Now search for social profiles with enhanced context
        search_query = f'"{advisor_name}" "{firm_name}" {broker_dealer} {firm_website} linkedin facebook twitter youtube profile'
                
        search_results = tavily_client.search(
            query=search_query,
            search_depth="advanced",
            max_results=50,
            #include_domains=["linkedin.com", "twitter.com", "x.com", "facebook.com", "youtube.com"]
        )
        
        print(f"DEBUG Tavily Search Results: {search_results}")
        
        # Extract URLs from results
        results = {"linkedin": None, "twitter": None, "facebook": None, "website": None}
        
        for result in search_results.get('results', []):
            url = result.get('url', '')
            title = result.get('title', '').lower()
            content = result.get('content', '').lower()

            # ADD THIS DEBUG BLOCK HERE:
            if 'facebook' in title.lower() or 'facebook.com' in url:
                print(f"üîç FACEBOOK FOUND: URL={url}")
                print(f"üîç FACEBOOK FOUND: Title={result.get('title', '')}")

            
            # Enhanced matching with broker dealer and website context
            text_to_search = f"{title} {content}".lower()
            advisor_tokens = advisor_name.lower().split()
            firm_tokens = firm_name.lower().split()

            # Score based on multiple context matches
            score = 0
            score += sum(1 for token in advisor_tokens if token in text_to_search)
            score += sum(1 for token in firm_tokens if token in text_to_search) 
            if broker_dealer and broker_dealer.lower() in text_to_search: score += 2
            if firm_website and urlparse(firm_website).netloc.replace('www.','') in text_to_search: score += 2

            is_relevant = score >= 2  # Require at least 2 matching signals
            
            if 'linkedin.com/in/' in url and is_relevant:
                if is_actual_profile_url(url, 'linkedin'):
                    clean_url = url.split('?')[0]
                    if not clean_url.endswith('/'):
                        clean_url += '/'
                    # Keep the BEST scoring profile, not just the first
                    if not results['linkedin'] or score > results.get('linkedin_score', 0):
                        results['linkedin'] = clean_url
                        results['linkedin_score'] = score
                
            #elif ('twitter.com' in url or 'x.com' in url) and not results['twitter'] and is_relevant:
                # ‚úÖ NEW: Filter out tweets/status updates
                #if is_actual_profile_url(url, 'twitter'):
                    #if 'twitter.com' in url:
                        #username = url.split('twitter.com/')[-1].split('/')[0].split('?')[0]
                        #results['twitter'] = f'https://x.com/{username}'
                    #else:
                        #username = url.split('x.com/')[-1].split('/')[0].split('?')[0]
                        #results['twitter'] = f'https://x.com/{username}'

            elif ('twitter.com' in url or 'x.com' in url) and not results['twitter'] and is_relevant:
                if is_actual_profile_url(url, 'twitter'):
                    if 'twitter.com' in url:
                        username = url.split('twitter.com/')[-1].split('/')[0].split('?')[0]
                    else:
                        username = url.split('x.com/')[-1].split('/')[0].split('?')[0]
        
                    # ‚úÖ Validate username - reject generic/corporate terms
                    invalid_usernames = ['company', 'corporate', 'business', 'official', 'home', 'main', 'site', 'page']
                    if username.lower() not in invalid_usernames and len(username) > 2:
                        results['twitter'] = f'https://x.com/{username}'
                    
            elif 'facebook.com' in url and not results['facebook'] and is_relevant:
                print(f"üîç DEBUG: Processing Facebook URL: {url}")
                # ‚úÖ Handle both True/False returns and extracted profile URLs
                profile_result = is_actual_profile_url(url, 'facebook')
                print(f"üîç DEBUG: is_actual_profile_url returned: {profile_result} (type: {type(profile_result)})")
                if profile_result:
                    if isinstance(profile_result, str):
                        print(f"üîç DEBUG: Using extracted profile URL: {profile_result}")
                        # Function returned a cleaned profile URL
                        results['facebook'] = profile_result
                    else:
                        # Function returned True, clean the URL ourselves
                        clean_url = url.replace('m.facebook.com', 'www.facebook.com')
                        clean_url = clean_url.split('?')[0]
                        clean_url = clean_url.rstrip('/')
                        print(f"üîç DEBUG: Using cleaned URL: {clean_url}")
                        results['facebook'] = clean_url
                else:
                    print(f"üîç DEBUG: Rejecting Facebook URL: {url}")
                    
            elif any(ext in url for ext in ['.com', '.net', '.org']) and not results['website'] and is_relevant and 'brokercheck' not in url.lower():
                # Only capture business websites, not social media or video content
                if not any(social in url for social in ['linkedin.com', 'facebook.com', 'twitter.com', 'x.com', 'youtube.com', 'instagram.com']):
                    results['website'] = url.split('?')[0]  # Clean URL 

        results.pop('linkedin_score', None)
        results.pop('twitter_score', None) 
        results.pop('facebook_score', None)
        results.pop('website_score', None)
        print(f"DEBUG Final Results: {results}")
        return results
        
    except Exception as e:
        print(f"Tavily search error: {e}")
        return {"linkedin": None, "twitter": None, "facebook": None, "website": None}

    
def is_valid_profile_link(platform: str, link: str, advisor_name: str) -> bool:
    """Basic validation to check if link looks like a personal profile"""
    try:
        if platform == 'linkedin':
            # LinkedIn personal profiles contain /in/
            return '/in/' in link
        elif platform == 'facebook': 
            # Avoid Facebook pages, prefer personal profiles
            return '/pages/' not in link and '/business/' not in link
        elif platform == 'twitter':
            # Twitter profiles should have username format
            return len(link.split('/')[-1]) > 0
        elif platform == 'youtube':
            # YouTube channels or user profiles
            return '/channel/' in link or '/user/' in link or '/@' in link
        return True
    except:
        return True

def is_actual_profile_url(url: str, platform: str) -> bool:
    """
    Strict filtering to ensure URLs are actual profiles, not posts/content
    """
    url_lower = url.lower()
    
    if platform == 'facebook':
        print(f"üîß DEBUG: Checking Facebook URL with new logic: {url}")

        # ‚úÖ If URL contains excluded paths but has a recognizable profile ID, extract the profile
        excluded_paths = [
            '/posts/', '/post/', '/permalink.php', '/story.php',
            '/events/', '/event/', '/photos/', '/photo/', '/videos/', '/video/',
            '/groups/', '/group/', '/timeline/', '/about/overview',
            '/public/',
            '/search/',
            '/directory/', '/find-friends/',
            'photo.php'
        ]

        # If URL contains any excluded path, try to extract profile info
        if any(path in url_lower for path in excluded_paths):
            # For /public/ URLs, there's no extractable profile - reject them
            if any(reject_path in url_lower for reject_path in ['/public/', '/search/', 'photo.php', '/photo.php']):
                return False
    
        # Check if it's an excluded path but contains a profile identifier
        if any(path in url_lower for path in excluded_paths):
            # Extract profile ID from URLs like facebook.com/100076434464935/videos/...
            import re
            profile_match = re.search(r'facebook\.com/(\d{10,}|people/[^/]+/\d+)', url_lower)
            if profile_match:
                # Convert to clean profile URL
                profile_id = profile_match.group(1)
                if profile_id.startswith('people/'):
                    return f"https://www.facebook.com/{profile_id}"
                else:
                    return f"https://www.facebook.com/profile.php?id={profile_id}"
            return False
        
        # ‚ùå Exclude URLs with post IDs (long numbers) unless it's a direct profile
        import re
        if re.search(r'/\d{10,}', url) and not url_lower.endswith('/'):
            return False
        
        # ‚úÖ Accept clean profile URLs
        path_parts = url.split('facebook.com/')[-1].split('/')
        if len(path_parts) <= 2:  # Direct username or people/pages
            return True
        elif '/people/' in url_lower or '/pages/' in url_lower:
            return True
        return False
    
    elif platform == 'linkedin':
        # ‚ùå Exclude company pages, posts, articles
        excluded_paths = [
            '/school/', '/posts/', '/pulse/', '/in/recent-activity',
            '/feed/', '/detail/', '/activity-'
        ]
        if any(path in url_lower for path in excluded_paths):
            return False
            
        # ‚úÖ Only accept /in/ personal profiles
        return '/in/' in url_lower
    
    elif platform == 'twitter':
        # ‚ùå Exclude tweets, status updates, search results
        excluded_paths = [
            '/status/', '/statuses/', '/search', '/hashtag/', '/i/web/status',
            '/intent/', '/home', '/notifications', '/messages'
        ]
        if any(path in url_lower for path in excluded_paths):
            return False
            
        # ‚úÖ Accept clean profile URLs (x.com/username)
        username = url.split('/')[-1].split('?')[0]
        return len(username) > 0 and not username.isdigit()
    
    elif platform == 'website':
        # Website: Basic validation for business websites
        from urllib.parse import urlparse
        try:
            parsed = urlparse(url)
            # Ensure it's a proper website, not a social media platform
            excluded_domains = ['linkedin.com', 'facebook.com', 'twitter.com', 'x.com', 'youtube.com', 'instagram.com']
            return parsed.netloc not in excluded_domains and '.' in parsed.netloc
        except:
            return False

def get_crd_number(advisor_name, firm_name):
    """Use Tavily to find CRD number with multiple search strategies"""
    try:
        name_parts = advisor_name.strip().split()
        first_name = name_parts[0].lower()
        last_name = name_parts[-1].lower()
        
        # ‚úÖ Strategy 1: Progressive search with increasing specificity
        search_variations = [
            f'"{advisor_name}" "{firm_name}" CRD BrokerCheck FINRA',
            f'"{first_name} {last_name}" "{firm_name}" advisor CRD',
            f'"{advisor_name}" broker dealer CRD FINRA',
            f'site:brokercheck.finra.org "{advisor_name}"',
            f'site:reports.adviserinfo.sec.gov "{advisor_name}"',
        ]
        
        print(f"DEBUG: Searching for CRD for {advisor_name} at {firm_name}")
        
        # Store potential matches for validation
        potential_crds = []
        
        for search_query in search_variations:
            print(f"DEBUG: Trying search: {search_query}")
            crd_results = tavily_client.search(query=search_query, search_depth="advanced", max_results=8)
            
            for result in crd_results.get('results', []):
                url = result.get('url', '')
                title = result.get('title', '')
                content = result.get('content', '')
                all_text = f"{title} {content}".lower()
                
                print(f"DEBUG: Checking result: {title[:60]}...")
                
                # ‚úÖ Extract CRD from official URLs (most reliable)
                if any(domain in url for domain in ['brokercheck.finra.org', 'reports.adviserinfo.sec.gov']):
                    url_crd_match = re.search(r'/individual[_/](\d{6,8})', url)
                    if url_crd_match:
                        crd_from_url = url_crd_match.group(1)
                        
                        # ‚úÖ Calculate match confidence score
                        confidence_score = calculate_advisor_match_confidence(
                            advisor_name, firm_name, title, content, url
                        )
                        
                        potential_crds.append({
                            'crd': crd_from_url,
                            'confidence': confidence_score,
                            'source': 'url',
                            'title': title,
                            'url': url
                        })
                        
                        print(f"DEBUG: Found CRD {crd_from_url} with confidence {confidence_score}")

                # Also extract from content even for official URLs
                # Look for CRD patterns in content
                crd_patterns = [
                    r'CRD#?\s*:?\s*(\d{6,8})',
                    r'CRD\s+(?:number|no\.?|#)?\s*:?\s*(\d{6,8})',
                    r'(?:registration|reg)\s+(?:id|number):\s*(\d{6,8})'
                ]
    
                for pattern in crd_patterns:
                    crd_matches = re.findall(pattern, f"{title} {content}", re.IGNORECASE)  # Don't lowercase for regex
                    for crd_match in crd_matches:
                        if 6 <= len(crd_match) <= 8:
                            confidence_score = calculate_advisor_match_confidence(
                                advisor_name, firm_name, title, content, url
                            )
                
                            potential_crds.append({
                                'crd': crd_match,
                                'confidence': confidence_score,
                                'source': 'content',
                                'title': title,
                                'url': url
                            })
                            print(f"DEBUG: Found CRD {crd_match} from content with confidence {confidence_score}")
        
        # ‚úÖ Strategy 2: Content-based CRD extraction
        for search_query in search_variations[:3]:  # Use top 3 queries
            crd_results = tavily_client.search(query=search_query, max_results=5)
            
            for result in crd_results.get('results', []):
                title = result.get('title', '')
                content = result.get('content', '')
                all_text = f"{title} {content}".lower()
                
                # Look for CRD patterns in content
                crd_patterns = [
                    r'CRD#?\s*:?\s*(\d{6,8})',
                    r'CRD\s+(?:number|no\.?|#)?\s*:?\s*(\d{6,8})',
                    r'(?:registration|reg)\s+(?:id|number):\s*(\d{6,8})'
                ]
                
                for pattern in crd_patterns:
                    crd_matches = re.findall(pattern, all_text, re.IGNORECASE)
                    for crd_match in crd_matches:
                        if 6 <= len(crd_match) <= 8:
                            confidence_score = calculate_advisor_match_confidence(
                                advisor_name, firm_name, title, content, ''
                            )
                            
                            potential_crds.append({
                                'crd': crd_match,
                                'confidence': confidence_score,
                                'source': 'content',
                                'title': title,
                                'url': result.get('url', '')
                            })
        
        # ‚úÖ Strategy 3: Return best match with better tie-breaking
        if potential_crds:
            # Sort by confidence score, then prefer longer CRDs (individual advisors)
            potential_crds.sort(key=lambda x: (
                x['confidence'],
                len(x.get('crd', '')),  # Longer CRDs win ties (individual > firm)
                1 if 'individual' in x.get('url', '').lower() else 0
            ), reverse=True)
        
            print("DEBUG: All potential CRDs found:")
            for match in potential_crds[:5]:
                print(f"  CRD {match['crd']}: confidence {match['confidence']:.2f} from {match['source']} (len: {len(match.get('crd', ''))})")
    
            best_match = potential_crds[0]
    
            if best_match['confidence'] >= 2.0:
                print(f"DEBUG: ‚úÖ Selected CRD {best_match['crd']} with confidence {best_match['confidence']:.2f}")
                return best_match['crd']
            else:
                print(f"DEBUG: Best match CRD {best_match['crd']} has insufficient confidence {best_match['confidence']:.2f}, rejecting")
                return None
        
    except Exception as e:
        print(f"Error getting CRD via Tavily: {e}")
        return None

def calculate_advisor_match_confidence(advisor_name, firm_name, title, content, url):
    """Calculate confidence score for advisor match (0-10 scale)"""
    score = 0.0
    all_text = f"{title} {content}".lower()
    advisor_name_lower = advisor_name.lower()
    firm_name_lower = firm_name.lower()
    
    name_parts = advisor_name_lower.split()
    firm_parts = [part for part in firm_name_lower.split() if len(part) > 2]

    # ‚úÖ NEW: Better individual vs firm detection
    if 'individual' in url or '/individual/' in url:
        score += 2.0  # Strong bonus for individual advisor records
    elif 'firm' in url or '/firm/' in url or 'organization' in url:
        score -= 1.0  # Penalize firm records when looking for individuals
    elif any(word in title.lower() for word in ['firm', 'company', 'organization', 'llc']) and not any(person_word in title.lower() for person_word in ['advisor', 'adviser', 'individual', 'person']):
        score -= 1.0  # Penalize firm-focused titles
    
    # ‚úÖ CRD length heuristic (individual CRDs are typically longer)
    extracted_crd = None
    for pattern in [r'CRD#?\s*:?\s*(\d{6,8})', r'/individual[_/](\d{6,8})']:
        match = re.search(pattern, f"{title} {content} {url}", re.IGNORECASE)
        if match:
            extracted_crd = match.group(1)
            break
    
    if extracted_crd and len(extracted_crd) >= 7:
        score += 1.0  # Bonus for longer CRDs (typically individual advisors)
    elif extracted_crd and len(extracted_crd) <= 6:
        score -= 0.5  # Slight penalty for shorter CRDs (often firms)

    
    # ‚úÖ NEW: Strict name validation - this is the key fix
    advisor_first = name_parts[0] if name_parts else ""
    advisor_last = name_parts[-1] if len(name_parts) > 1 else ""
    
    # Check if this result is actually about the target advisor
    contains_first = advisor_first in all_text if advisor_first else False
    contains_last = advisor_last in all_text if advisor_last else False
    contains_full_name = advisor_name_lower in all_text
    
    # ‚úÖ CRITICAL: If this result doesn't mention the advisor's name, heavily penalize
    if not contains_full_name and not (contains_first and contains_last):
        # Check if it mentions OTHER people's names instead
        other_names = re.findall(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', f"{title} {content}")
        if other_names and not any(advisor_name.lower() in name.lower() for name in other_names):
            return -5.0  # Heavy penalty - this is clearly about someone else
    
    # ‚úÖ Name matching (most important - up to 5 points)
    if contains_full_name:
        score += 5.0  # Exact full name match gets highest score
    elif contains_first and contains_last:
        score += 3.0  # First + last name match
    elif contains_first or contains_last:
        score += 1.0  # Partial match
    else:
        score -= 2.0  # No name match is bad
    
    # ‚úÖ Firm context matching (up to 3 points)
    firm_matches = sum(1 for part in firm_parts if part in all_text)
    if firm_matches >= 2:
        score += 2.0
    elif firm_matches == 1:
        score += 1.0
    
    # ‚úÖ Official source bonus (up to 2 points)
    if any(domain in url for domain in ['brokercheck.finra.org', 'reports.adviserinfo.sec.gov']):
        score += 1.5
    
    # ‚úÖ Title relevance (up to 1 point)
    if any(keyword in all_text for keyword in ['advisor', 'adviser', 'financial', 'broker', 'investment']):
        score += 0.5
    
    return max(-10.0, score)  # Allow negative scores to filter out wrong people

# Add this to your search function:
def search_advisor_info(advisor_name: str, firm_name: str) -> dict:
    """Test Tavily first, then search"""
    
    # Test Tavily capabilities
    tavily_works = test_tavily_capabilities()
    
    if tavily_works:
        print("‚úÖ Tavily works! Proceeding with search...")
        tavily_results = get_advisor_profiles_with_tavily(advisor_name, firm_name)
    else:
        print("‚ùå Tavily failed! Using fallback...")
        tavily_results = {"linkedin": None, "twitter": None, "facebook": None, "website": None}

    # Get CRD number
    crd_number = get_crd_number(advisor_name, firm_name)
    
    return {
        'name': advisor_name,
        'firm': firm_name,
        'location': 'Not specified',
        'crd_number': crd_number,
        'socialMedia': tavily_results
    }

from urllib.parse import urljoin

def find_firm_website_by_name(firm_name: str) -> str | None:
    """Use Google CSE to find the firm website (first plausible result)."""
    try:
        q = f'"{firm_name}" official site'
        url = f"https://www.googleapis.com/customsearch/v1?q={q}&key={GOOGLE_API_KEY}&cx={SEARCH_ENGINE_ID}"
        r = requests.get(url, timeout=10)
        data = r.json()
        for item in data.get("items", []):
            link = item.get("link", "")
            # quick filter to skip social domains
            if any(s in link for s in ["linkedin.com", "facebook.com", "x.com", "twitter.com", "youtube.com"]):
                continue
            return link
    except Exception as e:
        print("find_firm_website_by_name error:", e)
    return None

# --- Firm site resolution helpers ---
from functools import lru_cache

BAD_HOST_SUBSTRINGS = (
    "linkedin.com", "facebook.com", "x.com", "twitter.com", "youtube.com",
    "glassdoor", "indeed", "wikipedia", "sec.gov", "brokercheck.finra.org",
    "zoominfo", "rocketreach", "mapquest", "yelp", "crunchbase",
    "adp", "workday", "greenhouse", "lever.co", "bamboohr"
)

TEAM_HINTS = ("about", "our-team", "team", "people", "staff", "advisors", "leadership", "who-we-are", "meet")

def _firm_tokens(name: str) -> set[str]:
    return set(t for t in re.split(r'[^a-z0-9]+', (name or "").lower()) if len(t) > 2)

def _is_social_or_junk(host: str) -> bool:
    host = (host or "").lower()
    return any(bad in host for bad in BAD_HOST_SUBSTRINGS)

def _google_cse(query: str) -> list[dict]:
    url = f"https://www.googleapis.com/customsearch/v1?q={requests.utils.quote(query)}&key={GOOGLE_API_KEY}&cx={SEARCH_ENGINE_ID}"
    try:
        r = requests.get(url, timeout=10)
        return r.json().get("items", []) or []
    except Exception as e:
        print("CSE error:", e)
        return []


from collections import deque
from urllib.parse import urljoin, urlparse

def _tokenize(s: str) -> set[str]:
    return set(t for t in re.split(r'[^a-z0-9]+', (s or '').lower()) if t)

def _score_link(href: str, text: str, advisor_tokens: set[str], base_domain: str) -> int:
    if not href: 
        return -10
    url = urlparse(href)
    # keep only same-site links
    if url.netloc and url.netloc.replace('www.', '') != base_domain.replace('www.', ''):
        return -10
    path = (url.path or '').lower()
    text_l = (text or '').lower()

    score = 0
    # name match in anchor text or URL ‚Üí strong
    if len(advisor_tokens & _tokenize(text_l)) >= 2: score += 6
    if len(advisor_tokens & _tokenize(path)) >= 2:  score += 4

    # team/bio-ish hints (but discovered from link itself, not hardcoded paths)
    hints = (
    'team', 'people', 'advisor', 'adviser', 'our', 'us',
    'bio', 'leadership', 'who-we-are', 'meet', 'staff', 'professionals',
    'team-member', 'our-team', 'meet-the-team'
    )
    if any(h in text_l for h in hints): score += 3
    if any(h in path for h in hints):  score += 2

    # page depth‚Äîprefer shallow
    depth = path.count('/')
    score += max(0, 3 - depth)

    # skip obvious non-profile pages
    bad = ('blog','news','careers','privacy','terms','press','events','webinar','insights','article')
    if any(b in path for b in bad): score -= 3

    return score

def find_name_page_urls_on_site(site_url: str, advisor_name: str, max_pages: int = 12) -> list[str]:
    """
    Crawl the homepage (and one hop) and return site URLs most likely
    to contain team members or the advisor's profile, ranked by score.
    """
    if not site_url.startswith(('http://','https://')):
        site_url = 'https://' + site_url

    base = site_url.rstrip('/')
    base_domain = urlparse(base).netloc.replace('www.', '')
    advisor_tokens = _tokenize(advisor_name)

    headers = {"User-Agent": "Mozilla/5.0"}
    seen, scored = set(), {}

    def collect(url):
        try:
            r = requests.get(url, headers=headers, timeout=10)
            if r.status_code != 200: 
                return []
            soup = BeautifulSoup(r.text, 'html.parser')
            links = []
            for a in soup.find_all('a', href=True):
                href = urljoin(url, a['href'])
                if '#' in href: 
                    href = href.split('#',1)[0]
                if href in seen: 
                    continue
                s = _score_link(href, a.get_text(strip=True), advisor_tokens, base_domain)
                if s > -10:
                    links.append((s, href))
            return links
        except Exception:
            return []

    # gather from homepage
    queue = deque([base])
    seen.add(base)

    # collect and rank homepage links
    for s, href in collect(base):
        seen.add(href)
        scored[href] = max(s, scored.get(href, -10))

    # follow the top few to one more hop
    for _, top_href in sorted(scored.items(), key=lambda kv: kv[1], reverse=True)[:8]:
        for s, href in collect(top_href):
            if href not in seen:
                seen.add(href)
                scored[href] = max(s, scored.get(href, -10))

    # return best candidates
    ranked = [u for u,_ in sorted(scored.items(), key=lambda kv: kv[1], reverse=True)]
    return ranked[:max_pages]

def extract_headshot_from_html(page_url: str, advisor_name: str) -> str | None:
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(page_url, headers=headers, timeout=10)
        if r.status_code != 200:
            return None
        soup = BeautifulSoup(r.text, "html.parser")

        # schema.org Person image wins immediately
        img_ld = _jsonld_person_image(soup, advisor_name)
        if img_ld:
            return urljoin(page_url, img_ld)

        best = (-1, None)
        for img in soup.find_all("img"):
            s, abs_src = _score_img(img, page_url, soup, advisor_name)
            if s > best[0]:
                best = (s, abs_src)

        # require a decent confidence (‚â•5). This avoids blog thumbnails.
        return best[1] if best[0] >= 5 else None
    except Exception as e:
        print("extract_headshot_from_html error:", e)
        return None

def _norm(s: str) -> str:
    return re.sub(r'\s+', ' ', s or '').strip().lower()

def _name_parts(full: str):
    full = _norm(full)
    toks = [t for t in re.split(r'[^a-z0-9]+', full) if t]
    return full, set(toks)

def _near_text_has_name(node, name_full, name_tokens) -> bool:
    # check node + close relatives for the name
    for cand in [node, getattr(node, 'parent', None)] + list(getattr(node, 'parents', []))[:2]:
        if not cand: 
            continue
        txt = _norm(cand.get_text(" ", strip=True))
        if name_full and name_full in txt:
            return True
        if len([t for t in name_tokens if t in txt]) >= max(2, min(3, len(name_tokens))):  # first+last at least
            return True
    return False

def _looks_like_portrait(w: int, h: int) -> bool:
    if not w or not h: 
        return False
    ar = h / max(1, w)
    return (0.9 <= ar <= 1.8) and (max(w, h) >= 160)

def _jsonld_person_image(soup, advisor_name):
    # prefer schema.org Person ‚Üí image
    for tag in soup.find_all('script', type='application/ld+json'):
        try:
            data = _json.loads(tag.string or "null")
        except Exception:
            continue
        blocks = data if isinstance(data, list) else [data]
        for b in blocks:
            if not isinstance(b, dict):
                continue
            typ = b.get('@type') or b.get('@type'.lower())
            name = b.get('name')
            image = b.get('image')
            if isinstance(typ, list):
                typ = [t.lower() for t in typ]
            elif isinstance(typ, str):
                typ = [typ.lower()]
            else:
                typ = []
            if 'person' in typ and name and _norm(name) == _norm(advisor_name) and image:
                return image if isinstance(image, str) else (image[0] if isinstance(image, list) and image else None)
    return None

def _score_img(img, page_url, soup, advisor_name):
    name_full, name_tokens = _name_parts(advisor_name)

    # src & dimensions
    src = img.get('src') or img.get('data-src') or img.get('data-original') or ''
    if not src:
        return -math.inf, None
    abs_src = urljoin(page_url, src)

    # quick filters: skip obvious logos/blog thumbs
    low = abs_src.lower()
    if any(k in low for k in ['logo', 'favicon', 'sprite', 'icon', 'banner', 'header', 'footer', 'placeholder', 'default']):
        return -math.inf, None

    # width/height if present
    try:
        w = int(img.get('width') or 0)
        h = int(img.get('height') or 0)
    except:
        w = h = 0

    # signals
    score = 0

    # 1) alt text contains full name ‚Üí strong
    alt = _norm(img.get('alt'))
    if alt and name_full in alt:
        score += 5
    elif alt and len([t for t in name_tokens if t in alt]) >= 2:
        score += 3

    # 2) nearby text (container/headings) has the name
    if _near_text_has_name(img, name_full, name_tokens):
        score += 4

    # 3) file name has name tokens
    fn = _norm(low.split('/')[-1])
    if len([t for t in name_tokens if t in fn]) >= 2:
        score += 2

    # 4) portrait-ish aspect & decent size
    if _looks_like_portrait(w, h):
        score += 1

    # 5) page looks like team/bio
    page_l = page_url.lower()
    if any(k in page_l for k in ['/team', '/people', '/about', '/our-team', '/advisors', '/leadership', '/who-we-are']):
        score += 1

    return score, abs_src

@lru_cache(maxsize=256)
def resolve_firm_site(advisor_name: str, firm_name: str) -> str | None:
    """Find the firm's canonical site using advisor+firm signals + light verification."""
    advisor_name = (advisor_name or "").strip()
    firm_name = (firm_name or "").strip()
    firm_toks = _firm_tokens(firm_name)

    # 1) Queries that strongly bias to the real firm
    queries = [
        f'"{advisor_name}" "{firm_name}" official site',
        f'"{advisor_name}" "{firm_name}" site:.com',
        f'"{firm_name}" investment advisory site:.com',
        f'"{firm_name}" "About" site:.com',
    ]

    candidates: dict[str, float] = {}

    def add_candidate(url: str, base_score: float):
        try:
            host = urlparse(url).netloc.replace("www.", "").lower()
            if not host or _is_social_or_junk(host):
                return
            canon = f"https://{host}"
            score = base_score + sum(1 for t in firm_toks if t in host) * 1.5
            candidates[canon] = max(score, candidates.get(canon, -1e9))
        except Exception:
            pass

    # 2) collect candidate hosts from CSE
    for q in queries:
        for item in _google_cse(q):
            add_candidate(item.get("link", ""), 5.0)

    if not candidates:
        return None

    # 3) verify: does Google see advisor name on the domain? does homepage show team/ about?
    ranked = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)[:6]
    best = (0.0, None)
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/124 Safari/537.36"}

    for base, score in ranked:
        # advisor on site?
        site_q = f'site:{urlparse(base).netloc} "{advisor_name}"'
        site_hits = _google_cse(site_q)
        if site_hits:
            score += 6.0

        # homepage hints
        try:
            r = requests.get(base, headers=headers, timeout=8)
            if r.ok:
                soup = BeautifulSoup(r.text, "html.parser")
                hrefs = [a.get("href","").lower() for a in soup.find_all("a", href=True)]
                if any(any(k in h for k in TEAM_HINTS) for h in hrefs):
                    score += 2.0
        except Exception:
            pass

        if score > best[0]:
            best = (score, base)

    return best[1]


def get_simple_logo_fallback(domain: str, firm_name: str) -> str:
    """Simple logo fallback with multiple services - no size checking"""
    
    logo_services = [
        f"https://logo.clearbit.com/{domain}",
        f"https://img.logo.dev/{domain}?token=pk_X1DkbVgWSpeuvyFQyTKiIg",
        f"https://logo.uplead.com/{domain}",
        f"https://www.google.com/s2/favicons?domain={domain}&sz=128"
    ]
    
    # Test each service with simple availability check
    for logo_url in logo_services:
        try:
            response = requests.head(logo_url, timeout=3, allow_redirects=True)  # ‚úÖ Added allow_redirects=True
            if 200 <= response.status_code < 400:  # ‚úÖ Accept 200-399 (including redirects)
                print(f"‚úÖ Found logo: {logo_url}")
                return logo_url
            else:
                print(f"‚ö†Ô∏è Service returned {response.status_code}: {logo_url}")
        except Exception as e:
            print(f"‚ö†Ô∏è Service failed: {logo_url} - {e}")
    
    # Final fallback: company initials
    initials = ''.join([word[0].upper() for word in firm_name.split() if word and word[0].isalpha()][:3])
    if not initials or len(initials) < 2:
        initials = "CO"
    
    print(f"üìù Using placeholder for {firm_name}: {initials}")
    return f"https://via.placeholder.com/100x50/1e3a5f/ffffff?text={initials}"



def format_datetime(dt_string):
    """Format datetime string to MM/DD/YYYY at H:MM AM/PM in local timezone"""
    if not dt_string:
        return "Never"
    
    try:
        # Parse the datetime string (assuming it's in UTC)
        if 'T' in dt_string or 'Z' in dt_string:
            # ISO format
            dt = datetime.fromisoformat(dt_string.replace('Z', '+00:00'))
        else:
            # Standard format
            dt = datetime.strptime(dt_string, '%Y-%m-%d %H:%M:%S')
            # Assume it's UTC if no timezone info
            dt = dt.replace(tzinfo=timezone.utc)
        
        # Convert to local timezone
        local_dt = dt.replace(tzinfo=timezone.utc).astimezone()
        
        # Format to MM/DD/YYYY at H:MM AM/PM
        formatted = local_dt.strftime('%m/%d/%Y at %I:%M %p')
        # Remove leading zero from hour if present
        if formatted[11] == '0':
            formatted = formatted[:11] + formatted[12:]
        return formatted
    except:
        try:
            # Fallback: try parsing without timezone assumptions
            dt = datetime.strptime(dt_string, '%Y-%m-%d %H:%M:%S')
            formatted = dt.strftime('%m/%d/%Y at %I:%M %p')
            if formatted[11] == '0':
                formatted = formatted[:11] + formatted[12:]
            return formatted + " (Server Time)"
        except:
            return dt_string  # Return original if all parsing fails        

# Register the filter for use in templates
app.jinja_env.filters['format_datetime'] = format_datetime

def update_compliance_schema():
    """Add compliance_checked_at column to website_snapshots table"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check if compliance_checked_at column exists
            c.execute("PRAGMA table_info(website_snapshots)")
            columns = [column[1] for column in c.fetchall()]
            
            if 'compliance_checked_at' not in columns:
                try:
                    c.execute('ALTER TABLE website_snapshots ADD COLUMN compliance_checked_at TIMESTAMP')
                    logger.info("Added compliance_checked_at column to website_snapshots table")
                except Exception as e:
                    logger.error(f"Error adding compliance_checked_at column: {str(e)}")
            
            conn.commit()
            conn.close()
            logger.info("Compliance schema update completed successfully")
            
    except Exception as e:
        logger.error(f"Error updating compliance schema: {str(e)}")

def perform_whole_text_compliance_check(text, page_num=None):
    """Check entire text as one unit without sentence splitting"""
    try:
        global BERT_MODEL, BERT_TOKENIZER
        if BERT_MODEL is None or BERT_TOKENIZER is None:
            logger.info("Initializing BERT model for compliance check...")
            if not initialize_bert():
                logger.error("Failed to initialize BERT model")
                raise Exception("Failed to initialize BERT model")

        logger.info(f"Running whole-text compliance check on {len(text)} characters")
        
        # If text is empty, contains only whitespace, or is just a bullet point
        if not text or text.isspace() or text == "‚Ä¢" or text == "\u2022":
            logger.info("Text is empty or just a bullet point - skipping")
            return {"compliant": True, "flagged_instances": []}

        # If text is a dictionary, extract the 'text' field
        if isinstance(text, dict):
            text = text.get('text', '')
            logger.info(f"Extracted text from dictionary, length: {len(text)}")
        
        # Clean the text (same as your original function)
        original_len = len(text)
        text = re.sub(r'[‚Ä¢\u2022]', '', text)  # Remove bullet points
        text = re.sub(r'\s+', ' ', text).strip()  # Clean up whitespace
        logger.info(f"Cleaned text: {original_len} chars ‚Üí {len(text)} chars")
        
        # Check for skepticism words
        skepticism_words = load_skepticism_words()
        text_lower = text.lower()
        has_skepticism_words = contains_skepticism_words(text)
        
        # Prepare input for BERT
        inputs = BERT_TOKENIZER(text, 
                              return_tensors="pt",
                              truncation=True,
                              max_length=1024,
                              padding=True)
        
        # Make prediction using BERT
        with torch.no_grad():
            outputs = BERT_MODEL(**inputs)
            probabilities = softmax(outputs.logits, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][prediction].item()

        # Override if skepticism words found
        if prediction == 0 and has_skepticism_words:
            prediction = 1  # Override to non-compliant
            found_words = [w for w in skepticism_words if w.lower() in text_lower]
            logger.info(f"BERT override: Found skepticism words {found_words} in text")

        logger.info(f"BERT prediction for entire text: {prediction} ({'compliant' if prediction == 0 else 'non-compliant'}), Confidence: {confidence:.1%}")
        
        if prediction == 1 and confidence > 0.7:  # Non-compliant with sufficient confidence
            # Create the flagged instance
            flagged_instance = {
                "flagged_instance": text,
                "confidence": confidence,
                "page": page_num
            }
            
            # Verify with GPT
            logger.info("Sending entire text to GPT for verification...")
            verified_instances = verify_with_gpt_fully_batched([flagged_instance], batch_size=1)
            
            if verified_instances and len(verified_instances) > 0:
                logger.info("GPT confirms entire text is non-compliant")
                return {
                    "compliant": False,
                    "flagged_instances": verified_instances
                }
            else:
                logger.info("GPT determined entire text is compliant")
                return {"compliant": True, "flagged_instances": []}
        else:
            logger.info("BERT determined entire text is compliant or confidence too low")
            return {"compliant": True, "flagged_instances": []}
        
    except Exception as e:
        logger.error(f"Error in whole-text compliance check: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return {"compliant": True, "flagged_instances": []}

# Helper to scrape new tweets with Selenium (replicates your manual login method)
def scrape_new_tweets_selenium(twitter_handle, last_known_tweet_url=None):
    options = Options()
    # options.add_argument("--headless")  # Comment this out to make browser visible for manual login
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    driver = webdriver.Chrome(options=options)
    new_tweets = []
    
    try:
        url = f"https://twitter.com/{twitter_handle}"
        driver.get(url)
        
        # Optional: Add login if needed for private/protected content (uncomment and fill credentials)
        # This replicates "manual login" - use your actual Twitter test account creds
        # try:
        #     WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, "session[username_or_email]")))
        #     driver.find_element(By.NAME, "session[username_or_email]").send_keys("your_twitter_username")
        #     driver.find_element(By.NAME, "session[password]").send_keys("your_twitter_password")
        #     driver.find_element(By.CSS_SELECTOR, '[data-testid="LoginForm_Login_Button"]').click()
        #     time.sleep(5)  # Wait for login
        # except TimeoutException:
        #     logger.warning("Login elements not found - proceeding without login")
        
        # Wait for tweets to load
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="tweet"]')))
        
        scroll_attempts = 0
        max_scrolls = 20  # Limit to prevent infinite scrolling
        
        while scroll_attempts < max_scrolls:
            tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
            
            for tweet in tweet_elements:
                try:
                    # Extract tweet details
                    tweet_text = tweet.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]').text
                    tweet_links = tweet.find_elements(By.CSS_SELECTOR, 'a[href*="status"]')
                    tweet_url = tweet_links[0].get_attribute('href') if tweet_links else None
                    
                    if tweet_url and last_known_tweet_url and tweet_url == last_known_tweet_url:
                        # Found the last known tweet - stop scraping
                        logger.info(f"Reached known tweet: {tweet_url}")
                        return new_tweets
                    
                    if tweet_url and tweet_text:  # Only add valid tweets
                        new_tweets.append({
                            'content': tweet_text,
                            'url': tweet_url,
                            # Add more metadata if needed, e.g., date from tweet.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')
                        })
                except:
                    continue  # Skip malformed tweets
            
            # Scroll down for more
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # Short wait for load
            scroll_attempts += 1
            
    except Exception as e:
        logger.error(f"Error scraping Twitter with Selenium: {e}")
    finally:
        driver.quit()
    
    return new_tweets

# Helper to get the URL of the most recent Twitter post in DB (for stopping point)
def get_last_twitter_post_url(domain):
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT url FROM advisor_carousels 
            WHERE domain = %s AND platform = 'twitter'
            ORDER BY scan_date DESC LIMIT 1
        """, (domain,))
        
        last_url = cursor.fetchone()
        cursor.close()
        
        return last_url[0] if last_url else None
        
    except Exception as e:
        logger.error(f"Error getting last Twitter URL for {domain}: {e}")
        return None
    finally:
        if conn:
            release_db_connection(conn)

# Global dictionaries to track bulk compliance check progress
bulk_facebook_compliance_in_progress = False
bulk_linkedin_compliance_in_progress = False
bulk_twitter_compliance_in_progress = False

@app.route('/good')
def compliance_scanner():
    return render_template('good.html')

@app.route('/check_all_facebook_posts', methods=['GET'])
def check_all_facebook_posts():
    global bulk_facebook_compliance_in_progress
    
    if bulk_facebook_compliance_in_progress:
        flash('Bulk Facebook compliance check is already in progress. Please wait for it to complete.', 'warning')
        return redirect(url_for('dashboard'))
    
    try:
        # Get all Facebook profiles and their unchecked posts
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get all Facebook pages with content
            c.execute('''SELECT DISTINCT advisor_id, page_url, content_text FROM website_snapshots 
                        WHERE (page_url LIKE '%facebook.com%' OR page_url LIKE '%fb.com%') 
                        AND content_text IS NOT NULL AND content_text != ''
                        ORDER BY advisor_id''')
            facebook_pages = c.fetchall()
            conn.close()
        
        if not facebook_pages:
            flash('No Facebook profiles found to check.', 'info')
            return redirect(url_for('dashboard'))
        
        def bulk_facebook_compliance_check():
            global bulk_facebook_compliance_in_progress
            with app.app_context():
                try:
                    bulk_facebook_compliance_in_progress = True
                    total_posts_checked = 0
                    posts_with_issues = []
                
                    logger.info(f"=== BULK FACEBOOK COMPLIANCE CHECK STARTED ===")
                    logger.info(f"Found {len(facebook_pages)} Facebook profiles to check")
                
                    for advisor_id, page_url, content_text in facebook_pages:
                        try:
                            logger.info(f"Processing Facebook profile: {page_url} (Advisor ID: {advisor_id})")
                        
                            # Parse all posts from this page
                            if content_text and 'Post ' in content_text:
                                post_sections = content_text.split('Post ')
                            
                                for section in post_sections:
                                    if section.strip() and section.strip() != '':
                                        lines = section.strip().split('\n')
                                        if len(lines) > 1:
                                            raw_post_id = lines[0].strip()
                                            post_id = raw_post_id.split('(')[0].strip().rstrip(':')
                                        
                                            # Check if this post has already been compliance checked
                                            with db_lock:
                                                conn = get_db_connection_sqlite()
                                                c = conn.cursor()
                                                c.execute('''SELECT COUNT(*) FROM compliance_checks 
                                                        WHERE advisor_id = ? AND page_url = ? 
                                                        AND post_type = 'facebook_post' AND post_id = ?''',
                                                     (advisor_id, page_url, post_id))
                                                check_exists = c.fetchone()[0] > 0
                                                conn.close()
                                        
                                            if not check_exists:
                                                logger.info(f"Checking unchecked Facebook post: {post_id}")
                                            
                                                # Extract post content
                                                post_text = []
                                                for line in lines[1:]:
                                                    if line.strip():
                                                        post_text.append(line.strip())
                                                specific_post_content = '\n'.join(post_text)
                                            
                                                if specific_post_content:
                                                    # Run compliance check
                                                    compliance_result = perform_compliance_check(specific_post_content, page_num=1)
                                                
                                                    # Store compliance check record
                                                    result_status = 'compliant' if compliance_result.get("compliant") else 'non-compliant'
                                                
                                                    with db_lock:
                                                        conn = get_db_connection_sqlite()
                                                        c = conn.cursor()
                                                    
                                                        # Record the check
                                                        c.execute('''INSERT INTO compliance_checks 
                                                                    (advisor_id, page_url, post_type, post_id, result)
                                                                    VALUES (?, ?, ?, ?, ?)''',
                                                                 (advisor_id, page_url, 'facebook_post', post_id, result_status))
                                                    
                                                        # Clear existing issues for this post
                                                        c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?', 
                                                                 (advisor_id, page_url, f'%Post {post_id}%'))
                                                    
                                                        issues_added = 0
                                                        # Store new compliance issues if any
                                                        if not compliance_result.get("compliant"):
                                                            flagged_instances = compliance_result.get("flagged_instances", [])
                                                            posts_with_issues.append(f"{page_url} - Post {post_id}")
                                                        
                                                            for instance in flagged_instances:
                                                                flagged_text = f"Facebook Post {post_id}: {instance.get('flagged_instance', '')}"
                                                            
                                                                c.execute('''INSERT INTO compliance_issues 
                                                                            (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                                                            VALUES (?, ?, ?, ?, ?, ?)''',
                                                                         (advisor_id, page_url, 
                                                                          flagged_text,
                                                                          instance.get('specific_compliant_alternative', ''), 
                                                                          str(instance.get('confidence', '')), 
                                                                          instance.get('rationale', '')))
                                                                issues_added += 1
                                                    
                                                        # Update compliance check timestamp
                                                        from datetime import datetime
                                                        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                                        c.execute('''UPDATE website_snapshots 
                                                                    SET compliance_checked_at = ? 
                                                                    WHERE advisor_id = ? AND page_url = ?''',
                                                                 (current_time, advisor_id, page_url))
                                                    
                                                        conn.commit()
                                                        conn.close()
                                                
                                                    total_posts_checked += 1
                                                    logger.info(f"‚úÖ Checked Facebook Post {post_id}: {issues_added} issues found")
                                                
                                                    # Small delay between posts
                                                    time.sleep(1)
                        
                            # Small delay between profiles  
                            time.sleep(2)
                        
                        except Exception as e:
                            logger.error(f"Error checking Facebook profile {page_url}: {e}")
                            continue
                
                    # Final summary
                    logger.info(f"=== BULK FACEBOOK COMPLIANCE CHECK COMPLETED ===")
                    logger.info(f"{total_posts_checked} Facebook posts checked, {len(posts_with_issues)} posts found with issues")
                    if posts_with_issues:
                        for post_info in posts_with_issues:
                            logger.info(f"{post_info}")
                        
                except Exception as e:
                    logger.error(f"Error in bulk Facebook compliance check: {e}")
                finally:
                    bulk_facebook_compliance_in_progress = False
        
        # Start background compliance check
        threading.Thread(target=bulk_facebook_compliance_check, daemon=True).start()
        
        
    except Exception as e:
        logger.error(f"Error starting bulk Facebook compliance check: {e}")
        flash('Error starting bulk Facebook compliance check.', 'error')
    
    return redirect(url_for('dashboard'))


@app.route('/check_all_linkedin_posts', methods=['GET'])
def check_all_linkedin_posts():
    global bulk_linkedin_compliance_in_progress
    
    if bulk_linkedin_compliance_in_progress:
        flash('Bulk LinkedIn compliance check is already in progress. Please wait for it to complete.', 'warning')
        return redirect(url_for('dashboard'))
    
    try:
        # Get all LinkedIn profiles and their unchecked posts
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get all LinkedIn pages with content
            c.execute('''SELECT DISTINCT advisor_id, page_url, content_text FROM website_snapshots 
                        WHERE page_url LIKE '%linkedin.com%' 
                        AND content_text IS NOT NULL AND content_text != ''
                        ORDER BY advisor_id''')
            linkedin_pages = c.fetchall()
            conn.close()
        
        if not linkedin_pages:
            flash('No LinkedIn profiles found to check.', 'info')
            return redirect(url_for('dashboard'))
        
        def bulk_linkedin_compliance_check():
            global bulk_linkedin_compliance_in_progress
            with app.app_context():
                try:
                    bulk_linkedin_compliance_in_progress = True
                    total_posts_checked = 0
                    posts_with_issues = []
                
                    logger.info(f"=== BULK LINKEDIN COMPLIANCE CHECK STARTED ===")
                    logger.info(f"Found {len(linkedin_pages)} LinkedIn profiles to check")
                
                    for advisor_id, page_url, content_text in linkedin_pages:
                        try:
                            logger.info(f"Processing LinkedIn profile: {page_url} (Advisor ID: {advisor_id})")
                        
                            # Parse all posts from this page
                            if content_text and 'Post ' in content_text:
                                post_sections = content_text.split('Post ')
                            
                                for section in post_sections:
                                    if section.strip() and section.strip() != '':
                                        lines = section.strip().split('\n')
                                        if len(lines) > 1:
                                            raw_post_id = lines[0].strip()
                                            post_id = raw_post_id.split('(')[0].strip().rstrip(':')
                                            
                                            # Check if this post has already been compliance checked
                                            with db_lock:
                                                conn = get_db_connection_sqlite()
                                                c = conn.cursor()
                                                c.execute('''SELECT COUNT(*) FROM compliance_checks 
                                                            WHERE advisor_id = ? AND page_url = ? 
                                                            AND post_type = 'linkedin_post' AND post_id = ?''',
                                                         (advisor_id, page_url, post_id))
                                                check_exists = c.fetchone()[0] > 0
                                                conn.close()
                                        
                                            if not check_exists:
                                                logger.info(f"Checking unchecked LinkedIn post: {post_id}")
                                            
                                                # Extract post content
                                                post_text = []
                                                for line in lines[1:]:
                                                    if line.strip():
                                                        post_text.append(line.strip())
                                                specific_post_content = '\n'.join(post_text)
                                            
                                                if specific_post_content:
                                                    # Run compliance check
                                                    compliance_result = perform_compliance_check(specific_post_content, page_num=1)
                                                
                                                    # Store compliance check record
                                                    result_status = 'compliant' if compliance_result.get("compliant") else 'non-compliant'
                                                
                                                    with db_lock:
                                                        conn = get_db_connection_sqlite()
                                                        c = conn.cursor()
                                                    
                                                        # Record the check
                                                        c.execute('''INSERT INTO compliance_checks 
                                                                    (advisor_id, page_url, post_type, post_id, result)
                                                                    VALUES (?, ?, ?, ?, ?)''',
                                                                 (advisor_id, page_url, 'linkedin_post', post_id, result_status))
                                                    
                                                        # Clear existing issues for this post
                                                        c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?', 
                                                                 (advisor_id, page_url, f'%LinkedIn Post {post_id}%'))
                                                    
                                                        issues_added = 0
                                                        # Store new compliance issues if any
                                                        if not compliance_result.get("compliant"):
                                                            flagged_instances = compliance_result.get("flagged_instances", [])
                                                            posts_with_issues.append(f"{page_url} - Post {post_id}")
                                                        
                                                            for instance in flagged_instances:
                                                                flagged_text = f"LinkedIn Post {post_id}: {instance.get('flagged_instance', '')}"
                                                            
                                                                c.execute('''INSERT INTO compliance_issues 
                                                                            (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                                                            VALUES (?, ?, ?, ?, ?, ?)''',
                                                                         (advisor_id, page_url, 
                                                                          flagged_text,
                                                                          instance.get('specific_compliant_alternative', ''), 
                                                                          str(instance.get('confidence', '')), 
                                                                          instance.get('rationale', '')))
                                                                issues_added += 1
                                                    
                                                        # Update compliance check timestamp
                                                        from datetime import datetime
                                                        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                                        c.execute('''UPDATE website_snapshots 
                                                                    SET compliance_checked_at = ? 
                                                                    WHERE advisor_id = ? AND page_url = ?''',
                                                                 (current_time, advisor_id, page_url))
                                                    
                                                        conn.commit()
                                                        conn.close()
                                                    
                                                    total_posts_checked += 1
                                                    logger.info(f"‚úÖ Checked LinkedIn Post {post_id}: {issues_added} issues found")
                                                
                                                    # Small delay between posts
                                                    time.sleep(1)
                        
                            # Small delay between profiles  
                            time.sleep(2)
                        
                        except Exception as e:
                            logger.error(f"Error checking LinkedIn profile {page_url}: {e}")
                            continue
                
                    # Final summary
                    logger.info(f"=== BULK LINKEDIN COMPLIANCE CHECK COMPLETED ===")
                    logger.info(f"{total_posts_checked} LinkedIn posts checked, {len(posts_with_issues)} posts found with issues")
                    if posts_with_issues:
                        for post_info in posts_with_issues:
                            logger.info(f"{post_info}")
                        
                except Exception as e:
                    logger.error(f"Error in bulk LinkedIn compliance check: {e}")
                finally:
                    bulk_linkedin_compliance_in_progress = False
        
        # Start background compliance check
        threading.Thread(target=bulk_linkedin_compliance_check, daemon=True).start()
        
        
    except Exception as e:
        logger.error(f"Error starting bulk LinkedIn compliance check: {e}")
        flash('Error starting bulk LinkedIn compliance check.', 'error')
    
    return redirect(url_for('dashboard'))


@app.route('/check_all_twitter_posts', methods=['GET'])
def check_all_twitter_posts():
    global bulk_twitter_compliance_in_progress
    
    if bulk_twitter_compliance_in_progress:
        flash('Bulk Twitter compliance check is already in progress. Please wait for it to complete.', 'warning')
        return redirect(url_for('dashboard'))
    
    try:
        # Get all Twitter profiles and their unchecked posts
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get all Twitter pages with content
            c.execute('''SELECT DISTINCT advisor_id, page_url, content_text FROM website_snapshots 
                        WHERE (page_url LIKE '%x.com%' OR page_url LIKE '%twitter.com%') 
                        AND content_text IS NOT NULL AND content_text != ''
                        ORDER BY advisor_id''')
            twitter_pages = c.fetchall()
            conn.close()
        
        if not twitter_pages:
            flash('No Twitter profiles found to check.', 'info')
            return redirect(url_for('dashboard'))
        
        def bulk_twitter_compliance_check():
            global bulk_twitter_compliance_in_progress
            with app.app_context():
                try:
                    bulk_twitter_compliance_in_progress = True
                    total_posts_checked = 0
                    posts_with_issues = []
                
                    logger.info(f"=== BULK TWITTER COMPLIANCE CHECK STARTED ===")
                    logger.info(f"Found {len(twitter_pages)} Twitter profiles to check")
                
                    for advisor_id, page_url, content_text in twitter_pages:
                        try:
                            logger.info(f"Processing Twitter profile: {page_url} (Advisor ID: {advisor_id})")
                        
                            # Parse all tweets from this page
                            if content_text and 'Tweet ' in content_text:
                                tweet_sections = content_text.split('Tweet ')
                            
                                for section in tweet_sections:
                                    if section.strip() and section.strip() != '':
                                        lines = section.strip().split('\n')
                                        if len(lines) > 1:
                                            raw_tweet_id = lines[0].strip()
                                            tweet_id = raw_tweet_id.split('(')[0].strip().rstrip(':')
                                        
                                            # Check if this tweet has already been compliance checked
                                            with db_lock:
                                                conn = get_db_connection_sqlite()
                                                c = conn.cursor()
                                                c.execute('''SELECT COUNT(*) FROM compliance_checks 
                                                            WHERE advisor_id = ? AND page_url = ? 
                                                            AND post_type = 'tweet' AND post_id = ?''',
                                                         (advisor_id, page_url, tweet_id))
                                                check_exists = c.fetchone()[0] > 0
                                                conn.close()
                                        
                                            if not check_exists:
                                                logger.info(f"Checking unchecked tweet: {tweet_id}")
                                            
                                                # Extract tweet content
                                                tweet_text = []
                                                for line in lines[1:]:
                                                    if line.strip():
                                                        tweet_text.append(line.strip())
                                                specific_tweet_content = '\n'.join(tweet_text)
                                            
                                                if specific_tweet_content:
                                                    # Run compliance check (use the Twitter-specific function)
                                                    compliance_result = perform_whole_text_compliance_check(specific_tweet_content)
                                                
                                                    # Store compliance check record
                                                    result_status = 'compliant' if compliance_result.get("compliant") else 'non-compliant'
                                                
                                                    with db_lock:
                                                        conn = get_db_connection_sqlite()
                                                        c = conn.cursor()
                                                    
                                                        # Record the check
                                                        c.execute('''INSERT INTO compliance_checks 
                                                                    (advisor_id, page_url, post_type, post_id, result)
                                                                    VALUES (?, ?, ?, ?, ?)''',
                                                                 (advisor_id, page_url, 'tweet', tweet_id, result_status))
                                                    
                                                        # Clear existing issues for this tweet
                                                        c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?', 
                                                                 (advisor_id, page_url, f'%Tweet {tweet_id}%'))
                                                    
                                                        issues_added = 0
                                                        # Store new compliance issues if any
                                                        if not compliance_result.get("compliant"):
                                                            flagged_instances = compliance_result.get("flagged_instances", [])
                                                            posts_with_issues.append(f"{page_url} - Tweet {tweet_id}")
                                                        
                                                            for instance in flagged_instances:
                                                                flagged_text = f"Tweet {tweet_id}: {instance.get('flagged_instance', '')}"
                                                            
                                                                c.execute('''INSERT INTO compliance_issues 
                                                                            (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                                                            VALUES (?, ?, ?, ?, ?, ?)''',
                                                                         (advisor_id, page_url, 
                                                                          flagged_text,
                                                                          instance.get('specific_compliant_alternative', ''), 
                                                                          str(instance.get('confidence', '')), 
                                                                          instance.get('rationale', '')))
                                                                issues_added += 1
                                                    
                                                        # Update compliance check timestamp
                                                        from datetime import datetime
                                                        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                                        c.execute('''UPDATE website_snapshots 
                                                                    SET compliance_checked_at = ? 
                                                                    WHERE advisor_id = ? AND page_url = ?''',
                                                                 (current_time, advisor_id, page_url))
                                                    
                                                        conn.commit()
                                                        conn.close()
                                                
                                                    total_posts_checked += 1
                                                    logger.info(f"‚úÖ Checked Tweet {tweet_id}: {issues_added} issues found")
                                                
                                                    # Small delay between posts
                                                    time.sleep(1)
                        
                            # Small delay between profiles  
                            time.sleep(2)
                        
                        except Exception as e:
                            logger.error(f"Error checking Twitter profile {page_url}: {e}")
                            continue
                
                    # Final summary
                    logger.info(f"=== BULK TWITTER COMPLIANCE CHECK COMPLETED ===")
                    logger.info(f"{total_posts_checked} Twitter posts checked, {len(posts_with_issues)} posts found with issues")
                    if posts_with_issues:
                        for post_info in posts_with_issues:
                            logger.info(f"{post_info}")
                        
                except Exception as e:
                    logger.error(f"Error in bulk Twitter compliance check: {e}")
                finally:
                    bulk_twitter_compliance_in_progress = False
        
        # Start background compliance check
        threading.Thread(target=bulk_twitter_compliance_check, daemon=True).start()
        
        
    except Exception as e:
        logger.error(f"Error starting bulk Twitter compliance check: {e}")
        flash('Error starting bulk Twitter compliance check.', 'error')
    
    return redirect(url_for('dashboard'))

# Global dictionaries to track bulk refresh progress
bulk_facebook_in_progress = False
bulk_linkedin_in_progress = False
bulk_twitter_in_progress = False

@app.route('/refresh_all_facebook', methods=['GET'])
def refresh_all_facebook():
    global bulk_facebook_in_progress
    
    if bulk_facebook_in_progress:
        flash('Bulk Facebook refresh is already in progress. Please wait for it to complete.', 'warning')
        return redirect(url_for('dashboard'))
    
    try:
        # Get all Facebook profiles across all advisors
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            c.execute('''SELECT DISTINCT advisor_id, page_url FROM website_snapshots 
                        WHERE page_url LIKE '%facebook.com%' OR page_url LIKE '%fb.com%'
                        ORDER BY advisor_id''')
            facebook_profiles = c.fetchall()
            conn.close()
        
        if not facebook_profiles:
            flash('No Facebook profiles found to refresh.', 'info')
            return redirect(url_for('dashboard'))
        
        def bulk_facebook_refresh():
            global bulk_facebook_in_progress
            try:
                bulk_facebook_in_progress = True
                total_profiles = len(facebook_profiles)
                profiles_with_new_content = []
                
                logger.info(f"=== BULK FACEBOOK REFRESH STARTED ===")
                logger.info(f"Found {total_profiles} Facebook profiles to refresh")
                
                for advisor_id, facebook_url in facebook_profiles:
                    try:
                        logger.info(f"Processing Facebook profile: {facebook_url} (Advisor ID: {advisor_id})")
                        
                        # Get existing content
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                                     (advisor_id, facebook_url))
                            existing_data = c.fetchone()
                            conn.close()
                        
                        existing_posts_text = existing_data[0] if existing_data else ""
                        
                        # Use the incremental scraper
                        new_posts = scrape_facebook_new_posts_only(facebook_url, existing_posts_text)
                        
                        if new_posts:
                            logger.info(f"Found {len(new_posts)} NEW posts for {facebook_url}")
                            profiles_with_new_content.append(facebook_url)
                            
                            # Prepend new posts to existing content
                            new_post_content = ""
                            for i, post in enumerate(new_posts, 1):
                                post_text = post.get('text', '')
                                post_timestamp = post.get('timestamp', 'Unknown')
                                if post_text:
                                    clean_post_text = post_text.replace('... See more', '').replace('...See more', '').strip()
                                    new_post_content += f"Post NEW{i} ({post_timestamp}):\n{clean_post_text}\n\n"
                            
                            # Smart content combination
                            if existing_posts_text and "FACEBOOK PROFILE:" in existing_posts_text:
                                parts = existing_posts_text.split('\n\n', 1)
                                header = parts[0] if parts else f"FACEBOOK PROFILE: {facebook_url}"
                                existing_content = parts[1] if len(parts) > 1 else ""
                                updated_content = f"{header}\n\n{new_post_content}{existing_content}"
                            else:
                                updated_content = f"FACEBOOK PROFILE: {facebook_url}\n\n{new_post_content}{existing_posts_text}"
                            
                            content_hash = hashlib.md5(updated_content.encode()).hexdigest()
                            
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET content_text = ?, content_hash = ?, last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (updated_content, content_hash, advisor_id, facebook_url))
                                conn.commit()
                                conn.close()
                        else:
                            # Still update timestamp
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (advisor_id, facebook_url))
                                conn.commit()
                                conn.close()
                        
                        # Small delay between profiles
                        time.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"Error refreshing Facebook profile {facebook_url}: {e}")
                        continue
                
                # Final summary
                logger.info(f"=== BULK FACEBOOK REFRESH COMPLETED ===")
                logger.info(f"{total_profiles} Facebook profiles scraped, {len(profiles_with_new_content)} profiles added with New Content")
                if profiles_with_new_content:
                    for profile_url in profiles_with_new_content:
                        logger.info(f"{profile_url}")

                app.config['BULK_FACEBOOK_COMPLETE'] = {
                    'platform': 'Facebook',
                    'total_profiles': total_profiles,
                    'profiles_with_new_content': len(profiles_with_new_content)
                }
                        
            except Exception as e:
                logger.error(f"Error in bulk Facebook refresh: {e}")
            finally:
                bulk_facebook_in_progress = False
        
        # Start background refresh
        threading.Thread(target=bulk_facebook_refresh, daemon=True).start()
                
    except Exception as e:
        logger.error(f"Error starting bulk Facebook refresh: {e}")
        flash('Error starting bulk Facebook refresh.', 'error')
    
    return redirect(url_for('dashboard'))


@app.route('/refresh_all_linkedin', methods=['GET'])
def refresh_all_linkedin():
    global bulk_linkedin_in_progress
    
    if bulk_linkedin_in_progress:
        flash('Bulk LinkedIn refresh is already in progress. Please wait for it to complete.', 'warning')
        return redirect(url_for('dashboard'))
    
    try:
        # Get all LinkedIn profiles across all advisors
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            c.execute('''SELECT DISTINCT advisor_id, page_url FROM website_snapshots 
                        WHERE page_url LIKE '%linkedin.com%'
                        ORDER BY advisor_id''')
            linkedin_profiles = c.fetchall()
            conn.close()
        
        if not linkedin_profiles:
            flash('No LinkedIn profiles found to refresh.', 'info')
            return redirect(url_for('dashboard'))
        
        def bulk_linkedin_refresh():
            global bulk_linkedin_in_progress
            try:
                bulk_linkedin_in_progress = True
                total_profiles = len(linkedin_profiles)
                profiles_with_new_content = []
                
                logger.info(f"=== BULK LINKEDIN REFRESH STARTED ===")
                logger.info(f"Found {total_profiles} LinkedIn profiles to refresh")
                
                for advisor_id, linkedin_url in linkedin_profiles:
                    try:
                        logger.info(f"Processing LinkedIn profile: {linkedin_url} (Advisor ID: {advisor_id})")
                        
                        # Get existing content
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                                     (advisor_id, linkedin_url))
                            existing_data = c.fetchone()
                            conn.close()
                        
                        existing_posts_text = existing_data[0] if existing_data else ""
                        
                        
                        new_posts = scrape_linkedin_refresh_only(linkedin_url, existing_posts_text)
                        
                        if new_posts:
                            logger.info(f"Found {len(new_posts)} NEW posts for {linkedin_url}")
                            profiles_with_new_content.append(linkedin_url)
                            
                            # Prepend new posts to existing content
                            new_post_content = ""
                            for i, post in enumerate(new_posts, 1):
                                post_text = post.get('text', '')
                                post_timestamp = post.get('timestamp', 'Unknown')
                                if post_text:
                                    clean_post_text = post_text.replace('... See more', '').replace('...See more', '').strip()
                                    new_post_content += f"Post NEW{i} ({post_timestamp}):\n{clean_post_text}\n\n"
                            
                            # Smart content combination
                            if existing_posts_text and "LINKEDIN PROFILE:" in existing_posts_text:
                                parts = existing_posts_text.split('\n\n', 1)
                                header = parts[0] if parts else f"LINKEDIN PROFILE: {linkedin_url}"
                                existing_content = parts[1] if len(parts) > 1 else ""
                                updated_content = f"{header}\n\n{new_post_content}{existing_content}"
                            else:
                                updated_content = f"LINKEDIN PROFILE: {linkedin_url}\n\n{new_post_content}{existing_posts_text}"
                            
                            content_hash = hashlib.md5(updated_content.encode()).hexdigest()
                            
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET content_text = ?, content_hash = ?, last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (updated_content, content_hash, advisor_id, linkedin_url))
                                conn.commit()
                                conn.close()
                        else:
                            # Still update timestamp
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (advisor_id, linkedin_url))
                                conn.commit()
                                conn.close()
                        
                        # Small delay between profiles
                        time.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"Error refreshing LinkedIn profile {linkedin_url}: {e}")
                        continue
                
                # Final summary
                logger.info(f"=== BULK LINKEDIN REFRESH COMPLETED ===")
                logger.info(f"{total_profiles} LinkedIn profiles scraped, {len(profiles_with_new_content)} profiles added with New Content")
                if profiles_with_new_content:
                    for profile_url in profiles_with_new_content:
                        logger.info(f"{profile_url}")

                app.config['BULK_LINKEDIN_COMPLETE'] = {
                    'platform': 'LinkedIn',
                    'total_profiles': total_profiles,
                    'profiles_with_new_content': len(profiles_with_new_content)
                }
                        
            except Exception as e:
                logger.error(f"Error in bulk LinkedIn refresh: {e}")
            finally:
                bulk_linkedin_in_progress = False
        
        # Start background refresh
        threading.Thread(target=bulk_linkedin_refresh, daemon=True).start()
                
    except Exception as e:
        logger.error(f"Error starting bulk LinkedIn refresh: {e}")
        flash('Error starting bulk LinkedIn refresh.', 'error')
    
    return redirect(url_for('dashboard'))


@app.route('/refresh_all_twitter', methods=['GET'])
def refresh_all_twitter():
    global bulk_twitter_in_progress
    
    if bulk_twitter_in_progress:
        flash('Bulk Twitter refresh is already in progress. Please wait for it to complete.', 'warning')
        return redirect(url_for('dashboard'))
    
    try:
        # Get all Twitter profiles across all advisors
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            c.execute('''SELECT DISTINCT advisor_id, page_url FROM website_snapshots 
                        WHERE page_url LIKE '%x.com%' OR page_url LIKE '%twitter.com%'
                        ORDER BY advisor_id''')
            twitter_profiles = c.fetchall()
            conn.close()
        
        if not twitter_profiles:
            flash('No Twitter profiles found to refresh.', 'info')
            return redirect(url_for('dashboard'))
        
        def bulk_twitter_refresh():
            global bulk_twitter_in_progress
            try:
                bulk_twitter_in_progress = True
                ensure_twitter_login() 
                total_profiles = len(twitter_profiles)
                profiles_with_new_content = []
                
                logger.info(f"=== BULK TWITTER REFRESH STARTED ===")
                logger.info(f"Found {total_profiles} Twitter profiles to refresh")
                
                for advisor_id, twitter_url in twitter_profiles:
                    try:
                        logger.info(f"Processing Twitter profile: {twitter_url} (Advisor ID: {advisor_id})")
                        
                        # Get existing content
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                                     (advisor_id, twitter_url))
                            existing_data = c.fetchone()
                            conn.close()
                        
                        existing_tweets_text = existing_data[0] if existing_data else ""
                        
                        # Use the incremental scraper
                        new_tweets = scrape_twitter_new_posts_only(twitter_url, existing_tweets_text)
                        
                        if new_tweets:
                            logger.info(f"Found {len(new_tweets)} NEW tweets for {twitter_url}")
                            profiles_with_new_content.append(twitter_url)
                            
                            # **PREPEND new tweets to existing content**
                            new_tweet_content = ""

                            # Find the highest existing NEW tweet number
                            import re
                            existing_new_numbers = []
                            if existing_tweets_text:
                                new_tweet_matches = re.findall(r'Tweet NEW(\d+)', existing_tweets_text)
                                existing_new_numbers = [int(num) for num in new_tweet_matches]

                            next_new_number = max(existing_new_numbers, default=0) + 1

                            for tweet in new_tweets:
                                tweet_text = tweet.get('text', '')
                                tweet_timestamp = tweet.get('timestamp', 'Unknown')
                                if tweet_text:
                                    new_tweet_content += f"Tweet NEW{next_new_number} ({tweet_timestamp}):\n{tweet_text}\n\n"
                                    next_new_number += 1
        
                            # Smart content combination
                            if existing_tweets_text and "TWITTER PROFILE:" in existing_tweets_text:
                                parts = existing_tweets_text.split('\n\n', 1)
                                header = parts[0] if parts else f"TWITTER PROFILE: {twitter_url}"
                                existing_content = parts[1] if len(parts) > 1 else ""
                                updated_content = f"{header}\n\n{new_tweet_content}{existing_content}"
                            else:
                                updated_content = f"TWITTER PROFILE: {twitter_url}\n\n{new_tweet_content}{existing_tweets_text}"
                            
                            content_hash = hashlib.md5(updated_content.encode()).hexdigest()
                            
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET content_text = ?, content_hash = ?, last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (updated_content, content_hash, advisor_id, twitter_url))
                                conn.commit()
                                conn.close()
                        else:
                            # Still update timestamp
                            with db_lock:
                                conn = get_db_connection_sqlite()
                                c = conn.cursor()
                                c.execute('''UPDATE website_snapshots 
                                           SET last_checked = CURRENT_TIMESTAMP 
                                           WHERE advisor_id = ? AND page_url = ?''',
                                         (advisor_id, twitter_url))
                                conn.commit()
                                conn.close()
                        
                        # Small delay between profiles
                        time.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"Error refreshing Twitter profile {twitter_url}: {e}")
                        continue
                
                # Final summary
                logger.info(f"=== BULK TWITTER REFRESH COMPLETED ===")
                logger.info(f"{total_profiles} Twitter profiles scraped, {len(profiles_with_new_content)} profiles added with New Content")
                if profiles_with_new_content:
                    for profile_url in profiles_with_new_content:
                        logger.info(f"{profile_url}")

                app.config['BULK_TWITTER_COMPLETE'] = {
                'platform': 'Twitter',
                'total_profiles': total_profiles,
                'profiles_with_new_content': len(profiles_with_new_content)
            }

            except Exception as e:
                logger.error(f"Error in bulk Twitter refresh: {e}")
            finally:
                bulk_twitter_in_progress = False
        
        # Start background refresh
        threading.Thread(target=bulk_twitter_refresh, daemon=True).start()
                
    except Exception as e:
        logger.error(f"Error starting bulk Twitter refresh: {e}")
        flash('Error starting bulk Twitter refresh.', 'error')
    
    return redirect(url_for('dashboard'))

@app.route('/delete_post', methods=['POST'])
def delete_post():
    conn = None
    try:
        data = request.get_json()
        advisor_id = data.get('advisor_id')
        page_url = data.get('page_url')
        post_id = data.get('post_id')
        post_type = data.get('post_type')
        
        if not all([advisor_id, page_url, post_id]):
            return jsonify({'success': False, 'error': 'Missing required data'})
        
        conn = get_db_connection_sqlite()
        cursor = conn.cursor()
        
        # Get the current content
        cursor.execute("""
            SELECT id, content_text FROM website_snapshots 
            WHERE advisor_id = ? AND page_url = ?
        """, (advisor_id, page_url))
        
        page_record = cursor.fetchone()
        
        if not page_record:
            return jsonify({'success': False, 'error': 'Page not found'})
        
        page_db_id, current_content = page_record
        
        if not current_content:
            return jsonify({'success': False, 'error': 'No content found'})
        
        # Determine post prefix
        if 'x.com' in page_url.lower() or 'twitter.com' in page_url.lower():
            post_prefix = "Tweet "
        elif 'facebook.com' in page_url.lower() or 'fb.com' in page_url.lower():
            post_prefix = "Post "
        elif 'linkedin.com' in page_url.lower():
            post_prefix = "Post "
        else:
            post_prefix = "Post "
        
        # Split content into sections
        sections = current_content.split(f'\n\n{post_prefix}')
        
        # Keep header and filter out the specific post
        updated_sections = [sections[0]]  # Keep header
        
        found_and_removed = False
        for section in sections[1:]:  # Skip header
            if section.strip():
                lines = section.strip().split('\n')
                # Clean the post ID from content the same way as frontend
                content_post_id = lines[0].strip().split('(')[0].strip().rstrip(':')
                if lines and content_post_id == post_id.strip():
                    # This is the post to delete - skip it
                    found_and_removed = True
                    continue
                else:
                    # Keep this post
                    updated_sections.append(f"{post_prefix}{section}")
        
        if not found_and_removed:
            return jsonify({'success': False, 'error': f'Post {post_id} not found'})
        
        # Reconstruct content
        updated_content = updated_sections[0]  # Start with header
        for section in updated_sections[1:]:   # Add remaining posts
            updated_content += f'\n\n{section}'
        
        # Update database
        cursor.execute("""
            UPDATE website_snapshots 
            SET content_text = ?, last_checked = ? 
            WHERE id = ?
        """, (updated_content, datetime.now(), page_db_id))
        
        # Also delete any related compliance issues for this specific post
        cursor.execute("""
            DELETE FROM compliance_issues 
            WHERE advisor_id = ? AND page_url = ?
        """, (advisor_id, page_url))
        
        conn.commit()
        
        return jsonify({
            'success': True, 
            'message': f'{post_type.capitalize()} deleted successfully'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})
    finally:
        if conn:
            conn.close()


@app.route('/add_manual_post', methods=['POST'])
def add_manual_post():
    conn = None
    try:
        data = request.get_json()
        advisor_id = data.get('advisor_id')
        domain = data.get('domain')
        post_content = data.get('post_content')
        post_date = data.get('post_date')
        
        if not all([advisor_id, domain, post_content, post_date]):
            return jsonify({'success': False, 'error': 'Missing required data'})
        
        # Use your existing database connection function
        conn = get_db_connection_sqlite()
        cursor = conn.cursor()
        
        # Get existing page from website_snapshots table using correct column names
        cursor.execute("""
            SELECT id, page_url, content_text FROM website_snapshots 
            WHERE advisor_id = ? AND page_url LIKE ? 
            ORDER BY id DESC LIMIT 1
        """, (advisor_id, f'%{domain}%'))
        
        existing_page = cursor.fetchone()
        
        if existing_page:
            page_id, page_url, existing_content = existing_page
            
            # Parse the date and create a timestamp-based ID for sorting
            from datetime import datetime
            import time
            
            try:
                post_datetime = datetime.fromisoformat(post_date.replace('T', ' '))
                post_timestamp = int(post_datetime.timestamp())
                post_id = f"{post_timestamp}_{int(time.time() * 1000) % 1000}"
            except:
                post_id = str(int(time.time()))
                post_datetime = datetime.now()
            
            formatted_date = post_datetime.strftime("%Y-%m-%d %H:%M")
            
            # Determine post format based on domain (without showing date in content)
            if 'x.com' in domain.lower() or 'twitter.com' in domain.lower():
                new_post = f"\n\nTweet {post_id}\n{post_content}"
            elif 'facebook.com' in domain.lower() or 'fb.com' in domain.lower():
                new_post = f"\n\nPost {post_id}\n{post_content}"
            elif 'linkedin.com' in domain.lower():
                new_post = f"\n\nPost {post_id}\n{post_content}"
            else:
                new_post = f"\n\nPost {post_id}\n{post_content}"
                
            # Sort posts by date if you want, or just append
            updated_content = sort_posts_by_date(existing_content, new_post, domain)
            
            # Update the website_snapshots table with correct column names
            cursor.execute("""
                UPDATE website_snapshots 
                SET content_text = ?, last_checked = ? 
                WHERE id = ?
            """, (updated_content, datetime.now(), page_id))
            
            conn.commit()
            
            return jsonify({
                'success': True, 
                'message': f'New post added and sorted by date for {domain}'
            })
        else:
            return jsonify({'success': False, 'error': f'No existing page found for domain {domain}'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})
    finally:
        if conn:
            conn.close()

def sort_posts_by_date(existing_content, new_post, domain):
    """Sort all posts by their date/timestamp"""
    try:
        # Determine post prefix based on domain
        if 'facebook.com' in domain.lower() or 'fb.com' in domain.lower():
            post_prefix = "Post "
        elif 'x.com' in domain.lower() or 'twitter.com' in domain.lower():
            post_prefix = "Tweet "
        elif 'linkedin.com' in domain.lower():
            post_prefix = "Post "
        else:
            post_prefix = "Post "
        
        # Split content into posts
        all_content = (existing_content or '') + new_post
        sections = all_content.split(f'\n\n{post_prefix}')
        
        # Keep the header (everything before first post)
        header = sections[0] if sections else ''
        
        # Parse posts with their timestamps
        posts_with_timestamps = []
        
        for i, section in enumerate(sections[1:], 1):  # Skip header
            if section.strip():
                lines = section.strip().split('\n')
                if lines:
                    post_id = lines[0].strip()
                    
                    # Extract timestamp from post_id (timestamp_random format)
                    try:
                        timestamp = int(post_id.split('_')[0])
                        posts_with_timestamps.append((timestamp, f"{post_prefix}{section}"))
                    except:
                        # Fallback for posts without proper timestamp format
                        posts_with_timestamps.append((0, f"{post_prefix}{section}"))
        
        # Sort posts by timestamp (newest first)
        posts_with_timestamps.sort(key=lambda x: x[0], reverse=True)
        
        # Reconstruct content
        sorted_content = header
        for _, post in posts_with_timestamps:
            sorted_content += f'\n\n{post}'
        
        return sorted_content
        
    except Exception as e:
        print(f"Error sorting posts: {e}")
        # Fallback: just append the new post
        return (existing_content or '') + new_post
    
def sort_posts_by_date(existing_content, new_post, domain):
    """Sort all posts by their date/timestamp"""
    try:
        # Determine post prefix based on domain
        if 'facebook.com' in domain.lower() or 'fb.com' in domain.lower():
            post_prefix = "Post "
        elif 'x.com' in domain.lower() or 'twitter.com' in domain.lower():
            post_prefix = "Tweet "
        elif 'linkedin.com' in domain.lower():
            post_prefix = "Post "
        else:
            post_prefix = "Post "
        
        # Split content into posts
        all_content = (existing_content or '') + new_post
        sections = all_content.split(f'\n\n{post_prefix}')
        
        # Keep the header (everything before first post)
        header = sections[0] if sections else ''
        
        # Parse posts with their timestamps
        posts_with_timestamps = []
        
        for i, section in enumerate(sections[1:], 1):  # Skip header
            if section.strip():
                lines = section.strip().split('\n')
                if lines:
                    post_id = lines[0].strip()
                    
                    # Extract timestamp from post_id (timestamp_random format)
                    try:
                        timestamp = int(post_id.split('_')[0])
                        posts_with_timestamps.append((timestamp, f"{post_prefix}{section}"))
                    except:
                        # Fallback for posts without proper timestamp format
                        posts_with_timestamps.append((0, f"{post_prefix}{section}"))
        
        # Sort posts by timestamp (newest first)
        posts_with_timestamps.sort(key=lambda x: x[0], reverse=True)
        
        # Reconstruct content
        sorted_content = header
        for _, post in posts_with_timestamps:
            sorted_content += f'\n\n{post}'
        
        return sorted_content
        
    except Exception as e:
        print(f"Error sorting posts: {e}")
        # Fallback: just append the new post
        return (existing_content or '') + new_post

@app.route('/system-status')
def system_status():
    """Check current system memory status"""
    try:
        # System memory
        system_memory = psutil.virtual_memory()
        
        # Process memory
        process = psutil.Process(os.getpid())
        process_memory = process.memory_info()
        
        # Chrome processes
        chrome_memory = 0
        chrome_count = 0
        for proc in psutil.process_iter(['name', 'memory_info']):
            try:
                if 'chrome' in proc.info['name'].lower():
                    chrome_memory += proc.info['memory_info'].rss / 1024 / 1024
                    chrome_count += 1
            except:
                pass
        
        return {
            'system': {
                'total_gb': round(system_memory.total / 1024 / 1024 / 1024, 2),
                'available_gb': round(system_memory.available / 1024 / 1024 / 1024, 2),
                'used_percent': system_memory.percent
            },
            'process': {
                'memory_mb': round(process_memory.rss / 1024 / 1024, 2),
                'memory_percent': process.memory_percent()
            },
            'chrome': {
                'processes': chrome_count,
                'total_memory_mb': round(chrome_memory, 2)
            },
            'recommendations': get_memory_recommendations(system_memory, chrome_memory)
        }
    except Exception as e:
        return {'error': str(e)}

def get_memory_recommendations(system_memory, chrome_memory_mb):
    """Get memory optimization recommendations"""
    recommendations = []
    
    available_gb = system_memory.available / 1024 / 1024 / 1024
    
    if available_gb < 2:
        recommendations.append("‚ö†Ô∏è Low system memory - consider closing other applications")
    
    if chrome_memory_mb > 500:
        recommendations.append("‚ö†Ô∏è Chrome using high memory - consider restarting browser processes")
    
    if system_memory.percent > 90:
        recommendations.append("üî¥ Critical memory usage - stop scraping immediately")
    elif system_memory.percent > 80:
        recommendations.append("üü° High memory usage - reduce scraping intensity")
    else:
        recommendations.append("‚úÖ Memory usage looks good")
    
    return recommendations


@app.template_filter('clean_post_id')
def clean_post_id(post_id):
    """Clean post ID for display"""
    if not post_id:
        return post_id
    
    # Remove "Feed post number " prefix and keep only the number
    clean_id = re.sub(r'^Feed post number ', '', post_id)
    clean_id = re.sub(r'\s.*$', '', clean_id)  # Remove everything after first space
    
    return clean_id

@app.template_filter('linkedin_header_filter')
def linkedin_header_filter(text):
    import re
    
    # Remove the header pattern with organization names
    # "Feed post number X [Organization Name] 1yr ‚Ä¢ 1 year ago ‚Ä¢ "
    text = re.sub(r'^Feed post number \d+\s+[^‚Ä¢\n]*?\s*\d+[a-z]*\s*‚Ä¢\s*\d+\s+(year|month|week|day|hour|minute)s?\s+ago\s*‚Ä¢\s*', '', text, flags=re.MULTILINE)
    
    # Alternative pattern for simpler cases
    # "Feed post number X Verified ‚Ä¢ You Xmo ‚Ä¢ X months ago ‚Ä¢ "
    text = re.sub(r'^Feed post number \d+\s+Verified\s*‚Ä¢\s*You\s+\d+[a-z]*\s*‚Ä¢\s*\d+\s+(months?|days?|years?|hours?|minutes?)\s+ago\s*‚Ä¢\s*', '', text, flags=re.MULTILINE)
    
    # Your existing patterns (keep these as backups)
    text = re.sub(r'^.*?\d+[a-z]+\s*‚Ä¢\s*.*?\n', '', text)
    text = re.sub(r'^.*?‚Ä¢\s*You.*?\n', '', text)
    text = re.sub(r'^.*?Personal Financial Consultant.*?\n', '', text)
    text = re.sub(r'^.*?CFP¬Æ.*?\n', '', text)
    # Remove any remaining header lines with ‚Ä¢ symbol
    text = re.sub(r'^.*?‚Ä¢.*?\n', '', text)
    # Remove "Only visible to members of..." pattern
    text = re.sub(r'Only visible to members of [^‚Ä¢\n]*‚Ä¢?\s*', '', text)
    
    # Remove footer elements
    text = re.sub(r'Auto captions have been added to your video.*?$', '', text)
    text = re.sub(r'Edit captions.*?$', '', text)
    text = re.sub(r'\d+\s+impressions.*?$', '', text)
    text = re.sub(r'View analytics.*?$', '', text)
    text = re.sub(r'Like Comment Repost Send.*?$', '', text)
    
    return text.strip()
@app.route('/update_page_title/<int:advisor_id>', methods=['POST'])
def update_page_title(advisor_id):
    """Update the display title of a specific page"""
    try:
        data = request.get_json()
        page_url = data.get('page_url')
        new_title = data.get('new_title', '').strip()
        
        if not page_url:
            return jsonify({'success': False, 'error': 'Page URL is required'}), 400
        
        if not new_title:
            return jsonify({'success': False, 'error': 'Title cannot be empty'}), 400
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Update the page title in website_snapshots (using correct column name)
            c.execute('''UPDATE website_snapshots 
                        SET page_title = ? 
                        WHERE advisor_id = ? AND page_url = ?''', 
                     (new_title, advisor_id, page_url))
            
            if c.rowcount == 0:
                conn.close()
                return jsonify({'success': False, 'error': 'Page not found'}), 404
            
            conn.commit()
            conn.close()
            
            logger.info(f"Updated title for page {page_url} (advisor {advisor_id}) to: {new_title}")
            
            return jsonify({
                'success': True, 
                'message': 'Title updated successfully'
            })
            
    except Exception as e:
        logger.error(f"Error updating page title: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500
    

@app.route('/delete_page/<int:advisor_id>', methods=['POST'])
def delete_page(advisor_id):
    """Delete a specific page from an advisor"""
    try:
        data = request.get_json()
        page_url = data.get('page_url')
        
        if not page_url:
            return jsonify({'success': False, 'error': 'Page URL is required'}), 400
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Verify advisor exists
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                conn.close()
                return jsonify({'success': False, 'error': 'Advisor not found'}), 404
            
            # Delete all snapshots for this specific page
            c.execute('''DELETE FROM website_snapshots 
                        WHERE advisor_id = ? AND page_url = ?''', 
                     (advisor_id, page_url))
            deleted_snapshots = c.rowcount
            
            # Delete all changes for this specific page
            c.execute('''DELETE FROM changes 
                        WHERE advisor_id = ? AND page_url = ?''', 
                     (advisor_id, page_url))
            deleted_changes = c.rowcount
            
            # Delete all compliance issues for this specific page
            c.execute('''DELETE FROM compliance_issues 
                        WHERE advisor_id = ? AND page_url = ?''', 
                     (advisor_id, page_url))
            deleted_issues = c.rowcount
            
            conn.commit()
            conn.close()
            
            if deleted_snapshots == 0 and deleted_changes == 0 and deleted_issues == 0:
                return jsonify({'success': False, 'error': 'Page not found'}), 404
            
            logger.info(f"Deleted page {page_url} for advisor {advisor_id}: "
                       f"{deleted_snapshots} snapshots, {deleted_changes} changes, {deleted_issues} issues")
            
            return jsonify({
                'success': True, 
                'message': f'Successfully deleted page',
                'deleted': {
                    'snapshots': deleted_snapshots,
                    'changes': deleted_changes,
                    'issues': deleted_issues
                },
                'deleted_urls': [page_url]
            })
            
    except Exception as e:
        logger.error(f"Error deleting page: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500
    
@app.template_global()
def get_website_favicon(website_url):
    """Template function to get favicon URL"""
    if not website_url:
        return None
    return extract_favicon(website_url)

@app.route('/facebook_post_compliance_issues/<int:advisor_id>')
def facebook_post_compliance_issues(advisor_id):
    """Display all compliance issues for a specific Facebook post"""
    page_url = request.args.get('page_url')
    post_id = request.args.get('post_id')

    print(f"DEBUG: advisor_id={advisor_id}, page_url={page_url}, post_id={post_id}")
    
    if not page_url or not post_id:
        print(f"DEBUG: Missing parameters - page_url={page_url}, post_id={post_id}")
        flash('Invalid Facebook post parameters', 'error')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Get advisor info
        c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        
        if not advisor:
            flash('Advisor not found', 'error')
            return redirect(url_for('dashboard'))
        
        # Get all compliance issues for this specific Facebook post
        c.execute('''SELECT * FROM compliance_issues 
                    WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?
                    ORDER BY detected_at DESC''', 
                 (advisor_id, page_url, f'%Post {post_id}%'))
        issues = c.fetchall()
        
        conn.close()
    
    page_title = f"Facebook Post {post_id}"
    
    return render_template('facebook_post_compliance_issues.html', 
                         advisor=advisor, 
                         issues=issues, 
                         page_url=page_url,
                         post_id=post_id,
                         page_title=page_title)
    
@app.route('/tweet_compliance_issues/<int:advisor_id>')
def tweet_compliance_issues(advisor_id):
    """Display all compliance issues for a specific tweet"""
    page_url = request.args.get('page_url')
    tweet_id = request.args.get('tweet_id')
    
    if not page_url or not tweet_id:
        flash('Invalid tweet parameters', 'error')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Get advisor info
        c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        
        if not advisor:
            flash('Advisor not found', 'error')
            return redirect(url_for('dashboard'))
        
        # Get all compliance issues for this specific tweet (same logic as domain_details)
        c.execute('''SELECT * FROM compliance_issues 
                    WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?
                    ORDER BY detected_at DESC''', 
                 (advisor_id, page_url, f'%Tweet {tweet_id}%'))
        issues = c.fetchall()
        
        conn.close()
    
    page_title = f"Tweet {tweet_id}"
    
    return render_template('tweet_compliance_issues.html', 
                         advisor=advisor, 
                         issues=issues, 
                         page_url=page_url,
                         tweet_id=tweet_id,
                         page_title=page_title)

@app.route('/facebook_compliance_issues/<int:advisor_id>')
def facebook_compliance_issues(advisor_id):
    """Display all compliance issues for a specific Facebook post"""
    page_url = request.args.get('page_url')
    post_id = request.args.get('post_id')
    
    if not page_url or not post_id:
        flash('Invalid Facebook post parameters', 'error')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Get advisor info
        c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        
        if not advisor:
            flash('Advisor not found', 'error')
            return redirect(url_for('dashboard'))
        
        # FIXED: More specific query to only match Facebook posts
        c.execute('''SELECT * FROM compliance_issues 
                    WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?
                    ORDER BY detected_at DESC''', 
                 (advisor_id, page_url, f'%Facebook Post {post_id}%'))  # Changed this line
        issues = c.fetchall()
        
        conn.close()
    
    page_title = f"Facebook Post {post_id}"
    
    return render_template('facebook_compliance_issues.html', 
                         advisor=advisor, 
                         issues=issues, 
                         page_url=page_url,
                         post_id=post_id,
                         page_title=page_title)

@app.route('/refresh_facebook/<int:advisor_id>/<domain>', methods=['GET'])
def refresh_facebook(advisor_id, domain):
    try:
        logger.info(f"=== DEBUGGING FACEBOOK REFRESH ===")
        logger.info(f"Advisor ID: {advisor_id}, Domain requested: {domain}")
        
        # Create a unique key for this advisor/domain combination
        refresh_key = f"{advisor_id}_{domain}"
        
        # Check if refresh is already in progress
        if refresh_key in facebook_refresh_in_progress and facebook_refresh_in_progress[refresh_key]:
            flash('Facebook refresh is already in progress for this profile. Please wait for it to complete.', 'warning')

            # PRESERVE specific_page parameter in redirect
            specific_page = request.args.get('specific_page')
            if specific_page:
                return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain, specific_page=specific_page))
            else:
                return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))
        
        # BETTER APPROACH: Check if there's a specific_page parameter to identify which Facebook profile
        specific_page = request.args.get('specific_page')
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            if specific_page:
                # If we have a specific page URL, use that
                c.execute('''SELECT page_url FROM website_snapshots 
                            WHERE advisor_id = ? AND page_url = ?''', (advisor_id, specific_page))
                facebook_result = c.fetchone()
                facebook_url = facebook_result[0] if facebook_result else None
                logger.info(f"Using specific page parameter: {specific_page}")
            else:
                # Get ALL Facebook URLs for this advisor
                c.execute('''SELECT page_url FROM website_snapshots 
                            WHERE advisor_id = ? 
                            AND (page_url LIKE '%facebook.com%' OR page_url LIKE '%fb.com%')
                            ORDER BY page_url''', (advisor_id,))
                all_facebook_urls = c.fetchall()
                
                logger.info(f"Found {len(all_facebook_urls)} Facebook URLs for advisor {advisor_id}:")
                for i, url in enumerate(all_facebook_urls):
                    logger.info(f"  {i+1}. {url[0]}")
                
                # TEMPORARY: Let user choose or default to BlueOxAdvisor if available
                facebook_url = None
                for url_tuple in all_facebook_urls:
                    url = url_tuple[0]
                    if 'BlueOxAdvisor' in url:
                        facebook_url = url
                        logger.info(f"Found BlueOxAdvisor profile: {url}")
                        break
                
                if not facebook_url and all_facebook_urls:
                    facebook_url = all_facebook_urls[0][0]
                    logger.info(f"No BlueOxAdvisor found, using first URL: {facebook_url}")
            
            conn.close()
            
            if not facebook_url:
                flash('No Facebook profile found for this advisor.', 'error')
                return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))
            
            logger.info(f"**FINAL FACEBOOK URL TO SCRAPE: {facebook_url}**")
        
        # Rest of the background function remains the same...
        def refresh_facebook_background():
            try:
                facebook_refresh_in_progress[refresh_key] = True
                
                with db_lock:
                    conn = get_db_connection_sqlite()
                    c = conn.cursor()
                    c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, facebook_url))
                    existing_data = c.fetchone()
                    conn.close()
                
                existing_posts_text = existing_data[0] if existing_data else ""
                
                logger.info(f"About to call INCREMENTAL scrape for URL: {facebook_url}")
                logger.info(f"Existing content length: {len(existing_posts_text)} characters")
                
                new_posts = scrape_facebook_new_posts_only(facebook_url, existing_posts_text)
                logger.info(f"RECEIVED from scraper: {type(new_posts)} with {len(new_posts) if new_posts else 0} posts")
                if new_posts:
                    logger.info(f"First post preview: {new_posts[0].get('text', '')[:50]}...")

                if new_posts:
                    logger.info(f"Found {len(new_posts)} NEW posts to add")
                    
                    new_post_content = ""
                    for i, post in enumerate(new_posts, 1):
                        post_text = post.get('text', '')
                        post_timestamp = post.get('timestamp', 'Unknown')
                        if post_text:
                            clean_post_text = post_text.replace('... See more', '').replace('...See more', '').strip()
                            new_post_content += f"Post NEW{i} ({post_timestamp}):\n{clean_post_text}\n\n"                    

                    if existing_posts_text and "FACEBOOK PROFILE:" in existing_posts_text:
                        parts = existing_posts_text.split('\n\n', 1)
                        header = parts[0] if parts else f"FACEBOOK PROFILE: {facebook_url}"
                        existing_content = parts[1] if len(parts) > 1 else ""
                        updated_content = f"{header}\n\n{new_post_content}{existing_content}"
                    else:
                        updated_content = f"FACEBOOK PROFILE: {facebook_url}\n\n{new_post_content}{existing_posts_text}"
                    
                    content_hash = hashlib.md5(updated_content.encode()).hexdigest()
                    
                    with db_lock:
                        conn = get_db_connection_sqlite()
                        c = conn.cursor()
                        c.execute('''UPDATE website_snapshots 
                                   SET content_text = ?, content_hash = ?, last_checked = CURRENT_TIMESTAMP 
                                   WHERE advisor_id = ? AND page_url = ?''',
                                 (updated_content, content_hash, advisor_id, facebook_url))
                        conn.commit()
                        conn.close()
                    
                    logger.info(f"Facebook refresh completed: {len(new_posts)} NEW posts added to {facebook_url}")
                    
                    for i, post in enumerate(new_posts, 1):
                        logger.info(f"NEW Post {i}: {post['text'][:100]}...")
                        
                else:
                    logger.info(f"No new posts found for {facebook_url}")
                    
                    with db_lock:
                        conn = get_db_connection_sqlite()
                        c = conn.cursor()
                        c.execute('''UPDATE website_snapshots 
                                   SET last_checked = CURRENT_TIMESTAMP 
                                   WHERE advisor_id = ? AND page_url = ?''',
                                 (advisor_id, facebook_url))
                        conn.commit()
                        conn.close()
                    
                    logger.info(f"No new posts, but updated last_checked timestamp for {facebook_url}")
                    
            except Exception as e:
                logger.error(f"Error in Facebook refresh background task: {e}")
                logger.error(f"Traceback: {traceback.format_exc()}")
            finally:
                facebook_refresh_in_progress[refresh_key] = False
        
        threading.Thread(target=refresh_facebook_background, daemon=True).start()
        logger.info(f'Facebook refresh started for {facebook_url}! This will only scrape NEW posts.')
        
    except Exception as e:
        logger.error(f"Error refreshing Facebook for {domain}: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        flash('Error starting Facebook refresh.', 'error')

    # FIX: Preserve the specific_page parameter in the final redirect
    specific_page = request.args.get('specific_page')
    if specific_page:
        return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain, specific_page=specific_page))
    else:
        return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))


@app.route('/refresh_twitter/<int:advisor_id>/<domain>', methods=['GET'])
def refresh_twitter(advisor_id, domain):
    try:
        logger.info(f"=== DEBUGGING TWITTER REFRESH ===")
        logger.info(f"Advisor ID: {advisor_id}, Domain requested: {domain}")
        
        # Now we know exactly which advisor we're dealing with
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get the specific Twitter URL for THIS advisor and domain
            c.execute('''SELECT page_url FROM website_snapshots 
                        WHERE advisor_id = ? 
                        AND (page_url LIKE '%x.com%' OR page_url LIKE '%twitter.com%')
                        AND page_url LIKE ?''', (advisor_id, f'%{domain}%'))
            twitter_result = c.fetchone()
            
            if not twitter_result:
                # If no exact match, get any Twitter URL for this specific advisor
                c.execute('''SELECT page_url FROM website_snapshots 
                            WHERE advisor_id = ? 
                            AND (page_url LIKE '%x.com%' OR page_url LIKE '%twitter.com%')''', (advisor_id,))
                twitter_result = c.fetchone()
            
            conn.close()
            
            if not twitter_result:
                flash('No Twitter profile found for this advisor.', 'error')
                return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))
            
            twitter_url = twitter_result[0]
            logger.info(f"**FINAL TWITTER URL TO SCRAPE: {twitter_url}**")
        
        # Background function using the new incremental scraper
        def refresh_twitter_background():
            try:
                # **GET EXISTING TWEETS FROM DATABASE FIRST**
                with db_lock:
                    conn = get_db_connection_sqlite()
                    c = conn.cursor()
                    c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, twitter_url))
                    existing_data = c.fetchone()
                    conn.close()
                
                existing_tweets_text = existing_data[0] if existing_data else ""
                
                logger.info(f"About to call INCREMENTAL scrape for URL: {twitter_url}")
                logger.info(f"Existing content length: {len(existing_tweets_text)} characters")
                
                # **USE THE NEW INCREMENTAL SCRAPER**
                new_tweets = scrape_twitter_new_posts_only(twitter_url, existing_tweets_text)
                
                if new_tweets:
                    logger.info(f"Found {len(new_tweets)} NEW tweets to add")
                    
                    # **PREPEND new tweets to existing content**
                    new_tweet_content = ""

                    # Find the highest existing NEW tweet number
                    import re
                    existing_new_numbers = []
                    if existing_tweets_text:
                        new_tweet_matches = re.findall(r'Tweet NEW(\d+)', existing_tweets_text)
                        existing_new_numbers = [int(num) for num in new_tweet_matches]

                    next_new_number = max(existing_new_numbers, default=0) + 1

                    for tweet in new_tweets:
                        tweet_text = tweet.get('text', '')
                        tweet_timestamp = tweet.get('timestamp', 'Unknown')
                        if tweet_text:
                            new_tweet_content += f"Tweet NEW{next_new_number} ({tweet_timestamp}):\n{tweet_text}\n\n"
                            next_new_number += 1
                            
                    # **SMART CONTENT COMBINATION**
                    if existing_tweets_text and "TWITTER PROFILE:" in existing_tweets_text:
                        # Extract the header and existing tweets
                        parts = existing_tweets_text.split('\n\n', 1)
                        header = parts[0] if parts else f"TWITTER PROFILE: {twitter_url}"
                        existing_content = parts[1] if len(parts) > 1 else ""
                        
                        # Combine: header + new tweets + existing tweets  
                        updated_content = f"{header}\n\n{new_tweet_content}{existing_content}"
                    else:
                        # First time or corrupted data - create fresh content
                        updated_content = f"TWITTER PROFILE: {twitter_url}\n\n{new_tweet_content}{existing_tweets_text}"
                    
                    content_hash = hashlib.md5(updated_content.encode()).hexdigest()
                    
                    with db_lock:
                        conn = get_db_connection_sqlite()
                        c = conn.cursor()
                        c.execute('''UPDATE website_snapshots 
                                   SET content_text = ?, content_hash = ?, last_checked = CURRENT_TIMESTAMP 
                                   WHERE advisor_id = ? AND page_url = ?''',
                                 (updated_content, content_hash, advisor_id, twitter_url))
                        conn.commit()
                        conn.close()
                    
                    logger.info(f"Twitter refresh completed: {len(new_tweets)} NEW tweets added to {twitter_url}")
                    
                    # Log what we added for debugging
                    for i, tweet in enumerate(new_tweets, 1):
                        logger.info(f"NEW Tweet {i}: {tweet['text'][:100]}...")
                        
                else:
                    logger.info(f"No new tweets found for {twitter_url}")
                    
                    # Still update the last_checked timestamp even if no new content
                    with db_lock:
                        conn = get_db_connection_sqlite()
                        c = conn.cursor()
                        c.execute('''UPDATE website_snapshots 
                                   SET last_checked = CURRENT_TIMESTAMP 
                                   WHERE advisor_id = ? AND page_url = ?''',
                                 (advisor_id, twitter_url))
                        conn.commit()
                        conn.close()
                    
                    logger.info(f"No new tweets, but updated last_checked timestamp for {twitter_url}")
                    
            except Exception as e:
                logger.error(f"Error in Twitter refresh background task: {e}")
                logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Start the actual incremental Twitter scraping in background
        threading.Thread(target=refresh_twitter_background, daemon=True).start()
        
        #flash(f'Twitter refresh started for {twitter_url}! Please complete the login process in the browser window that opened. This will only scrape NEW tweets.', 'info')
        
    except Exception as e:
        logger.error(f"Error refreshing Twitter for {domain}: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        flash('Error starting Twitter refresh.', 'error')
    
    return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))


@app.route('/refresh_linkedin/<int:advisor_id>/<domain>', methods=['GET'])
def refresh_linkedin(advisor_id, domain):
    try:
        logger.info(f"=== LINKEDIN REFRESH WITH NEW FUNCTION ===")
        logger.info(f"Advisor ID: {advisor_id}, Domain requested: {domain}")
        
        # Create a unique key for this advisor/domain combination
        refresh_key = f"linkedin_{advisor_id}_{domain}"
        
        # Check if refresh is already in progress
        if refresh_key in globals() and globals().get(f'{refresh_key}_in_progress', False):
            flash('LinkedIn refresh is already in progress for this profile. Please wait for it to complete.', 'warning')
            return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get the specific LinkedIn URL for THIS advisor and domain
            c.execute('''SELECT page_url FROM website_snapshots 
                        WHERE advisor_id = ? 
                        AND page_url LIKE '%linkedin.com%'
                        AND page_url LIKE ?''', (advisor_id, f'%{domain}%'))
            linkedin_result = c.fetchone()
            
            if not linkedin_result:
                # If no exact match, get any LinkedIn URL for this specific advisor
                c.execute('''SELECT page_url FROM website_snapshots 
                            WHERE advisor_id = ? 
                            AND page_url LIKE '%linkedin.com%' ''', (advisor_id,))
                linkedin_result = c.fetchone()
            
            conn.close()
            
            if not linkedin_result:
                flash('No LinkedIn profile found for this advisor.', 'error')
                return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))
            
            linkedin_url = linkedin_result[0]
            logger.info(f"**LINKEDIN URL TO REFRESH: {linkedin_url}**")
        
        # Background function using NEW refresh-only scraper
        def refresh_linkedin_background():
            try:
                # Mark refresh as in progress
                globals()[f'{refresh_key}_in_progress'] = True
                
                # Get existing posts from database
                with db_lock:
                    conn = get_db_connection_sqlite()
                    c = conn.cursor()
                    c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, linkedin_url))
                    existing_data = c.fetchone()
                    conn.close()
                
                existing_posts_text = existing_data[0] if existing_data else ""
                
                logger.info(f"About to call NEW refresh scraper for URL: {linkedin_url}")
                logger.info(f"Existing content length: {len(existing_posts_text)} characters")
                
                # **USE THE NEW REFRESH-ONLY SCRAPER**
                new_posts = scrape_linkedin_refresh_only(linkedin_url, existing_posts_text)

                logger.info(f"NEW refresh scraper returned: {type(new_posts)} with {len(new_posts) if new_posts else 0} posts")
                if new_posts:
                    logger.info(f"First new post preview: {new_posts[0].get('text', '')[:50]}...")
                
                if new_posts:
                    logger.info(f"Found {len(new_posts)} NEW posts to add")
                    
                    # Prepend new posts to existing content
                    new_post_content = ""
                    for i, post in enumerate(new_posts, 1):
                        post_text = post.get('text', '')
                        post_timestamp = post.get('timestamp', 'Unknown')
                        if post_text:
                            # Clean the post text
                            clean_post_text = post_text.replace('... See more', '').replace('...See more', '').strip()
                            new_post_content += f"Post NEW{i} ({post_timestamp}):\n{clean_post_text}\n\n"                    

                    # Smart content combination
                    if existing_posts_text and "LINKEDIN PROFILE:" in existing_posts_text:
                        # Extract the header and existing posts
                        parts = existing_posts_text.split('\n\n', 1)
                        header = parts[0] if parts else f"LINKEDIN PROFILE: {linkedin_url}"
                        existing_content = parts[1] if len(parts) > 1 else ""
                        
                        # Combine: header + new posts + existing posts  
                        updated_content = f"{header}\n\n{new_post_content}{existing_content}"
                    else:
                        # First time or corrupted data - create fresh content
                        updated_content = f"LINKEDIN PROFILE: {linkedin_url}\n\n{new_post_content}{existing_posts_text}"
                    
                    content_hash = hashlib.md5(updated_content.encode()).hexdigest()
                    
                    with db_lock:
                        conn = get_db_connection_sqlite()
                        c = conn.cursor()
                        c.execute('''UPDATE website_snapshots 
                                   SET content_text = ?, content_hash = ?, last_checked = CURRENT_TIMESTAMP 
                                   WHERE advisor_id = ? AND page_url = ?''',
                                 (updated_content, content_hash, advisor_id, linkedin_url))
                        conn.commit()
                        conn.close()
                    
                    logger.info(f"LinkedIn refresh completed: {len(new_posts)} NEW posts added to {linkedin_url}")
                    
                    # Log what we added for debugging
                    for i, post in enumerate(new_posts, 1):
                        logger.info(f"NEW LinkedIn Post {i}: {post['text'][:100]}...")
                        
                else:
                    logger.info(f"No new posts found for {linkedin_url} - refresh scraper stopped at existing content")
                    
                    # Still update the last_checked timestamp even if no new content
                    with db_lock:
                        conn = get_db_connection_sqlite()
                        c = conn.cursor()
                        c.execute('''UPDATE website_snapshots 
                                   SET last_checked = CURRENT_TIMESTAMP 
                                   WHERE advisor_id = ? AND page_url = ?''',
                                 (advisor_id, linkedin_url))
                        conn.commit()
                        conn.close()
                    
                    logger.info(f"No new posts, but updated last_checked timestamp for {linkedin_url}")
                    
            except Exception as e:
                logger.error(f"Error in LinkedIn refresh background task: {e}")
                logger.error(f"Traceback: {traceback.format_exc()}")
            finally:
                # Always mark refresh as complete
                globals()[f'{refresh_key}_in_progress'] = False
        
        # Start the refresh scraping in background
        threading.Thread(target=refresh_linkedin_background, daemon=True).start()
        
        #flash(f'LinkedIn refresh started for {linkedin_url}! This will only scrape NEW posts and stop at existing ones.', 'info')
        
    except Exception as e:
        logger.error(f"Error refreshing LinkedIn for {domain}: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        flash('Error starting LinkedIn refresh.', 'error')
    
    return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))


@app.route('/debug_facebook_posts')
def debug_facebook_posts():
    """Debug Facebook post parsing"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get Facebook content
            c.execute('SELECT content_text FROM website_snapshots WHERE page_url LIKE "%facebook.com%"')
            result = c.fetchone()
            
            if result:
                content_text = result[0]
                post_sections = content_text.split('Post ')
                
                debug_info = f"<h1>Facebook Posts Debug</h1><style>body{{font-family:monospace;}}</style>"
                debug_info += f"<p>Found {len(post_sections)} sections</p>"
                
                for i, section in enumerate(post_sections):
                    if section.strip():
                        lines = section.strip().split('\n')
                        if lines:
                            post_id = lines[0].strip()
                            debug_info += f"<h3>Section {i}: Post ID = '{post_id}'</h3>"
                            debug_info += f"<pre>{section[:200]}...</pre><hr>"
                
                conn.close()
                return debug_info
            else:
                return "No Facebook posts found"
                
    except Exception as e:
        return f"Error: {str(e)}"
    
@app.route('/get_twitter_status/<int:advisor_id>')
def get_twitter_status(advisor_id):
    """Check if Twitter scraping is complete for an advisor"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check for any Twitter/X.com pages for this advisor
            c.execute('''SELECT page_url, page_title, last_checked 
                        FROM website_snapshots 
                        WHERE advisor_id = ? AND (page_url LIKE '%x.com%' OR page_url LIKE '%twitter.com%')
                        ORDER BY last_checked DESC''', (advisor_id,))
            twitter_pages = c.fetchall()
            
            conn.close()
            
            return jsonify({
                'has_twitter': len(twitter_pages) > 0,
                'twitter_count': len(twitter_pages),
                'latest_update': twitter_pages[0][2] if twitter_pages else None
            })
            
    except Exception as e:
        logger.error(f"Error checking Twitter status: {e}")
        return jsonify({'has_twitter': False, 'twitter_count': 0})

@app.route('/scan_advisor_domain', methods=['POST'])
def scan_advisor_domain():
    """Scan all pages for a specific advisor's domain"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data received'}), 400
            
        advisor_id = data.get('advisor_id')
        domain = data.get('domain')
        
        logger.info(f"Domain scan request - Advisor ID: {advisor_id}, Domain: {domain}")
        
        if not advisor_id or not domain:
            return jsonify({'success': False, 'error': 'Missing advisor_id or domain'})
        
        # Get current timestamp for scan tracking
        scan_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check if advisor exists
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                logger.error(f"Advisor {advisor_id} not found in database")
                conn.close()
                return jsonify({'success': False, 'error': f'Advisor {advisor_id} not found'})
            
            # Get all pages for this specific domain
            c.execute('''SELECT page_url, content_text, content_hash 
                        FROM website_snapshots 
                        WHERE advisor_id = ? AND page_url LIKE ?''', 
                     (advisor_id, f'%{domain}%'))
            domain_pages = c.fetchall()
            
            if not domain_pages:
                conn.close()
                return jsonify({'success': False, 'error': f'No pages found for domain {domain}'})
            
            conn.close()
        
        # Initialize the website monitor
        monitor = CarouselAwareMonitor()
        pages_scanned = 0
        changes_detected = 0
        
        # Track pages that return "Just a moment..." during the main scan
        just_a_moment_pages = []
        # Track all page results for final change detection
        all_page_results = {}
        
        logger.info(f"Found {len(domain_pages)} pages to scan for domain {domain}")
        
        # PHASE 1: Scan each page in the domain (NO CHANGE DETECTION YET)
        for page_url, old_content, old_hash in domain_pages:
            try:
                logger.info(f"Scanning page: {page_url}")
                
                # Store the original data for later comparison
                all_page_results[page_url] = {
                    'old_content': old_content,
                    'old_hash': old_hash,
                    'new_content': None,
                    'new_hash': None,
                    'title': None
                }
                
                # Scrape the current page content
                if monitor.likely_has_carousel(page_url):
                    logger.info(f"Detected carousel page during scan, using enhanced scraping: {page_url}")
                    soup, carousel_data = monitor.scrape_carousel_content(page_url)
                    if soup is None:
                        logger.warning(f"Could not scrape carousel page: {page_url}")
                        continue
                else:
                    # Use regular scraping for non-carousel pages
                    soup = monitor.scrape_page(page_url)
                    carousel_data = []

                if soup:
                    # Extract regular content
                    content = monitor.extract_content(soup)
                    
                    # Combine regular content with carousel content
                    if carousel_data:
                        carousel_text = extract_text_from_carousel_data(carousel_data)
                        content['text'] += f"\n\nCARROUSEL CONTENT:\n{carousel_text}"

                    new_full_content = content['text']
                    new_content_hash = hashlib.md5(new_full_content.encode()).hexdigest()
                    
                    # Store the results for later processing
                    all_page_results[page_url].update({
                        'new_content': new_full_content,
                        'new_hash': new_content_hash,
                        'title': content['title']
                    })
                    
                    pages_scanned += 1
                    
                    # Check if this page returned "Just a moment..." content during THIS scan
                    page_title = content.get('title', '')
                    has_just_a_moment_title = page_title and "just a moment" in page_title.lower()
                    has_verification_content = new_full_content and any(phrase in new_full_content.lower() for phrase in [
                        "just a moment", 
                        "verifying you are human",
                        "review the security of your connection",
                        "verification successful",
                        "waiting for",
                        "to respond"
                    ])
                    
                    # If this page shows "Just a moment..." content, add it to our rescan list
                    if has_just_a_moment_title or has_verification_content:
                        logger.info(f"üö´ Found 'Just a moment...' page during scan: {page_url}")
                        logger.info(f"   Title: {page_title}")
                        logger.info(f"   Content preview: {new_full_content[:200]}...")
                        just_a_moment_pages.append(page_url)
                
                # Small delay to avoid overwhelming the server
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"Error scanning page {page_url}: {str(e)}")
                continue
        
        # PHASE 2: AUTO-RESCAN PAGES THAT RETURNED "JUST A MOMENT..." DURING THE MAIN SCAN
        additional_rescans = 0
        if just_a_moment_pages:
            logger.info(f"üéØ Found {len(just_a_moment_pages)} 'Just a moment...' pages during scan - auto-rescanning...")
    
            for i, page_url in enumerate(just_a_moment_pages, 1):
                try:
                    logger.info(f"üîÑ Auto-rescanning blocked page {i}/{len(just_a_moment_pages)}: {page_url}")
            
                    # Add delay between rescans
                    if i > 1:
                        delay_time = random.uniform(5, 8)
                        logger.info(f"‚è≥ Waiting {delay_time:.1f} seconds to avoid rate limiting...")
                        time.sleep(delay_time)
            
                    # Scrape the page again with fresh session
                    soup = monitor.scrape_page(page_url)
            
                    if soup:
                        content = monitor.extract_content(soup)
                        new_full_content = content['text']
                        new_content_hash = hashlib.md5(new_full_content.encode()).hexdigest()
                        
                        # Get the old blocked data for logging
                        old_blocked_title = all_page_results[page_url]['title']
                        old_blocked_hash = all_page_results[page_url]['new_hash']
                        
                        # Check if we got different (actual) content this time
                        if new_content_hash != old_blocked_hash:
                            logger.info(f"‚úÖ Successfully rescanned: {page_url}")
                            logger.info(f"   Old title: {old_blocked_title}")
                            logger.info(f"   New title: {content['title']}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Rescan returned same blocked content: {page_url}")
                
                        # UPDATE the page results with the real content
                        all_page_results[page_url].update({
                            'new_content': new_full_content,
                            'new_hash': new_content_hash,
                            'title': content['title']
                        })
                
                        additional_rescans += 1
                    else:
                        logger.warning(f"‚ùå Failed to rescan blocked page: {page_url}")
            
                except Exception as e:
                    logger.error(f"Error auto-rescanning blocked page {page_url}: {str(e)}")
                    continue
        else:
            logger.info(f"‚úÖ No 'Just a moment...' pages found during domain scan of {domain}")

        # PHASE 3: NOW DO CHANGE DETECTION AND DATABASE UPDATES WITH FINAL CONTENT
        logger.info(f"üîç PHASE 3: Processing change detection with final content...")
        
        for page_url, page_data in all_page_results.items():
            if page_data['new_content'] is None:
                continue  # Skip pages that failed to scrape
                
            old_content = page_data['old_content']
            old_hash = page_data['old_hash']
            new_content = page_data['new_content']
            new_hash = page_data['new_hash']
            title = page_data['title']
            
            try:
                # Check if content has changed
                if old_hash != new_hash:
                    # NORMALIZE BOTH CONTENTS FOR ACCURATE COMPARISON
                    normalized_old_content = normalize_content_for_comparison(old_content)
                    normalized_new_content = normalize_content_for_comparison(new_content)
                    
                    # Compare normalized content to see if there's a real change
                    if normalized_old_content != normalized_new_content:
                        logger.info(f"Real content change detected on {page_url}")
                        
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            
                            # Find what changed using normalized content
                            changed_words = extract_changed_words(normalized_old_content, normalized_new_content)
                            
                            # Determine change type
                            if len(normalized_new_content) > len(normalized_old_content):
                                change_desc = "New content added to page"
                            elif len(normalized_new_content) < len(normalized_old_content):
                                change_desc = "Content removed from page"
                            else:
                                change_desc = "Page content modified"
                            
                            # Log the change with normalized content (this will show the REAL content in the After section)
                            c.execute('''INSERT INTO changes 
                                        (advisor_id, change_type, before_content, after_content, description, page_url, changed_words)
                                        VALUES (?, ?, ?, ?, ?, ?, ?)''',
                                     (advisor_id, 'update', normalized_old_content, normalized_new_content, change_desc, page_url, changed_words))
                            
                            conn.commit()
                            conn.close()
                            
                            changes_detected += 1
                            logger.info(f"Updated page content and logged change for {page_url}")
                    else:
                        logger.info(f"Content change was only formatting (title removal), no real change on {page_url}")
                
                # Always update the snapshot with the final content
                with db_lock:
                    conn = get_db_connection_sqlite()
                    c = conn.cursor()
                    
                    c.execute('''UPDATE website_snapshots 
                               SET content_hash = ?, content_text = ?, page_title = ?, last_checked = ?, last_scan_checked = ? 
                               WHERE advisor_id = ? AND page_url = ?''',
                             (new_hash, new_content, title, scan_timestamp, scan_timestamp, advisor_id, page_url))
                    
                    conn.commit()
                    conn.close()
                    
            except Exception as e:
                logger.error(f"Error processing final content for {page_url}: {str(e)}")
                continue

        logger.info(f"üéØ Domain scan complete: Rescanned {additional_rescans}/{len(just_a_moment_pages)} blocked pages")        

        # Create response message
        if changes_detected > 0:
            message = f"Domain scan completed for {domain}. {changes_detected} changes detected across {pages_scanned} pages. Auto-rescanned {additional_rescans} 'Just a moment...' pages."
        else:
            message = f"Domain scan completed for {domain}. No changes detected across {pages_scanned} pages. Auto-rescanned {additional_rescans} 'Just a moment...' pages."
        
        logger.info(f"Completed domain scan for {domain}: {pages_scanned} pages scanned, {changes_detected} changes detected, {additional_rescans} pages auto-rescanned")
        
        return jsonify({
            'success': True, 
            'message': message,
            'pages_scanned': pages_scanned,
            'changes_detected': changes_detected,
            'auto_rescanned': additional_rescans,
            'domain': domain
        })
        
    except Exception as e:
        logger.error(f"Error in scan_advisor_domain: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': f'An error occurred: {str(e)}'})
    
@app.route('/scan_single_page', methods=['POST'])
def scan_single_page():
    """Scan a single page for content changes"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data received'}), 400
            
        advisor_id = data.get('advisor_id')
        page_url = data.get('page_url')
        tweet_id = data.get('tweet_id')
        
        logger.info(f"Single page scan request - Advisor ID: {advisor_id}, Page: {page_url}")
        
        if not advisor_id or not page_url:
            return jsonify({'success': False, 'error': 'Missing advisor_id or page_url'})
        
        # Get current timestamp for scan tracking
        scan_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check if advisor exists
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                logger.error(f"Advisor {advisor_id} not found in database")
                conn.close()
                return jsonify({'success': False, 'error': f'Advisor {advisor_id} not found'})
            
            # Get current page data
            c.execute('SELECT content_text, content_hash FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                     (advisor_id, page_url))
            current_data = c.fetchone()
            
            if not current_data:
                logger.error(f"Page {page_url} not found for advisor {advisor_id}")
                conn.close()
                return jsonify({'success': False, 'error': f'Page {page_url} not found'})
            
            old_content, old_hash = current_data
            
            # Scrape the page to get new content
            monitor = CarouselAwareMonitor()
            soup = monitor.scrape_page(page_url)
            
            if not soup:
                # Even if scraping fails, update the scan timestamp
                c.execute('''UPDATE website_snapshots 
                           SET last_scan_checked = ? 
                           WHERE advisor_id = ? AND page_url = ?''',
                         (scan_timestamp, advisor_id, page_url))
                conn.commit()
                conn.close()
                return jsonify({'success': False, 'error': 'Could not access or scrape the page'})
            
            # Extract new content
            content = monitor.extract_content(soup)
            new_full_content = content['text']  # Just the filtered text content


            # ADD THIS SECTION - Filter to specific tweet if tweet_id provided
            if tweet_id:
                # Extract only the specific tweet content
                tweet_sections = new_full_content.split('Tweet ')
                specific_tweet_content = None
            
                for section in tweet_sections:
                    if section.strip().startswith(tweet_id):
                        # Get the content for this specific tweet
                        lines = section.strip().split('\n')
                        tweet_text = []
                        for line in lines[1:]:  # Skip the first line (tweet ID)
                            if line.strip():
                                tweet_text.append(line.strip())
                        specific_tweet_content = '\n'.join(tweet_text)
                        break
            
                if specific_tweet_content:
                    new_full_content = specific_tweet_content
                    logger.info(f"Processing specific tweet {tweet_id}: {len(new_full_content)} characters")
                else:
                    return jsonify({'success': False, 'error': f'Tweet {tweet_id} not found'})


            new_content_hash = hashlib.md5(new_full_content.encode()).hexdigest()
            
            changes_detected = 0
            content_updated = False
            
            # Check if content has changed
            if old_hash != new_content_hash:
                # NORMALIZE BOTH CONTENTS FOR ACCURATE COMPARISON
                normalized_old_content = normalize_content_for_comparison(old_content)
                normalized_new_content = normalize_content_for_comparison(new_full_content)
                
                # Compare normalized content to see if there's a real change
                if normalized_old_content != normalized_new_content:
                    logger.info(f"Real content change detected on {page_url}")
                    
                    # Find what changed using normalized content
                    changed_words = extract_changed_words(normalized_old_content, normalized_new_content)
                    
                    # Determine change type
                    if len(normalized_new_content) > len(normalized_old_content):
                        change_desc = "New content added to page"
                    elif len(normalized_new_content) < len(normalized_old_content):
                        change_desc = "Content removed from page"
                    else:
                        change_desc = "Page content modified"
                    
                    # STORE NORMALIZED CONTENT IN CHANGES TABLE
                    c.execute('''INSERT INTO changes 
                                (advisor_id, change_type, before_content, after_content, description, page_url, changed_words)
                                VALUES (?, ?, ?, ?, ?, ?, ?)''',
                             (advisor_id, 'update', normalized_old_content, normalized_new_content, change_desc, page_url, changed_words))
                    
                    changes_detected = 1
                    content_updated = True
                    
                    logger.info(f"Updated page content and logged change for {page_url}")
                else:
                    logger.info(f"Content change was only formatting (title removal), no real change on {page_url}")
                
                # Always update the snapshot to the new format regardless
                c.execute('''UPDATE website_snapshots 
                           SET content_hash = ?, content_text = ?, page_title = ?, last_checked = ?, last_scan_checked = ? 
                           WHERE advisor_id = ? AND page_url = ?''',
                         (new_content_hash, new_full_content, content['title'], scan_timestamp, scan_timestamp, advisor_id, page_url))
                
                content_updated = True
                
            else:
                # No content change, just update the timestamps
                c.execute('''UPDATE website_snapshots 
                           SET last_checked = ?, last_scan_checked = ? 
                           WHERE advisor_id = ? AND page_url = ?''',
                         (scan_timestamp, scan_timestamp, advisor_id, page_url))
                
                logger.info(f"No content changes detected on {page_url}, updated scan timestamp")
            
            conn.commit()
            conn.close()
        
        # Create response message
        if changes_detected > 0:
            message = f"Page scan completed. {changes_detected} change detected."
        else:
            message = "Page scan completed. No changes detected."
        
        return jsonify({
            'success': True, 
            'message': message,
            'changes_detected': changes_detected,
            'content_updated': content_updated
        })
        
    except Exception as e:
        logger.error(f"Error in scan_single_page: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': f'An error occurred: {str(e)}'})
    
@app.route('/fix_compliance_column')
def fix_compliance_column():
    """Manually add the compliance_checked_at column"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Add the column
            c.execute('ALTER TABLE website_snapshots ADD COLUMN compliance_checked_at TIMESTAMP')
            
            conn.commit()
            conn.close()
        
        return "‚úÖ Added compliance_checked_at column successfully! <a href='/advisor-dashboard'>Back to Dashboard</a>"
        
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

@app.route('/debug_compliance/<int:advisor_id>')
def debug_compliance(advisor_id):
    """Debug compliance check timestamps"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check the actual table structure
            c.execute("PRAGMA table_info(website_snapshots)")
            columns = c.fetchall()
            
            # Get all snapshots for this advisor
            c.execute('SELECT page_url, last_checked, compliance_checked_at FROM website_snapshots WHERE advisor_id = ?', (advisor_id,))
            snapshots = c.fetchall()
            
            conn.close()
        
        debug_info = f"""
        <h3>Debug Info for Advisor {advisor_id}</h3>
        <h4>Table Structure:</h4>
        <ul>
        """
        
        for col in columns:
            debug_info += f"<li>{col[1]} ({col[2]})</li>"
        
        debug_info += "</ul><h4>Page Data:</h4><ul>"
        
        for snapshot in snapshots:
            debug_info += f"<li><strong>{snapshot[0]}</strong><br>"
            debug_info += f"Last Checked: {snapshot[1]}<br>"
            debug_info += f"Compliance Checked: {snapshot[2]}</li><br>"
        
        debug_info += "</ul>"
        debug_info += f'<a href="/advisor_details/{advisor_id}">Back to Advisor</a>'
        
        return debug_info
        
    except Exception as e:
        return f"Debug error: {str(e)}"

from flask import request, jsonify
import requests, os

#ACCESS_TOKEN = os.environ.get("FB_ACCESS_TOKEN")  # store securely

@app.route('/add_site_link/<int:advisor_id>/<path:domain>', methods=['POST'])
def add_site_link(advisor_id, domain):
    """Add a new page link to a specific domain (Postgres). Keeps existing behavior; adds YouTube import."""
    from urllib.parse import urlparse, unquote
    import hashlib, datetime as dt

    try:
        data = request.get_json() or {}
        page_url = (data.get('page_url') or '').strip()
        page_title = (data.get('page_title') or '').strip()

        if not page_url:
            return jsonify({'success': False, 'error': 'Page URL is required'})

        # Ensure URL has protocol (kept)
        if not page_url.startswith(('http://', 'https://')):
            page_url = 'https://' + page_url

        # Validate that the URL belongs to the specified domain (kept; now robust to www.)
        raw_domain = unquote(domain).lower()
        expect_host = (urlparse(raw_domain).netloc or raw_domain).lower()
        expect_host = expect_host.replace('www.', '')
        parsed = urlparse(page_url)
        host = (parsed.netloc or '').lower().replace('www.', '')
        if host != expect_host:
            return jsonify({'success': False, 'error': f'URL must belong to domain {domain}'})

        # ‚Üí Postgres (no db_lock)
        conn = get_db_connection()
        cur = conn.cursor()

        # Check advisor exists (kept)
        cur.execute('SELECT 1 FROM advisors WHERE id = %s', (advisor_id,))
        if not cur.fetchone():
            release_db_connection(conn)
            return jsonify({'success': False, 'error': 'Advisor not found'})

        # Check if this page already exists (kept)
        cur.execute('SELECT 1 FROM website_snapshots WHERE advisor_id = %s AND page_url = %s',
                    (advisor_id, page_url))
        if cur.fetchone():
            release_db_connection(conn)
            return jsonify({'success': False, 'error': 'This page already exists'})

        # === YouTube special-case: fetch videos now and store post-style content ===
        if host == 'youtu.be' or host.endswith('youtube.com'):
            videos = fetch_youtube_videos(page_url, max_items=25)  # helper from 1A
            if videos:
                content_text = build_youtube_content_text(videos)   # helper from 1A
                content_hash = hashlib.sha256(content_text.encode()).hexdigest()
                actual_title = page_title or 'YouTube Channel'
            else:
                # still allow adding, but with a friendly placeholder (keeps your UX)
                content_text = f"YouTube page: {page_url}\n\n[No videos found or channel could not be resolved]"
                content_hash = hashlib.md5(content_text.encode()).hexdigest()
                actual_title = page_title or 'YouTube'

            now = dt.datetime.utcnow()

            # Upsert if you have a unique index on (advisor_id, page_url); fallback otherwise
            try:
                cur.execute("""
                    INSERT INTO website_snapshots
                        (advisor_id, page_url, page_title, content_hash, content_text, last_checked, last_scan_checked)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (advisor_id, page_url) DO UPDATE
                       SET page_title = COALESCE(EXCLUDED.page_title, website_snapshots.page_title),
                           content_hash = EXCLUDED.content_hash,
                           content_text = EXCLUDED.content_text,
                           last_scan_checked = EXCLUDED.last_scan_checked
                """, (advisor_id, page_url, actual_title, content_hash, content_text, now, now))
            except Exception:
                # fallback path if the unique constraint doesn't exist
                cur.execute("""
                    UPDATE website_snapshots
                       SET page_title = COALESCE(%s, page_title),
                           content_hash = %s,
                           content_text = %s,
                           last_scan_checked = %s
                     WHERE advisor_id = %s AND page_url = %s
                """, (actual_title, content_hash, content_text, now, advisor_id, page_url))
                if cur.rowcount == 0:
                    cur.execute("""
                        INSERT INTO website_snapshots
                            (advisor_id, page_url, page_title, content_hash, content_text, last_checked, last_scan_checked)
                        VALUES (%s, %s, %s, %s, %s, %s, %s)
                    """, (advisor_id, page_url, actual_title, content_hash, content_text, now, now))

            conn.commit()
            release_db_connection(conn)
            return jsonify({'success': True, 'message': f'Successfully added {actual_title}', 'page_title': actual_title})

        # === Default behavior (kept): placeholder; scanners will populate later ===
        page_content = f"Manual page: {page_url}\n\n[Content will be populated when scanned]"
        actual_title = page_title or 'Manual Page'
        content_hash = hashlib.md5(page_content.encode()).hexdigest()

        cur.execute("""
            INSERT INTO website_snapshots (advisor_id, page_url, content_hash, page_title, content_text, last_checked)
            VALUES (%s, %s, %s, %s, %s, NOW())
            ON CONFLICT (advisor_id, page_url) DO NOTHING
        """, (advisor_id, page_url, content_hash, actual_title, page_content))

        conn.commit()
        release_db_connection(conn)
        return jsonify({'success': True, 'message': f'Successfully added {actual_title}', 'page_title': actual_title})

    except Exception as e:
        logger.error(f"Error adding site link: {e}", exc_info=True)
        try:
            release_db_connection(conn)
        except Exception:
            pass
        return jsonify({'success': False, 'error': str(e)})

@app.route('/refresh_youtube/<int:advisor_id>/<path:domain>')
def refresh_youtube(advisor_id, domain):
    from urllib.parse import urlparse

    conn = None
    cur = None
    updated = 0
    try:
        conn = get_db_connection()
        cur = conn.cursor()

        # Find all YouTube snapshots for this advisor/domain
        cur.execute("""
            SELECT page_url
              FROM website_snapshots
             WHERE advisor_id = %s
               AND (position('youtube.com' in lower(page_url)) > 0
                    OR position('youtu.be' in lower(page_url)) > 0)
        """, (advisor_id,))
        rows = cur.fetchall()

        for (page_url,) in rows:
            vids = fetch_youtube_videos(page_url, 25)
            if not vids:
                continue
            content_text = build_youtube_content_text(vids)
            content_hash = hashlib.sha256(content_text.encode('utf-8')).hexdigest()
            cur.execute("""
                UPDATE website_snapshots
                   SET content_text = %s,
                       content_hash = %s,
                       last_scan_checked = NOW()
                 WHERE advisor_id = %s AND page_url = %s
            """, (content_text, content_hash, advisor_id, page_url))
            updated += cur.rowcount

        conn.commit()
        flash(f"Refreshed YouTube pages. Updated: {updated}", "success")
    except Exception as e:
        if conn: conn.rollback()
        app.logger.error("refresh_youtube error: %s", e, exc_info=True)
        flash("Error refreshing YouTube", "error")
    finally:
        try:
            if cur: cur.close()
        except Exception:
            pass
        try:
            if conn: release_db_connection(conn)
        except Exception:
            pass

    # redirect back to domain details
    return redirect(url_for('domain_details', advisor_id=advisor_id, domain=domain))


@app.route('/add-profile-to-advisor', methods=['POST'])
def add_profile_to_advisor():
    conn = None
    try:
        data = request.get_json()
        advisor_id = data.get('advisor_id')
        platform = data.get('platform')
        profile_url = data.get('profile_url')
        profile_name = data.get('profile_name')
        
        if not all([advisor_id, platform, profile_url]):
            return jsonify({'success': False, 'error': 'Missing required data'})
        
        conn = get_db_connection_sqlite()
        cursor = conn.cursor()
        
        # Insert the new profile into website_snapshots table so it shows up in future scans
        cursor.execute("""
            INSERT INTO website_snapshots (
                advisor_id, 
                page_url, 
                content_text, 
                page_title, 
                last_checked, 
                manually_added
            )
            VALUES (?, ?, ?, ?, ?, 1)
        """, (
            advisor_id,
            profile_url,
            f"Manually added {platform} profile: {profile_name}",
            f"{profile_name} - {platform.title()}",
            datetime.now(),
        ))
        
        conn.commit()
        
        return jsonify({
            'success': True,
            'message': f'Profile {profile_url} added to advisor {advisor_id}'
        })
        
    except Exception as e:
        print(f"Error adding profile to advisor: {e}")
        return jsonify({'success': False, 'error': str(e)})
    finally:
        if conn:
            conn.close()

@app.route('/add_site/<int:advisor_id>', methods=['POST'])
def add_site(advisor_id):
    """Add a new site to an existing advisor.
       Keeps existing Twitter/Facebook/LinkedIn flows, adds YouTube import, uses Postgres."""
    import threading, hashlib
    from urllib.parse import urlparse

    try:
        page_url   = (request.form.get('page_url')  or '').strip()
        page_title = (request.form.get('page_title') or '').strip()
        logger.info(f"üî• ADD_SITE received url={page_url}, title={page_title}")

        if not page_url:
            return render_template('add_link.html',
                                   advisor_id=advisor_id,
                                   error='Page URL is required')

        # Ensure protocol
        if not page_url.startswith(('http://', 'https://')):
            page_url = 'https://' + page_url

        lower = page_url.lower()

        # ADD THIS DEBUG LOGGING
        logger.info(f"üîç DEBUG: lower = '{lower}'")
        logger.info(f"üîç DEBUG: 'x.com' in lower = {'x.com' in lower}")
        logger.info(f"üîç DEBUG: 'twitter.com' in lower = {'twitter.com' in lower}")


        # === Existing special-cases ===
        if 'x.com' in lower or 'twitter.com' in lower:
            logger.info(f"üê¶ TWITTER DETECTED! Calling handle_twitter_profile")
            # UPDATE ADVISOR_PROFILES STATUS TO MONITORED
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('''
                    UPDATE advisor_profiles 
                    SET profiles_data = (
                        SELECT jsonb_agg(
                            CASE 
                                WHEN elem->>'url' = %s 
                                THEN jsonb_set(elem, '{status}', '"monitored"')
                                ELSE elem
                            END
                        )
                        FROM jsonb_array_elements(profiles_data) AS elem
                    )
                    WHERE advisor_id = %s AND profiles_data IS NOT NULL
                ''', (page_url, advisor_id))
                conn.commit()
                cursor.close()
                release_db_connection(conn)
            except Exception as e:
                logger.error(f"Error updating advisor_profiles status: {e}")
            return handle_twitter_profile(advisor_id, page_url, page_title)  # ‚úÖ Use your existing function

        elif 'facebook.com' in lower or 'fb.com' in lower:
            logger.info(f"üìò FACEBOOK DETECTED!")
            # UPDATE ADVISOR_PROFILES STATUS TO MONITORED
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('''
                    UPDATE advisor_profiles 
                    SET profiles_data = (
                        SELECT jsonb_agg(
                            CASE 
                                WHEN elem->>'url' = %s 
                                THEN jsonb_set(elem, '{status}', '"monitored"')
                                ELSE elem
                            END
                        )
                        FROM jsonb_array_elements(profiles_data) AS elem
                    )
                    WHERE advisor_id = %s AND profiles_data IS NOT NULL
                ''', (page_url, advisor_id))
                conn.commit()
                cursor.close()
                release_db_connection(conn)
            except Exception as e:
                logger.error(f"Error updating advisor_profiles status: {e}")
            return handle_facebook_graph(advisor_id, page_url, page_title)

        elif 'linkedin.com' in lower:
            logger.info(f"üíº LINKEDIN DETECTED!")
            # UPDATE ADVISOR_PROFILES STATUS TO MONITORED
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('''
                    UPDATE advisor_profiles 
                    SET profiles_data = (
                        SELECT jsonb_agg(
                            CASE 
                                WHEN elem->>'url' = %s 
                                THEN jsonb_set(elem, '{status}', '"monitored"')
                                ELSE elem
                            END
                        )
                        FROM jsonb_array_elements(profiles_data) AS elem
                    )
                    WHERE advisor_id = %s AND profiles_data IS NOT NULL
                ''', (page_url, advisor_id))
                conn.commit()
                cursor.close()
                release_db_connection(conn)
            except Exception as e:
                logger.error(f"Error updating advisor_profiles status: {e}")
            return handle_linkedin_profile(advisor_id, page_url, page_title)

        elif 'youtube.com' in lower or 'youtu.be' in lower:
            logger.info(f"üé¨ YOUTUBE DETECTED!")
            from datetime import datetime
            used_api = False
            try:
                logger.info(f"üé¨ YouTube import started for {page_url}")

                # Try API first; if it throws, fall back to Atom feed.
                try:
                    videos = fetch_youtube_videos_api(page_url, max_items=2500)
                    used_api = True
                    logger.info(f"üéØ YouTube API path returned {len(videos)} videos (max_items=250)")
                except Exception as api_err:
                    logger.warning(f"‚ö†Ô∏è YouTube API failed: {api_err}. Falling back to Atom feed.")
                    videos = fetch_youtube_videos(page_url, max_items=25)
                    used_api = False
                    logger.info(f"üìª Atom feed path returned {len(videos)} videos (feed caps at ~15-ish)")

                # Optional header only for logs/trace; snapshot will be compact in helper
                header = (
                    f"[youtube_source={'api' if used_api else 'feed'}; "
                    f"items={len(videos)}; "
                    f"ts={datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}Z]"
                )
                logger.info(header)

                # ‚¨áÔ∏è NEW: write rows, keep snapshot minimal
                saved = save_youtube_rows(advisor_id, page_url, page_title, videos, snapshot_limit=0)
                logger.info(f"üé• Saved {saved} YouTube videos to youtube_videos (no blob).")

                try:
                    ch_id = resolve_channel_id_api(page_url) or resolve_youtube_channel_id(page_url)
                    if ch_id:
                        subscribe_youtube_channel(advisor_id, ch_id, page_url)  # ‚Üê ADD THIS LINE
                    else:
                        logger.warning(f"‚ö†Ô∏è Could not resolve channel_id for {page_url}")
                except Exception as sub_err:
                    logger.warning(f"‚ö†Ô∏è WebSub subscribe failed for {page_url}: {sub_err}")

                # UPDATE ADVISOR_PROFILES STATUS TO MONITORED
                try:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute('''
                        UPDATE advisor_profiles 
                        SET profiles_data = (
                            SELECT jsonb_agg(
                                CASE 
                                    WHEN elem->>'url' = %s 
                                    THEN jsonb_set(elem, '{status}', '"monitored"')
                                    ELSE elem
                                END
                            )
                            FROM jsonb_array_elements(profiles_data) AS elem
                        )
                        WHERE advisor_id = %s AND profiles_data IS NOT NULL
                    ''', (page_url, advisor_id))
                    conn.commit()
                    cursor.close()
                    release_db_connection(conn)
                except Exception as e:
                    logger.error(f"Error updating advisor_profiles status: {e}")

                # Normalize host for domain_details
                host = urlparse(page_url).netloc.replace('www.', '').lower()
                if host == 'youtu.be':
                    host = 'youtube.com'
                logger.info(f"‚úÖ YouTube import complete using {'API' if used_api else 'Atom feed'}; redirecting to /domain_details/{advisor_id}/{host}")
                return redirect(url_for('domain_details', advisor_id=advisor_id, domain=host))

            except Exception as e:
                logger.error(f"‚ùå YouTube import error: {e}", exc_info=True)
                threading.Thread(
                    target=monitor_advisor_for_new_site,
                    args=(advisor_id, page_url),
                    daemon=True
                ).start()
                return redirect(url_for('advisor_details', advisor_id=advisor_id))
            # no finally needed for conn here




        # === Default: regular website (start monitor thread as before) ===
        threading.Thread(
            target=monitor_advisor_for_new_site,
            args=(advisor_id, page_url),
            daemon=True
        ).start()

        logger.info(f'Site {page_url} added. Website scan started.')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))

    except Exception as e:
        logger.error(f"Error adding site: {e}", exc_info=True)
        return render_template('add_link.html',
                               advisor_id=advisor_id,
                               error=f'Error adding site: {str(e)}')


def resubscribe_all_channels():
    """
    Re-subscribe all known channels so leases don't expire.
    Uses distinct (advisor_id, channel_id, channel_url) from youtube_videos.
    """
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        SELECT DISTINCT advisor_id, channel_id, channel_url
        FROM youtube_videos
        WHERE channel_id IS NOT NULL AND channel_url IS NOT NULL
    """)
    rows = cur.fetchall()
    cur.close()
    release_db_connection(conn)

    renewed = 0
    for advisor_id, channel_id, channel_url in rows:
        try:
            subscribe_youtube_channel(advisor_id, channel_id, channel_url)
            renewed += 1
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Re-subscribe failed: advisor={advisor_id} channel={channel_id}: {e}")

    logger.info(f"üîÅ Re-subscribed {renewed} YouTube channels")

# Start APScheduler once
def _start_scheduler_once():
    if getattr(app, "_aps_started", False):
        return
    from apscheduler.schedulers.background import BackgroundScheduler
    sched = BackgroundScheduler(daemon=True)
    # every 5 days, add some jitter so multiple instances don't align
    sched.add_job(resubscribe_all_channels, "interval", days=5, jitter=7200)
    sched.start()
    app._aps_started = True
    logger.info("‚è∞ APScheduler started (YouTube re-subscribe job)")

# Call this at app start (but not in reloader child)
if os.environ.get("ENABLE_APSCHEDULER", "1") == "1":
    try:
        _start_scheduler_once()
    except Exception as e:
        logger.warning(f"APSCHEDULER not started: {e}")

def update_profile_status_in_db(advisor_id, profile_url, new_status="monitored"):
    """Update a profile's status in the advisor_profiles table - ROBUST VERSION"""
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        logger.info(f"üîÑ Attempting to update profile status: {profile_url} -> {new_status} for advisor {advisor_id}")
        
        # Use PostgreSQL JSONB functions to update directly in the database
        cur.execute("""
            UPDATE advisor_profiles 
            SET profiles_data = (
                SELECT jsonb_agg(
                    CASE 
                        WHEN profile->>'url' = %s 
                        THEN jsonb_set(profile, '{status}', %s)
                        ELSE profile 
                    END
                )
                FROM jsonb_array_elements(profiles_data) AS profile
            )
            WHERE advisor_id = %s
            AND EXISTS (
                SELECT 1 
                FROM jsonb_array_elements(profiles_data) AS profile 
                WHERE profile->>'url' = %s
            )
        """, (profile_url, f'"{new_status}"', advisor_id, profile_url))
        
        rows_affected = cur.rowcount
        
        if rows_affected > 0:
            conn.commit()
            logger.info(f"‚úÖ Successfully updated profile status in database: {profile_url} -> {new_status}")
        else:
            logger.warning(f"‚ö†Ô∏è No profile found to update: {profile_url} for advisor {advisor_id}")
        
        release_db_connection(conn)
        return rows_affected > 0
        
    except Exception as e:
        logger.error(f"‚ùå Error updating profile status: {e}", exc_info=True)
        try:
            release_db_connection(conn)
        except:
            pass
        return False
    
def handle_twitter_profile(advisor_id, twitter_url, page_title):
    """Handle Twitter profile scraping"""
    try:
        # Use Postgres connection instead of SQLite
        conn = get_db_connection()
        cur = conn.cursor()
        
        cur.execute('SELECT name FROM advisors WHERE id = %s', (advisor_id,))
        advisor = cur.fetchone()
        
        if not advisor:
            release_db_connection(conn)
            return render_template('add_link.html', 
                                 advisor_id=advisor_id, 
                                 error='Advisor not found')
        
        advisor_name = advisor[0]
        
        # Start Twitter scraping in background
        def scrape_twitter_background():
            try:
                tweets = scrape_twitter_profile(twitter_url)
                
                # Better content formatting
                tweet_content = f"TWITTER PROFILE: {twitter_url}\n\n"
                for i, tweet in enumerate(tweets, 1):
                    tweet_text = tweet.get('text', '')
                    tweet_timestamp = tweet.get('timestamp', 'Unknown')
        
                    if tweet_text and len(tweet_text) >= 1:
                        tweet_content += f"Tweet {i} ({tweet_timestamp}):\n{tweet_text}\n\n"
                    else:
                        logger.warning(f"Tweet {i} has no content: {tweet}")
    
                logger.info(f"Final tweet content length: {len(tweet_content)} characters")
                logger.info(f"Content preview: {tweet_content[:200]}...")
                
                # Only save if we actually got content
                if len(tweets) > 0:
                    content_hash = hashlib.md5(tweet_content.encode()).hexdigest()
                    actual_title = page_title or f"Twitter Profile - {len(tweets)} tweets"
                    
                    # Save to Postgres database
                    conn = get_db_connection()
                    cur = conn.cursor()
                    
                    cur.execute('''INSERT INTO website_snapshots 
                                   (advisor_id, page_url, content_hash, page_title, content_text, last_checked, last_scan_checked)
                                   VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
                                   ON CONFLICT (advisor_id, page_url) DO UPDATE SET
                                       content_hash = EXCLUDED.content_hash,
                                       content_text = EXCLUDED.content_text,
                                       last_scan_checked = NOW()''',
                                 (advisor_id, twitter_url, content_hash, actual_title, tweet_content))
                    
                    conn.commit()
                    release_db_connection(conn)
                    
                    # ‚úÖ NEW: Update profile status to "monitored" in advisor_profiles
                    update_profile_status_in_db(advisor_id, twitter_url, "monitored")
                        
                    logger.info(f"Twitter profile scraped: {len(tweets)} tweets saved for {advisor_name}")
                else:
                    logger.warning(f"No tweets extracted for {advisor_name}")
                
            except Exception as e:
                logger.error(f"Error in Twitter background scraping: {e}")
        
        threading.Thread(target=scrape_twitter_background, daemon=True).start()
        release_db_connection(conn)
    
        logger.info(f'Twitter profile {twitter_url} added to {advisor_name}! Scraping started.')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
        
    except Exception as e:
        logger.error(f"Error handling Twitter profile: {e}")
        return render_template('add_link.html', 
                             advisor_id=advisor_id, 
                             error=f'Error adding Twitter profile: {str(e)}')
    
def handle_facebook_profile(advisor_id, facebook_url, page_title):
    """Handle Facebook profile scraping"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                conn.close()
                return render_template('add_link.html', 
                                     advisor_id=advisor_id, 
                                     error='Advisor not found')
            
            advisor_name = advisor[0]
            
            # Start Facebook scraping in background
            def scrape_facebook_background():
                try:
                    posts = scrape_facebook_profile(facebook_url)

                    # Add this right after posts = scrape_facebook_profile(facebook_url)
                    logger.info(f"=== POST PROCESSING DEBUG ===")
                    for i, post in enumerate(posts):
                        logger.info(f"Post {i+1}: text='{post['text']}', len={len(post['text'])}")
                        logger.info(f"  Valid content: {bool(post['text'] and len(post['text'].strip()) > 0)}")
                    
                    # **Format Facebook content**
                    facebook_content = f"FACEBOOK PROFILE: {facebook_url}\n\n"
                    for i, post in enumerate(posts, 1):
                        post_text = post.get('text', '')
                        post_timestamp = post.get('timestamp', 'Unknown')
            
                        if post_text:
                            facebook_content += f"Post {i} ({post_timestamp}):\n{post_text}\n\n"
                        else:
                            logger.warning(f"Post {i} has no content: {post}")
        
                    logger.info(f"Final Facebook content length: {len(facebook_content)} characters")
                    logger.info(f"Content preview: {facebook_content[:200]}...")
                    
                    # Only save if we actually got content
                    if len(posts) > 0:
                        content_hash = hashlib.md5(facebook_content.encode()).hexdigest()
                        actual_title = page_title or f"Facebook Profile - {len(posts)} posts"
                        
                        # Save to database
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            
                            c.execute('''INSERT INTO website_snapshots 
                                       (advisor_id, page_url, content_hash, page_title, content_text)
                                       VALUES (?, ?, ?, ?, ?)''',
                                     (advisor_id, facebook_url, content_hash, actual_title, facebook_content))
                            
                            conn.commit()
                            conn.close()
                            
                        logger.info(f"Facebook profile scraped: {len(posts)} posts saved for {advisor_name}")
                    else:
                        logger.warning(f"No posts extracted for {advisor_name}")
                    
                except Exception as e:
                    logger.error(f"Error in Facebook background scraping: {e}")
            
            threading.Thread(target=scrape_facebook_background, daemon=True).start()
            conn.close()
        
        logger.info(f'Facebook profile {facebook_url} added to {advisor_name}! Scraping started.')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
        
    except Exception as e:
        logger.error(f"Error handling Facebook profile: {e}")
        return render_template('add_link.html', 
                             advisor_id=advisor_id, 
                             error=f'Error adding Facebook profile: {str(e)}')

def handle_linkedin_profile(advisor_id, linkedin_url, page_title):
    """Handle LinkedIn profile scraping"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                conn.close()
                return render_template('add_link.html', 
                                     advisor_id=advisor_id, 
                                     error='Advisor not found')
            
            advisor_name = advisor[0]
            
            # Start LinkedIn scraping in background
            def scrape_linkedin_background():
                try:
                    posts = scrape_linkedin_profile(linkedin_url)
                    
                    # **Format LinkedIn content**
                    linkedin_content = f"LINKEDIN PROFILE: {linkedin_url}\n\n"
                    for i, post in enumerate(posts, 1):
                        post_text = post.get('text', '')
                        post_timestamp = post.get('timestamp', 'Unknown')
            
                        if post_text:
                            linkedin_content += f"Post {i} ({post_timestamp}):\n{post_text}\n\n"
                        else:
                            logger.warning(f"Post {i} has no content: {post}")
        
                    logger.info(f"Final LinkedIn content length: {len(linkedin_content)} characters")
                    logger.info(f"Content preview: {linkedin_content[:200]}...")
                    
                    # Only save if we actually got content
                    if len(posts) > 0:
                        content_hash = hashlib.md5(linkedin_content.encode()).hexdigest()
                        actual_title = page_title or f"LinkedIn Profile - {len(posts)} posts"
                        
                        # Save to database
                        with db_lock:
                            conn = get_db_connection_sqlite()
                            c = conn.cursor()
                            
                            c.execute('''INSERT INTO website_snapshots 
                                       (advisor_id, page_url, content_hash, page_title, content_text)
                                       VALUES (?, ?, ?, ?, ?)''',
                                     (advisor_id, linkedin_url, content_hash, actual_title, linkedin_content))
                            
                            conn.commit()
                            conn.close()
                            
                        logger.info(f"LinkedIn profile scraped: {len(posts)} posts saved for {advisor_name}")
                    else:
                        logger.warning(f"No posts extracted for {advisor_name}")
                    
                except Exception as e:
                    logger.error(f"Error in LinkedIn background scraping: {e}")
            
            threading.Thread(target=scrape_linkedin_background, daemon=True).start()
            conn.close()
        
        logger.info(f'LinkedIn profile {linkedin_url} added to {advisor_name}! Scraping started.')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
        
    except Exception as e:
        logger.error(f"Error handling LinkedIn profile: {e}")
        return render_template('add_link.html', 
                             advisor_id=advisor_id, 
                             error=f'Error adding LinkedIn profile: {str(e)}')
    

def monitor_advisor_for_new_site(advisor_id, website_url):
    """Monitor a new site using the EXACT same logic as add_advisor"""
    with db_lock:
        try:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                conn.close()
                return
            
            advisor_name = advisor[0]
            monitor = CarouselAwareMonitor()
            
            # **USE THE EXACT SAME PAGE DISCOVERY AS ADD_ADVISOR**
            pages = monitor.find_all_pages(website_url)
            logger.info(f"Found {len(pages)} pages for new site {website_url}")
            
            for page_url in pages:
                try:
                    # Check if this page already exists
                    c.execute('SELECT id FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, page_url))
                    if c.fetchone():
                        logger.info(f"Page {page_url} already exists, skipping")
                        continue
                    
                    # Use the same scraping logic as add_advisor/monitor_advisor
                    if monitor.likely_has_carousel(page_url):
                        logger.info(f"Detected carousel page, using enhanced scraping: {page_url}")
                        soup, carousel_data = monitor.scrape_carousel_content(page_url)
                        if soup is None:
                            logger.warning(f"Could not scrape carousel page: {page_url}")
                            continue
                    else:
                        soup = monitor.scrape_page(page_url)
                        carousel_data = []
                    
                    if soup:
                        content = monitor.extract_content(soup)
                        
                        # Combine regular content with carousel content
                        if carousel_data:
                            carousel_text = extract_text_from_carousel_data(carousel_data)
                            content['text'] += f"\n\nCARROUSEL CONTENT:\n{carousel_text}"

                        # Store content in the EXACT same format as add_advisor
                        new_full_content = content['text']
                        content_hash = hashlib.md5(new_full_content.encode()).hexdigest()
                        
                        c.execute('''INSERT INTO website_snapshots (advisor_id, page_url, content_hash, page_title, content_text)
                                    VALUES (?, ?, ?, ?, ?)''',
                                 (advisor_id, page_url, content_hash, content['title'], new_full_content))
                        
                        logger.info(f"Added page: {page_url}")
                
                except Exception as e:
                    logger.error(f"Error monitoring page {page_url}: {str(e)}")
                    continue
            
            conn.commit()
            conn.close()
            
            logger.info(f"Completed site monitoring for {advisor_name}: {len(pages)} pages processed")
            
        except Exception as e:
            logger.error(f"Error in monitor_advisor_for_new_site: {str(e)}")
            
@app.route('/delete_website/<int:advisor_id>', methods=['POST'])
def delete_website(advisor_id):
    """Delete a website/domain from an advisor"""
    try:
        data = request.get_json()
        domain_to_delete = data.get('domain')
        
        if not domain_to_delete:
            return jsonify({'success': False, 'error': 'Domain is required'}), 400
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            # Verify advisor exists
            cursor.execute('SELECT name FROM advisors WHERE id = %s', (advisor_id,))
            advisor = cursor.fetchone()
    
            if not advisor:
                return jsonify({'success': False, 'error': 'Advisor not found'}), 404
    
            # GET URLS BEFORE DELETION
            cursor.execute('''SELECT DISTINCT page_url FROM website_snapshots 
                             WHERE advisor_id = %s AND page_url LIKE %s''', 
                          (advisor_id, f'%{domain_to_delete}%'))
            deleted_urls = [row[0] for row in cursor.fetchall()]
    
            # Delete all snapshots for pages that belong to this domain
            cursor.execute('''DELETE FROM website_snapshots 
                             WHERE advisor_id = %s AND page_url LIKE %s''', 
                          (advisor_id, f'%{domain_to_delete}%'))
            deleted_snapshots = cursor.rowcount
    
            # Delete all changes for this advisor (changes table doesn't have page_url)
            cursor.execute('''DELETE FROM changes 
                             WHERE advisor_id = %s''', 
                          (advisor_id,))
            deleted_changes = cursor.rowcount
    
            # Delete all compliance issues for this advisor/domain
            cursor.execute('''DELETE FROM compliance_issues 
                             WHERE advisor_id = %s AND page_url LIKE %s''', 
                          (advisor_id, f'%{domain_to_delete}%'))
            deleted_issues = cursor.rowcount
    
            # Clean up advisor_profiles.profiles_data - remove deleted URLs
            if deleted_urls:
                cursor.execute('''
                    UPDATE advisor_profiles 
                    SET profiles_data = (
                        SELECT jsonb_agg(elem)
                        FROM jsonb_array_elements(profiles_data) AS elem
                        WHERE NOT (elem->>'url' LIKE %s)
                    )
                    WHERE advisor_id = %s AND profiles_data IS NOT NULL
                ''', (f'%{domain_to_delete}%', advisor_id))
                
                logger.info(f"Updated advisor_profiles for advisor {advisor_id}, removed URLs matching {domain_to_delete}")
    
            conn.commit()
    
            logger.info(f"Deleted website {domain_to_delete} for advisor {advisor_id}: "
                       f"{deleted_snapshots} pages, {deleted_changes} changes, {deleted_issues} issues")
    
            return jsonify({
                'success': True, 
                'message': f'Successfully deleted {domain_to_delete}',
                'deleted': {
                    'pages': deleted_snapshots,
                    'changes': deleted_changes,
                    'issues': deleted_issues
                },
                'deleted_urls': deleted_urls
            })
            
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            cursor.close()
            release_db_connection(conn)
            
    except Exception as e:
        logger.error(f"Error deleting website: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500
    
@app.route('/add_website/<int:advisor_id>', methods=['POST'])
def add_website(advisor_id):
    """Add a new website to an advisor"""
    try:
        data = request.get_json()
        website_url = data.get('website_url', '').strip()
        
        if not website_url:
            return jsonify({'success': False, 'error': 'Website URL is required'}), 400
        
        # Ensure URL has protocol
        if not website_url.startswith(('http://', 'https://')):
            website_url = 'https://' + website_url
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Verify advisor exists
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                conn.close()
                return jsonify({'success': False, 'error': 'Advisor not found'}), 404
            
            # Check if this URL already exists for this advisor
            c.execute('SELECT id FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                     (advisor_id, website_url))
            existing = c.fetchone()
            
            if existing:
                conn.close()
                return jsonify({'success': False, 'error': 'This website is already being monitored'}), 400
            
            # Try to scrape the website
            monitor = CarouselAwareMonitor()
            soup = monitor.scrape_page(website_url)
            
            if soup:
                content = monitor.extract_content(soup)
                full_content = f"TITLE: {content['title']}\nCONTENT: {content['text']}"
                content_hash = hashlib.md5(full_content.encode()).hexdigest()
                page_title = content['title'] or 'Manual Website'
                
                # Add to website_snapshots
                c.execute('''INSERT INTO website_snapshots (advisor_id, page_url, content_hash, page_title, content_text)
                            VALUES (?, ?, ?, ?, ?)''',
                         (advisor_id, website_url, content_hash, page_title, full_content))
                
                conn.commit()
                conn.close()
                
                # Start monitoring this website in the background
                def start_monitoring():
                    try:
                        monitor_advisor(advisor_id)
                    except Exception as e:
                        logger.error(f"Error starting monitoring: {e}")
                
                threading.Thread(target=start_monitoring, daemon=True).start()
                
                return jsonify({
                    'success': True, 
                    'message': f'Successfully added {website_url}',
                    'page_title': page_title
                })
            else:
                # If scraping fails, still add it as a placeholder
                placeholder_content = f"TITLE: Manual Website\nCONTENT: Unable to scrape content from {website_url}"
                content_hash = hashlib.md5(placeholder_content.encode()).hexdigest()
                
                c.execute('''INSERT INTO website_snapshots (advisor_id, page_url, content_hash, page_title, content_text)
                            VALUES (?, ?, ?, ?, ?)''',
                         (advisor_id, website_url, content_hash, 'Manual Website', placeholder_content))
                
                conn.commit()
                conn.close()
                
                return jsonify({
                    'success': True, 
                    'message': f'Added {website_url} (content could not be scraped)',
                    'page_title': 'Manual Website'
                })
                
    except Exception as e:
        logger.error(f"Error adding website: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/compliance_issues_page/<int:advisor_id>')
def compliance_issues_page(advisor_id):
    """Display all compliance issues for a specific page"""
    page_url = request.args.get('page_url')
    
    if not page_url:
        flash('Page URL is required', 'error')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()

        # ADD THIS TEMPORARILY HERE:
        c.execute("PRAGMA table_info(compliance_issues)")
        columns = c.fetchall()
        print("Compliance issues table columns:", columns)
        # END OF TEMPORARY CODE
        
        # Get advisor info
        c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        
        if not advisor:
            flash('Advisor not found', 'error')
            return redirect(url_for('dashboard'))
        
        # Get all compliance issues for this specific page
        c.execute('''SELECT * FROM compliance_issues 
                    WHERE advisor_id = ? AND page_url = ? 
                    ORDER BY detected_at DESC''', (advisor_id, page_url))
        issues = c.fetchall()
        
        # Get page title from website_snapshots
        c.execute('SELECT page_title FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                 (advisor_id, page_url))
        page_info = c.fetchone()
        page_title = page_info[0] if page_info else 'Unknown Page'
        
        conn.close()
    
    return render_template('compliance_issues_page.html', 
                         advisor=advisor, 
                         issues=issues, 
                         page_url=page_url,
                         page_title=page_title)



@app.route('/compliance_details/<int:issue_id>')
def compliance_details(issue_id):
    """Display detailed view of a specific compliance issue"""
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Get the compliance issue with advisor information
        c.execute('''SELECT ci.*, a.name as advisor_name 
                    FROM compliance_issues ci 
                    JOIN advisors a ON ci.advisor_id = a.id 
                    WHERE ci.id = ?''', (issue_id,))
        issue_row = c.fetchone()
        
        conn.close()
    
    if not issue_row:
        flash('Compliance issue not found', 'error')
        return redirect(url_for('dashboard'))
    
    # Convert to a dict-like object with the properties used in the template
    issue_data = {
        'id': issue_row[0],
        'advisor_id': issue_row[1],
        'page_url': issue_row[2],
        'flagged_text': issue_row[3],
        'suggested_text': issue_row[4],
        'confidence': issue_row[5],
        'rationale': issue_row[6],
        'detected_at': issue_row[7],
        'advisor_name': issue_row[8],
        'severity': 'High',  # You can add this to your database if needed
        'issue_type': 'Content Compliance',
        'rule_violated': 'Standard Compliance Rule',
        'context': None
    }
    
    return render_template('compliance_details.html', issue=issue_data)

@app.route('/domain_details/<int:advisor_id>/<path:domain>')
def domain_details(advisor_id, domain):
    from urllib.parse import urlparse, unquote
    import itertools

    # normalize incoming domain param
    raw = unquote(domain)
    if '://' in raw:
        _u = urlparse(raw)
        want_host = (_u.netloc or raw).lower()
    else:
        want_host = raw.lower()
    want_host = want_host.replace('www.', '')

    # optional filter: /domain_details/...?...specific_page=URL
    specific_page = request.args.get('specific_page')

    conn = None
    c = None
    try:
        conn = get_db_connection()              # ‚úÖ Postgres pool
        c = conn.cursor()

        # --- advisor check ---
        c.execute('SELECT * FROM advisors WHERE id = %s', (advisor_id,))
        advisor = c.fetchone()
        if not advisor:
            app.logger.warning('Advisor not found: %s', advisor_id)
            return redirect(url_for('dashboard'))

        # --- website_snapshots: introspect columns so we can SELECT safely ---
        c.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = current_schema() AND table_name = 'website_snapshots'
        """)
        ws_cols = {r[0] for r in c.fetchall()}

        def col_or_null(col, cast):
            # eg: 'last_checked' or 'NULL::timestamp AS last_checked'
            return col if col in ws_cols else f'NULL::{cast} AS {col}'

        # keep your tuple shape:
        # [0]=page_url, [1]=page_title, [2]=content_hash, [3]=last_checked,
        # [4]=compliance_checked_at, [5]=last_scan_checked, [6]=content_text
        snap_select = ", ".join([
            col_or_null('page_url', 'text'),
            col_or_null('page_title', 'text'),
            col_or_null('content_hash', 'text'),
            col_or_null('last_checked', 'timestamp'),
            col_or_null('compliance_checked_at', 'timestamp'),
            col_or_null('last_scan_checked', 'timestamp'),
            col_or_null('content_text', 'text'),
        ])

        # grab all pages for advisor (we filter by host in Python)
        c.execute(f"""
            SELECT {snap_select}
            FROM website_snapshots
            WHERE advisor_id = %s
            ORDER BY page_url NULLS LAST
        """, (advisor_id,))
        all_pages = c.fetchall()

        # --- filter to requested domain (or specific_page if provided) ---
        domain_pages = []
        norm = lambda h: (h or '').lower().replace('www.', '')
        for page in all_pages:
            page_url = page[0]
            if not page_url:
                continue
            if specific_page and page_url == specific_page:
                domain_pages.append(page)
                continue
            parsed = urlparse(page_url)
            host = norm(parsed.netloc)
            if host == want_host:
                domain_pages.append(page)

        pages_data = []
        total_changes = 0
        total_issues = 0

        # --- table existence helpers (for optional tables) ---
        def table_exists(name):
            c.execute("""
                SELECT EXISTS (
                  SELECT 1 FROM information_schema.tables
                  WHERE table_schema = current_schema() AND table_name = %s
                )
            """, (name,))
            return bool(c.fetchone()[0])

        has_changes = table_exists('changes')
        has_issues = table_exists('compliance_issues')
        has_checks = table_exists('compliance_checks')

        # --- Prefetch & index CHANGES by URL (handles tables without page_url) ---
        from collections import defaultdict

        changes_by_url = defaultdict(list)
        recent_changes_ordered = []  # keep overall ordering by timestamp for recency

        if has_changes:
            # introspect columns
            c.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = current_schema() AND table_name = 'changes'
            """)
            ch_cols = [r[0] for r in c.fetchall()]

            # pick a timestamp column if present
            ts_col = next((x for x in ('timestamp','changed_at','created_at','updated_at','ts') if x in ch_cols), None)
            order_clause = f"ORDER BY {ts_col} DESC NULLS LAST" if ts_col else ""

            # fetch all rows for this advisor
            c.execute(f"SELECT * FROM changes WHERE advisor_id = %s {order_clause}", (advisor_id,))
            recent_changes_ordered = c.fetchall()
            desc = [d.name for d in c.description]

            # pick a likely URL column (if any)
            url_idx = None
            for candidate in ('page_url','url','resource_url','link','page'):
                if candidate in desc:
                    url_idx = desc.index(candidate)
                    break

            # helper to find a URL from a row if no explicit column exists
            def row_url(row):
                if url_idx is not None:
                    return row[url_idx]
                for val in row:
                    if isinstance(val, str) and val.startswith('http'):
                        return val
                return None

            # build index: exact page_url -> list of change rows (already ordered by time)
            for r in recent_changes_ordered:
                u = row_url(r)
                if u:
                    changes_by_url[u].append(r)

        def count_changes_for_url(u: str) -> int:
            return len(changes_by_url.get(u, []))

        def recent_changes_for_url(u: str, limit: int = 3):
            return changes_by_url.get(u, [])[:limit]

        # --- Prefetch & index COMPLIANCE ISSUES by URL (works even if no page_url col) ---
        from collections import defaultdict

        issues_by_url = defaultdict(list)
        recent_issues_ordered = []

        if has_issues:
            # introspect columns
            c.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = current_schema()
                  AND table_name   = 'compliance_issues'
            """)
            ci_cols = [r[0] for r in c.fetchall()]

            # choose a timestamp column if present
            ts_issue = next((x for x in ('detected_at','created_at','timestamp','updated_at','ts')
                             if x in ci_cols), None)
            order_clause = f"ORDER BY {ts_issue} DESC NULLS LAST" if ts_issue else ""

            # fetch all issues for this advisor (no page_url predicate)
            c.execute(f"SELECT * FROM compliance_issues WHERE advisor_id = %s {order_clause}", (advisor_id,))
            recent_issues_ordered = c.fetchall()
            desc = [d.name for d in c.description]

            # find a URL-like column if one exists
            issue_url_idx = None
            for cand in ('page_url','url','resource_url','link','page'):
                if cand in desc:
                    issue_url_idx = desc.index(cand)
                    break

            def issue_row_url(row):
                if issue_url_idx is not None:
                    return row[issue_url_idx]
                # fall back: sniff a URL-like string
                for v in row:
                    if isinstance(v, str) and v.startswith('http'):
                        return v
                return None

            for r in recent_issues_ordered:
                u = issue_row_url(r)
                if u:
                    issues_by_url[u].append(r)

        # locate the flagged text column if present
        flagged_idx = desc.index('flagged_text') if 'flagged_text' in desc else None

        def issues_count_for_pattern(page_url: str, prefix: str) -> int:
            """Count issues whose flagged text starts with a given prefix on a specific page_url."""
            rows = issues_by_url.get(page_url, [])
            if flagged_idx is not None:
                return sum(1 for r in rows
                           if isinstance(r[flagged_idx], str) and r[flagged_idx].startswith(prefix))
            # fallback: scan all string cells in the row
            count = 0
            for r in rows:
                for v in r:
                    if isinstance(v, str) and v.startswith(prefix):
                        count += 1
                        break
            return count


        def count_issues_for_url(u: str) -> int:
            return len(issues_by_url.get(u, []))

        def recent_issues_for_url(u: str, limit: int = 3):
            return issues_by_url.get(u, [])[:limit]



        for page in domain_pages:
            page_url = page[0]
            content_text = page[6] if len(page) > 6 and page[6] is not None else ''

            # --- NEW: Prefer row-based YouTube data over blob ---
            youtube_rows = []
            if 'youtube.com' in page_url.lower() or 'youtu.be' in page_url.lower():
                c.execute("""
                    SELECT video_id, title, video_url, published_at, description
                    FROM youtube_videos
                    WHERE advisor_id = %s AND channel_url = %s
                    ORDER BY published_at DESC NULLS LAST
                    LIMIT 500
                """, (advisor_id, page_url))
                youtube_rows = [
                    {
                        'id': r[0],
                        'title': r[1],
                        'url': r[2],
                        'published': r[3],
                        'description': r[4] or ''
                    }
                    for r in c.fetchall()
                ]


            # --- counts ---
            changes_count = 0
            issues_count  = 0

            recent_changes = []
            recent_issues  = []

            if has_changes:
                
                # Get changes for this page (from prefetch)
                changes_count = count_changes_for_url(page_url)
                total_changes += changes_count
                recent_changes = recent_changes_for_url(page_url, 3)


            if has_issues:
                issues_count = count_issues_for_url(page_url)
                recent_issues = recent_issues_for_url(page_url, 3)

            total_issues  += issues_count

            # --- compliance_checks (optional) ---
            post_compliance_times = {}
            tweet_compliance_times = {}
            if has_checks:
                # Introspect columns in compliance_checks
                c.execute("""
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_schema = current_schema() AND table_name = 'compliance_checks'
                """)
                cc_cols = [r[0] for r in c.fetchall()]

                # Choose likely column names that exist in your schema
                id_col = next((x for x in ('post_id','tweet_id','content_id','item_id','message_id','object_id','id') if x in cc_cols), None)
                checked_col = next((x for x in ('checked_at','created_at','timestamp','updated_at','ts') if x in cc_cols), None)
                page_col = next((x for x in ('page_url','url','resource_url','link','page') if x in cc_cols), None)

                if id_col and checked_col:
                    if page_col:
                        query = sql.SQL("""
                            SELECT {id_col}, MAX({checked_col}) AS last_check
                            FROM compliance_checks
                            WHERE advisor_id = %s AND {page_col} = %s
                            GROUP BY {id_col}
                        """).format(
                            id_col=sql.Identifier(id_col),
                            checked_col=sql.Identifier(checked_col),
                            page_col=sql.Identifier(page_col)
                        )
                        params = (advisor_id, page_url)
                    else:
                        query = sql.SQL("""
                            SELECT {id_col}, MAX({checked_col}) AS last_check
                            FROM compliance_checks
                            WHERE advisor_id = %s
                            GROUP BY {id_col}
                        """).format(
                            id_col=sql.Identifier(id_col),
                            checked_col=sql.Identifier(checked_col)
                        )
                        params = (advisor_id,)

                    c.execute(query, params)
                    for raw_id, last_check in c.fetchall():
                        pid = (raw_id or '').strip()
                        # normalize label-only IDs to match your template lookups
                        clean_id = (pid.split('(')[0].strip().rstrip(':')) if '(' in pid else pid.rstrip(':')

                        if 'x.com' in page_url.lower() or 'twitter.com' in page_url.lower():
                            # store by cleaned id; template asks for cleaned IDs
                            tweet_compliance_times[clean_id] = last_check
                        else:
                            post_compliance_times[clean_id] = last_check
                else:
                    app.logger.info("compliance_checks present but missing id/timestamp columns; skipping.")

            # --- per-post/tweet issue breakout using content_text (if present) ---
            tweet_issues = {}
            post_issues  = {}

            if content_text:
                if ('x.com' in page_url.lower() or 'twitter.com' in page_url.lower()):
                    # "Tweet <id>:" sections
                    for section in content_text.split('Tweet '):
                        sec = section.strip()
                        if not sec:
                            continue
                        first = sec.split('\n', 1)[0].strip()
                        clean_id = first.split('(')[0].strip().rstrip(':')
                        if not clean_id:
                            continue
                        if has_issues:
                            # pattern stored like "Tweet <id>:%"
                            like_pattern = f"Tweet {clean_id}:%"
                            tweet_issues[clean_id] = issues_count_for_pattern(page_url, f"Tweet {clean_id}:")


                elif ('facebook.com' in page_url.lower() or 'fb.com' in page_url.lower()):
                    for section in content_text.split('Post '):
                        sec = section.strip()
                        if not sec:
                            continue
                        first = sec.split('\n', 1)[0].strip()
                        clean_id = first.split('(')[0].strip().rstrip(':')
                        if not clean_id:
                            continue
                        if has_issues:
                            like_pattern = f"Facebook Post {clean_id}:%"
                            post_issues[clean_id] = issues_count_for_pattern(page_url, f"Facebook Post {clean_id}:")


                elif 'linkedin.com' in page_url.lower():
                    for section in content_text.split('Post '):
                        sec = section.strip()
                        if not sec:
                            continue
                        first = sec.split('\n', 1)[0].strip()
                        clean_id = first.split('(')[0].strip().rstrip(':')
                        if not clean_id:
                            continue
                        if has_issues:
                            like_pattern = f"LinkedIn Post {clean_id}:%"
                            post_issues[clean_id] = issues_count_for_pattern(page_url, f"LinkedIn Post {clean_id}:")

                elif ('youtube.com' in page_url.lower() or 'youtu.be' in page_url.lower()):
                    # Split on "YouTube Video <id>:" like we store in content_text
                    for section in content_text.split('YouTube Video '):
                        sec = section.strip()
                        if not sec:
                            continue
                        first = sec.split('\n', 1)[0].strip()
                        clean_id = first.split('(')[0].strip().rstrip(':')
                        if not clean_id:
                            continue
                        if has_issues:
                            post_issues[clean_id] = issues_count_for_pattern(page_url, f"YouTube Video {clean_id}:")


            app.logger.debug("DEBUG domain page calc: %s", {
                'url': page_url,
                'tweet_compliance_times': tweet_compliance_times,
                'post_compliance_times': post_compliance_times,
                'tweet_issues': tweet_issues,
                'post_issues': post_issues
            })

            pages_data.append({
                'page': page,
                'changes_count': changes_count,
                'issues_count': issues_count,
                'recent_changes': recent_changes,
                'recent_issues': recent_issues,
                'tweet_issues': tweet_issues,
                'post_issues': post_issues,
                'post_compliance_times': post_compliance_times,
                'tweet_compliance_times': tweet_compliance_times,
                'youtube_videos': youtube_rows
            })

        app.logger.info(
            "DEBUG Domain Details: %d pages, %d changes, %d issues (host=%s)",
            len(domain_pages), total_changes, total_issues, want_host
        )

        return render_template('domain_details.html',
                               advisor=advisor,
                               domain=want_host,
                               pages_data=pages_data,
                               total_pages=len(domain_pages),
                               total_changes=total_changes,
                               total_issues=total_issues)

    except Exception as e:
        app.logger.error("domain_details error: %s", e, exc_info=True)
        return redirect(url_for('dashboard'))

    finally:
        try:
            if c: c.close()
        except Exception:
            pass
        try:
            if conn: release_db_connection(conn)   # ‚úÖ give it back to the pool
        except Exception:
            pass


@app.route('/force_scan/<int:advisor_id>')
def force_scan(advisor_id):
    """Force a website scan for a specific advisor"""
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        c.execute('SELECT name, website_url FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        conn.close()
    
    if not advisor:
        return "Advisor not found"
    
    def scan_advisor():
        try:
            logger.info(f"Force scanning advisor {advisor_id}: {advisor[0]}")
            monitor_advisor(advisor_id)
            logger.info(f"Force scan completed for {advisor[0]}")
        except Exception as e:
            logger.error(f"Error in force scan: {str(e)}")
    
    # Run scan in background thread
    threading.Thread(target=scan_advisor, daemon=True).start()
    
    return f'''
    <div style="text-align: center; margin-top: 50px; font-family: Arial;">
        <h3>Scanning Website...</h3>
        <p>Force scanning <strong>{advisor[0]}</strong></p>
        <p>Website: <a href="{advisor[1]}" target="_blank">{advisor[1]}</a></p>
        <p>This may take a few minutes. Check back shortly.</p>
        <br>
        <a href="/debug_changes/{advisor_id}" style="background: #28a745; color: white; padding: 10px 20px; text-decoration: none; border-radius: 4px; margin: 5px;">Check Scan Results</a>
        <a href="/advisor_details/{advisor_id}" style="background: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 4px; margin: 5px;">Back to Advisor Details</a>
    </div>
    '''

@app.route('/debug_changes/<int:advisor_id>')
def debug_changes(advisor_id):
    """Debug what changes exist for an advisor"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get advisor info
            c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            # Get website snapshots
            c.execute('SELECT page_url, page_title, last_checked FROM website_snapshots WHERE advisor_id = ?', (advisor_id,))
            pages = c.fetchall()
            
            # Get all changes
            c.execute('SELECT * FROM changes WHERE advisor_id = ?', (advisor_id,))
            changes = c.fetchall()
            
            conn.close()
        
        return f"""
        <h3>Debug Changes for Advisor {advisor_id}:</h3>
        <p><strong>Advisor:</strong> {advisor}</p>
        <hr>
        <p><strong>Website Pages Scanned ({len(pages)}):</strong></p>
        <ul>{''.join([f'<li>{p[0]} - {p[1]} (Last checked: {p[2]})</li>' for p in pages])}</ul>
        <hr>
        <p><strong>Changes Recorded ({len(changes)}):</strong></p>
        <ul>{''.join([f'<li>Change ID {c[0]}: {c[2]} on {c[7]} at {c[5]}</li>' for c in changes])}</ul>
        <br><a href="/advisor_details/{advisor_id}">Back to Advisor Details</a>
        """
        
    except Exception as e:
        return f"Debug error: {str(e)}"

@app.route('/remove_advisor/<int:advisor_id>', endpoint='remove_advisor')
def remove_advisor_get(advisor_id):
    """Handle GET request for removing advisor (with confirmation) ‚Äî Postgres version"""
    conn = None
    cur = None
    try:
        conn = get_db_connection()       # ‚úÖ Postgres pool
        cur = conn.cursor()
        cur.execute('SELECT name FROM advisors WHERE id = %s', (advisor_id,))
        row = cur.fetchone()
    except Exception as e:
        app.logger.error("remove_advisor_get error: %s", e, exc_info=True)
        # Fall back to dashboard if anything goes wrong
        return redirect(url_for('dashboard'))
    finally:
        try:
            if cur: cur.close()
        except Exception:
            pass
        try:
            if conn: release_db_connection(conn)
        except Exception:
            pass

    if not row:
        return redirect(url_for('dashboard'))

    advisor_name = row[0] or 'this advisor'
    # Same inline confirmation UI you had, unchanged
    return f'''
    <div style="text-align: center; margin-top: 50px; font-family: Arial;">
        <h3>Confirm Removal</h3>
        <p>Are you sure you want to remove <strong>"{advisor_name}"</strong> and all associated data?</p>
        <p style="color: red;"><strong>This action cannot be undone.</strong></p>
        <br>
        <a href="/remove_advisor_confirm/{advisor_id}" style="background: #dc3545; color: white; padding: 10px 20px; text-decoration: none; border-radius: 4px; margin: 10px;">YES, REMOVE</a>
        <a href="/advisor-dashboard" style="background: #6c757d; color: white; padding: 10px 20px; text-decoration: none; border-radius: 4px; margin: 10px;">CANCEL</a>
    </div>
    '''

#@app.route('/remove_advisor_confirm/<int:advisor_id>')
#def remove_advisor_confirm(advisor_id):
    #"""Actually remove the advisor after confirmation"""
    #return remove_advisor(advisor_id)  # Call the existing removal function

from psycopg2 import sql  # make sure this import exists near your other imports

@app.route('/remove_advisor_confirm/<int:advisor_id>')
def remove_advisor_confirm(advisor_id):
    """Delete advisor and all related rows ‚Äî Postgres version"""
    conn = None
    cur = None

    # Helper: does table exist?
    def table_exists(c, table_name: str) -> bool:
        c.execute("""
            SELECT EXISTS (
              SELECT 1
              FROM information_schema.tables
              WHERE table_schema = current_schema()
                AND table_name = %s
            )
        """, (table_name,))
        return bool(c.fetchone()[0])

    # Helper: does a column exist on a table?
    def column_exists(c, table_name: str, col_name: str) -> bool:
        c.execute("""
            SELECT EXISTS (
              SELECT 1
              FROM information_schema.columns
              WHERE table_schema = current_schema()
                AND table_name = %s
                AND column_name = %s
            )
        """, (table_name, col_name))
        return bool(c.fetchone()[0])

    try:
        conn = get_db_connection()
        conn.autocommit = False  # explicit transaction
        cur = conn.cursor()

        # Fetch name for the success message
        cur.execute("SELECT name FROM advisors WHERE id = %s", (advisor_id,))
        row = cur.fetchone()
        advisor_name = (row[0] if row and row[0] else f'Advisor {advisor_id}')

        # List of related tables we commonly see
        candidate_tables = [
            'advisor_profiles',
            'website_snapshots',
            'compliance_issues',
            'compliance_checks',
            # several possible names for "changes" history
            'changes', 'advisor_changes', 'changes_log', 'change_log', 'profile_change_log',
            # optional social tables if you created any
            'facebook_posts', 'linkedin_posts', 'twitter_posts',
            # scans or queue tables if any
            'scan_jobs', 'scan_results'
        ]

        # Delete rows by advisor_id where both table+column exist
        for tbl in candidate_tables:
            if table_exists(cur, tbl) and column_exists(cur, tbl, 'advisor_id'):
                app.logger.info("Deleting from %s for advisor_id=%s", tbl, advisor_id)
                cur.execute(sql.SQL("DELETE FROM {} WHERE advisor_id = %s").format(sql.Identifier(tbl)),
                            (advisor_id,))

        # Finally delete the advisor row itself
        if table_exists(cur, 'advisors'):
            app.logger.info("Deleting advisor %s from advisors table", advisor_id)
            cur.execute("DELETE FROM advisors WHERE id = %s", (advisor_id,))

        conn.commit()

        try:
            # If you use Flask flash messages in your templates:
            flash(f'Removed "{advisor_name}" and all associated data.', 'success')
        except Exception:
            pass

        return redirect(url_for('dashboard'))

    except Exception as e:
        app.logger.error("remove_advisor_confirm error: %s", e, exc_info=True)
        if conn:
            try:
                conn.rollback()
            except Exception:
                pass
        try:
            flash('Error removing advisor. See logs for details.', 'danger')
        except Exception:
            pass
        return redirect(url_for('dashboard'))

    finally:
        try:
            if cur: cur.close()
        except Exception:
            pass
        try:
            if conn: release_db_connection(conn)
        except Exception:
            pass


@app.route('/add_link/<int:advisor_id>', methods=['GET', 'POST'])
def add_link(advisor_id):
    """Add a manual link/page to an existing advisor (legacy route)"""
    if request.method == 'POST':
        # Redirect to the new add_site route for processing
        return add_site(advisor_id)
    
    # GET request - show the form
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        conn.close()
    
    if not advisor:
        return redirect(url_for('dashboard'))
    
    return render_template('add_link.html', advisor_id=advisor_id, advisor_name=advisor[0])


@app.route('/cleanup_advisors')
def cleanup_advisors():
    """Remove duplicate advisors and clean up database"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Find duplicates by name
            c.execute('''SELECT name, COUNT(*) as count, GROUP_CONCAT(id) as ids 
                        FROM advisors 
                        GROUP BY name 
                        HAVING COUNT(*) > 1''')
            duplicates = c.fetchall()
            
            removed_count = 0
            for name, count, ids in duplicates:
                id_list = ids.split(',')
                # Keep the first one, remove the others
                for duplicate_id in id_list[1:]:
                    # Remove related data first
                    c.execute('DELETE FROM changes WHERE advisor_id = ?', (duplicate_id,))
                    c.execute('DELETE FROM website_snapshots WHERE advisor_id = ?', (duplicate_id,))
                    c.execute('DELETE FROM compliance_issues WHERE advisor_id = ?', (duplicate_id,))
                    # Remove the advisor
                    c.execute('DELETE FROM advisors WHERE id = ?', (duplicate_id,))
                    removed_count += 1
            
            conn.commit()
            
            # Show what's left
            c.execute('SELECT * FROM advisors')
            remaining = c.fetchall()
            
            conn.close()
        
        return f"<h3>Cleanup Complete!</h3>Removed {removed_count} duplicate advisors.<br><br><h3>Remaining Advisors:</h3>{remaining}"
        
    except Exception as e:
        return f"Error during cleanup: {str(e)}"


@app.route('/get_all_queued_profiles', methods=['GET'])
def get_all_queued_profiles():
    """Get ALL queued profiles from database"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT advisor_id, profiles_data FROM advisor_profiles")
        results = cursor.fetchall()
        
        all_queued = {
            'facebook': [],
            'twitter': [],  
            'youtube': []
        }
        
        for advisor_id, profiles_data in results:
            if profiles_data:
                for profile in profiles_data:
                    # Only include if not monitored
                    if profile.get('status') != 'monitored':
                        profile_data = {
                            'advisor_id': advisor_id,
                            'url': profile['url'],
                            'title': profile.get('name', 'Profile')
                        }
                        
                        if profile.get('platform') == 'facebook':
                            all_queued['facebook'].append(profile_data)
                        elif profile.get('platform') in ['twitter'] or 'x.com' in profile.get('url', '') or 'twitter.com' in profile.get('url', ''):
                            all_queued['twitter'].append(profile_data)
                        elif profile.get('platform') == 'youtube' or 'youtube.com' in profile.get('url', ''):
                            all_queued['youtube'].append(profile_data)
        
        cursor.close()
        release_db_connection(conn)
        
        return jsonify({
            'success': True,
            'profiles': all_queued
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/advisor-dashboard')
def dashboard():
    import json
    # Default session filter
    if 'bd_name' not in session:
        session['bd_name'] = 'All Advisors'

    advisor_data = {}

    conn = get_db_connection()
    cursor = conn.cursor()

    def table_exists(name: str) -> bool:
        cursor.execute("""
            SELECT EXISTS (
              SELECT 1 FROM information_schema.tables
              WHERE table_schema = current_schema() AND table_name = %s
            )
        """, (name,))
        (exists,) = cursor.fetchone() or (False,)
        return bool(exists)

    def columns_for(table: str) -> set:
        cursor.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = current_schema() AND table_name = %s
        """, (table,))
        return {r[0] for r in cursor.fetchall()}

    try:
        # === 1) Fetch advisors safely (only existing columns) ===
        adv_cols = columns_for('advisors')
        base_fields = ['id']
        for c in ('name', 'firm', 'website_url', 'added_by_username'):
            if c in adv_cols:
                base_fields.append(c)

        order_col = 'name' if 'name' in adv_cols else 'id'
        cursor.execute(f"SELECT {', '.join(base_fields)} FROM advisors ORDER BY {order_col}")
        advisors = cursor.fetchall()
        logger.info(f"Found {len(advisors)} total advisors")

        # Preload existence/columns for related tables
        has_advisor_profiles = table_exists('advisor_profiles')
        ap_cols = columns_for('advisor_profiles') if has_advisor_profiles else set()

        has_compliance = table_exists('compliance_issues')
        ci_cols = columns_for('compliance_issues') if has_compliance else set()

        has_snapshots = table_exists('website_snapshots')
        ws_cols = columns_for('website_snapshots') if has_snapshots else set()

        # Pick a change-log table if any
        change_table = None
        for name in ('changes', 'advisor_changes', 'changes_log', 'change_log',
                     'advisor_change_log', 'profile_change_log'):
            if table_exists(name):
                change_table = name
                break
        ch_cols = columns_for(change_table) if change_table else set()

        for row in advisors:
            # Map row to dict by field name
            idx = {col: i for i, col in enumerate(base_fields)}
            advisor_id = row[idx['id']]
            name = row[idx['name']] if 'name' in idx else None
            firm = row[idx['firm']] if 'firm' in idx else None
            website_url = row[idx['website_url']] if 'website_url' in idx else None
            added_by_username = row[idx['added_by_username']] if 'added_by_username' in idx else None

            # === 2) queued_count from advisor_profiles.profiles_data (if present) ===
            queued_count = 0
            if has_advisor_profiles and 'advisor_id' in ap_cols and 'profiles_data' in ap_cols:
                try:
                    cursor.execute("SELECT profiles_data FROM advisor_profiles WHERE advisor_id = %s", (advisor_id,))
                    profile_row = cursor.fetchone()
                    if profile_row and profile_row[0]:
                        profiles_data = profile_row[0]
                        if isinstance(profiles_data, str):
                            profiles_data = json.loads(profiles_data)
                        if isinstance(profiles_data, list):
                            queued_profiles = [p for p in profiles_data if p.get("status") != "monitored"]
                            queued_count = len(queued_profiles)
                except Exception as e:
                    logger.warning(f"advisor {advisor_id}: couldn't parse profiles_data ({e})")

            # === 3) Recent changes (resilient to column names) ===
            changes = []
            if change_table:
                try:
                    fields = []
                    # always try these if present
                    if 'id' in ch_cols: fields.append('id')
                    if 'advisor_id' in ch_cols: fields.append('advisor_id')
                    if 'change_type' in ch_cols: fields.append('change_type')

                    # find a details-like column and alias
                    details_col = next((c for c in (
                        'change_details','details','diff','payload','data','details_json','change_payload'
                    ) if c in ch_cols), None)
                    if details_col:
                        fields.append(f"{details_col}::text AS change_details")
                    else:
                        fields.append("NULL::text AS change_details")

                    # pick a timestamp-ish column and alias
                    ts_col = next((c for c in (
                        'timestamp','changed_at','created_at','updated_at','ts'
                    ) if c in ch_cols), None)
                    if ts_col:
                        fields.append(f"{ts_col} AS changed_at")
                        order_by = "changed_at"
                    else:
                        fields.append("NOW() AS changed_at")
                        order_by = fields[0].split()[0]  # first real column if any

                    sel = ", ".join(fields)
                    if 'advisor_id' in ch_cols:
                        cursor.execute(
                            f"SELECT {sel} FROM {change_table} WHERE advisor_id = %s "
                            f"ORDER BY {order_by} DESC NULLS LAST LIMIT 5",
                            (advisor_id,)
                        )
                    else:
                        # no advisor_id column: skip (or show nothing)
                        cursor.execute(
                            f"SELECT {sel} FROM {change_table} ORDER BY {order_by} DESC NULLS LAST LIMIT 0"
                        )

                    colnames = [d.name for d in cursor.description]
                    changes = [dict(zip(colnames, r)) for r in cursor.fetchall()]
                except Exception as e:
                    logger.warning(f"advisor {advisor_id}: changes feed skipped ({e})")
                    changes = []

            # === 4) Compliance issues count ===
            compliance_count = 0
            if has_compliance and 'advisor_id' in ci_cols:
                try:
                    cursor.execute("SELECT COUNT(*) FROM compliance_issues WHERE advisor_id = %s", (advisor_id,))
                    compliance_count = cursor.fetchone()[0] or 0
                except Exception as e:
                    logger.warning(f"advisor {advisor_id}: compliance count failed ({e})")

            # === 5) Last scan check (if column exists) ===
            last_scan_check = None
            if has_snapshots and 'advisor_id' in ws_cols and 'last_scan_checked' in ws_cols:
                try:
                    cursor.execute(
                        "SELECT MAX(last_scan_checked) FROM website_snapshots WHERE advisor_id = %s",
                        (advisor_id,)
                    )
                    last_scan_check = cursor.fetchone()[0]
                except Exception as e:
                    logger.warning(f"advisor {advisor_id}: last_scan_checked failed ({e})")

            # Build advisor entry (always)
            advisor_data[advisor_id] = {
                'advisor': {
                    'id': advisor_id,
                    'name': name,
                    'firm': firm,
                    'website_url': website_url,
                    'added_by_username': added_by_username
                },
                'changes': changes,
                'compliance_count': compliance_count,
                'last_scan_check': last_scan_check,
                'issue_count': compliance_count,  # alias for template
                'queued_count': queued_count
            }

    except Exception as e:
        logger.error(f"Error building advisor dashboard: {e}", exc_info=True)
    finally:
        cursor.close()
        release_db_connection(conn)

    return render_template('dashboard.html', advisor_data=advisor_data)

    
@app.route('/add_advisor', methods=['GET', 'POST'])
def add_advisor():
    if request.method == 'POST':
        firm = request.form['firm'].strip()
        name = firm  # Use firm name as the advisor name
        location = request.form['location'].strip()
        website = request.form.get('website', '').strip()
        broker = request.form.get('broker', '').strip()

        # Ignore brokercheck.finra.org URLs
        if 'brokercheck.finra.org' in website.lower():
            website = ''
        
        if not name or not website:
            print('Advisor name and website are required')
            return render_template('add_advisor.html', error='Advisor name and website are required')
        
        if not website.startswith(('http://', 'https://')):
            website = 'https://' + website
        
        user_id = 1  # All advisors belong to same "user"
        
        try:
            with db_lock:
                conn = get_db_connection_sqlite()
                c = conn.cursor()
                
                # Check for duplicate advisor by name (case-insensitive)
                c.execute('SELECT id, name, website_url FROM advisors WHERE LOWER(name) = LOWER(?)', (name,))
                existing_advisor = c.fetchone()
                
                if existing_advisor:
                    existing_id, existing_name, existing_website = existing_advisor
                    error_msg = f'Advisor "{existing_name}" already exists with website: {existing_website}. You can add additional links to this advisor from their details page.'
                    print(error_msg)
                    conn.close()
                    return render_template('add_advisor.html', error=error_msg)
                
                # Get current logged-in username
                username = session.get('username', 'Unknown')

                # Insert new advisor with firm, broker, and username
                c.execute('''INSERT INTO advisors (bd_id, name, location, website_url, firm, broker, added_by_username)
                            VALUES (?, ?, ?, ?, ?, ?, ?)''',
                         (user_id, name, location, website, firm, broker, username))
                
                advisor_id = c.lastrowid
                logger.info(f"Advisor {name} inserted with ID: {advisor_id}")
                
                conn.commit()
                conn.close()
            
            # Start website scan
            threading.Thread(target=monitor_advisor, args=(advisor_id,), daemon=True).start()
            
            print(f'Advisor {name} added successfully! Website scan started.')
            return redirect(url_for('advisor_details', advisor_id=advisor_id))
            
        except Exception as e:
            logger.error(f"Error adding advisor: {str(e)}")
            error_msg = f'Error adding advisor: {str(e)}'
            print(error_msg)
            return render_template('add_advisor.html', error=error_msg)
    
    return render_template('add_advisor.html')

@app.route('/admin/users')
def view_users():
    try:
        conn = get_db()  # This connects to PostgreSQL
        cur = conn.cursor()
        cur.execute("SELECT id, username, created_at FROM users ORDER BY created_at DESC")
        users = cur.fetchall()
        cur.close()
        conn.close()
        
        html = "<h1>Users</h1><table border='1'><tr><th>ID</th><th>Username</th><th>Created</th></tr>"
        for user in users:
            html += f"<tr><td>{user[0]}</td><td>{user[1]}</td><td>{user[2]}</td></tr>"
        html += "</table>"
        return html
    except Exception as e:
        return f"Error: {e}"

@app.route('/advisor_details/<int:advisor_id>')
def advisor_details(advisor_id):
    from urllib.parse import urlparse
    import itertools

    advisor = None
    grouped_pages = {}

    conn = None
    c = None
    try:
        conn = get_db_connection()   # ‚Üê acquire inside try
        c = conn.cursor()

        # ---- Advisor (keep as tuple to match your template) ----
        c.execute('SELECT * FROM advisors WHERE id = %s', (advisor_id,))
        advisor = c.fetchone()
        if not advisor:
            app.logger.warning('Advisor not found: %s', advisor_id)
            return redirect(url_for('dashboard'))

        # ---- Introspect website_snapshots to build a stable 6-column SELECT ----
        c.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = current_schema() AND table_name = 'website_snapshots'
        """)
        ws_cols = {r[0] for r in c.fetchall()}

        def col_or_null(col, cast):
            return col if col in ws_cols else f'NULL::{cast} AS {col}'

        # [0]=page_url, [1]=page_title, [2]=content_hash, [3]=last_checked, [4]=compliance_checked_at, [5]=last_scan_checked
        snap_select = ", ".join([
            col_or_null('page_url', 'text'),
            col_or_null('page_title', 'text'),
            col_or_null('content_hash', 'text'),
            col_or_null('last_checked', 'timestamp'),
            col_or_null('compliance_checked_at', 'timestamp'),
            col_or_null('last_scan_checked', 'timestamp'),
        ])

        c.execute(f'''
            SELECT {snap_select}
            FROM website_snapshots
            WHERE advisor_id = %s
            ORDER BY page_url NULLS LAST
        ''', (advisor_id,))
        pages = c.fetchall()
        app.logger.info("DEBUG: Found %d pages for advisor %s", len(pages), advisor_id)

        # ---- Helpers ----
        def table_exists(name):
            c.execute("""
                SELECT EXISTS (
                  SELECT 1 FROM information_schema.tables
                  WHERE table_schema = current_schema() AND table_name = %s
                )
            """, (name,))
            return bool(c.fetchone()[0])

        # ---- Changes ----
        change_table = None
        for nm in ('changes','advisor_changes','changes_log','change_log','profile_change_log'):
            if table_exists(nm):
                change_table = nm
                break

        all_changes = []
        change_page_url_idx = None
        if change_table:
            c.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = current_schema() AND table_name = %s
            """, (change_table,))
            ch_cols = [r[0] for r in c.fetchall()]
            ts_col = next((x for x in ('timestamp','changed_at','created_at','updated_at','ts') if x in ch_cols), None)
            order_piece = f"ORDER BY {ts_col} DESC NULLS LAST" if ts_col else ""
            c.execute(f"SELECT * FROM {change_table} WHERE advisor_id = %s {order_piece}", (advisor_id,))
            all_changes = c.fetchall()
            desc = [d.name for d in c.description]
            change_page_url_idx = desc.index('page_url') if 'page_url' in desc else None

        app.logger.info("DEBUG: Found %d changes for advisor %s", len(all_changes), advisor_id)

        # ---- Compliance issues ----
        all_issues = []
        issue_page_url_idx = None
        if table_exists('compliance_issues'):
            c.execute("""
                SELECT column_name FROM information_schema.columns
                WHERE table_schema = current_schema() AND table_name = 'compliance_issues'
            """)
            ci_cols = [r[0] for r in c.fetchall()]
            ts_issue = 'detected_at' if 'detected_at' in ci_cols else ('created_at' if 'created_at' in ci_cols else None)
            order_piece = f"ORDER BY {ts_issue} DESC NULLS LAST" if ts_issue else ""
            c.execute(f"SELECT * FROM compliance_issues WHERE advisor_id = %s {order_piece}", (advisor_id,))
            all_issues = c.fetchall()
            desc = [d.name for d in c.description]
            issue_page_url_idx = desc.index('page_url') if 'page_url' in desc else None

        app.logger.info("DEBUG: Found %d issues for advisor %s", len(all_issues), advisor_id)

        # ---- Group by root domain ----
        grouped_pages = {}
        for page in pages:
            page_url = page[0]
            parsed = urlparse(page_url) if page_url else None
            root = f"{parsed.scheme}://{parsed.netloc}" if parsed else page_url or 'unknown'
            if root not in grouped_pages:
                grouped_pages[root] = {
                    'domain': parsed.netloc if parsed else root,
                    'pages': [],
                    'changes': [],
                    'issues': [],
                    'last_checked': None,
                    'last_compliance_check': None,
                    'last_scan_check': None
                }
            grouped_pages[root]['pages'].append(page)
            if page[3] and (not grouped_pages[root]['last_checked'] or page[3] > grouped_pages[root]['last_checked']):
                grouped_pages[root]['last_checked'] = page[3]
            if page[4] and (not grouped_pages[root]['last_compliance_check'] or page[4] > grouped_pages[root]['last_compliance_check']):
                grouped_pages[root]['last_compliance_check'] = page[4]
            if page[5] and (not grouped_pages[root]['last_scan_check'] or page[5] > grouped_pages[root]['last_scan_check']):
                grouped_pages[root]['last_scan_check'] = page[5]

        if all_changes:
            for ch in all_changes:
                page_url = ch[change_page_url_idx] if change_page_url_idx is not None else None
                if not page_url:
                    for val in ch:
                        if isinstance(val, str) and val.startswith('http'):
                            page_url = val
                            break
                if page_url:
                    parsed = urlparse(page_url)
                    root = f"{parsed.scheme}://{parsed.netloc}"
                    grouped_pages.setdefault(root, {
                        'domain': parsed.netloc,
                        'pages': [], 'changes': [], 'issues': [],
                        'last_checked': None, 'last_compliance_check': None, 'last_scan_check': None
                    })['changes'].append(ch)

        if all_issues:
            for iss in all_issues:
                page_url = iss[issue_page_url_idx] if issue_page_url_idx is not None else None
                if not page_url:
                    for val in iss:
                        if isinstance(val, str) and val.startswith('http'):
                            page_url = val
                            break
                if page_url:
                    parsed = urlparse(page_url)
                    root = f"{parsed.scheme}://{parsed.netloc}"
                    grouped_pages.setdefault(root, {
                        'domain': parsed.netloc,
                        'pages': [], 'changes': [], 'issues': [],
                        'last_checked': None, 'last_compliance_check': None, 'last_scan_check': None
                    })['issues'].append(iss)

        app.logger.info("DEBUG: Grouped pages structure:")
        for domain, data in itertools.islice(grouped_pages.items(), 1000):
            app.logger.info("  %s: %d pages, %d changes, %d issues",
                            domain, len(data['pages']), len(data['changes']), len(data['issues']))
            app.logger.info("    Last checked: %s", data['last_checked'])
            app.logger.info("    Last compliance check: %s", data['last_compliance_check'])
            app.logger.info("    Last scan check: %s", data['last_scan_check'])

    except Exception as e:
        app.logger.error("advisor_details error: %s", e, exc_info=True)
        # Optional: fail soft instead of 500
        return redirect(url_for('dashboard'))

    finally:
        try:
            if c: c.close()
        except Exception:
            pass
        try:
            if conn: release_db_connection(conn)  # return to pool
        except Exception:
            pass

    return render_template('advisor_details.html',
                           advisor=advisor,
                           grouped_pages=grouped_pages)


@app.route('/compliance_issues/<int:advisor_id>')
def compliance_issues(advisor_id):
    # Remove login check
    # Get user ID
    user_email = session.get('user_email', 'default_user@example.com')
    user_id = hash(user_email)
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        
        if not advisor:
            flash('Advisor not found', 'error')
            return redirect(url_for('dashboard'))
        
        c.execute('''SELECT * FROM compliance_issues WHERE advisor_id = ? 
                    ORDER BY detected_at DESC''', (advisor_id,))
        issues = c.fetchall()
        
        conn.close()
    
    return render_template('compliance_issues.html', advisor=advisor, issues=issues)

@app.route('/change_details/<int:change_id>')
def change_details(change_id):
    logger.info(f"Looking for change with ID: {change_id}")
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Get the specific change
        c.execute('''SELECT c.*, a.name FROM changes c 
                    JOIN advisors a ON c.advisor_id = a.id 
                    WHERE c.id = ?''', (change_id,))
        change = c.fetchone()
        
        conn.close()
    
    if not change:
        logger.error(f"Change with ID {change_id} not found")
        return f'''
        <div style="text-align: center; margin-top: 50px; font-family: Arial;">
            <h3>Change Not Found</h3>
            <p>Change ID {change_id} was not found in the database.</p>
            <p>This usually means:</p>
            <ul style="text-align: left; display: inline-block;">
                <li>The website hasn't been scanned yet</li>
                <li>No changes have been detected on the website</li>
                <li>The change may have been from a previous database session</li>
            </ul>
            <br><br>
            <a href="/advisor-dashboard" style="background: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 4px;">Back to Dashboard</a>
        </div>
        '''
    
    # Generate highlighted diff
    before_content = change[3] or ""  # before_content
    after_content = change[4] or ""   # after_content
    
    # Create highlighted versions
    highlighted_before, highlighted_after = generate_highlighted_diff(before_content, after_content)
    
    # Add highlighted content to change data
    change_data = {
        'id': change[0],
        'advisor_id': change[1],
        'change_type': change[2],
        'before_content': change[3],
        'after_content': change[4],
        'timestamp': change[5],
        'description': change[6],
        'page_url': change[7],
        'changed_words': change[8],
        'advisor_name': change[9],
        'highlighted_before': highlighted_before,
        'highlighted_after': highlighted_after
    }
    
    return render_template('change_details.html', change=change_data)

def generate_highlighted_diff(before_text, after_text):
    """Generate HTML with highlighted differences between before and after text"""
    import difflib
    
    if not before_text and not after_text:
        return "No content", "No content"
    
    if not before_text:
        # Everything in after is new
        highlighted_before = '<span class="text-muted">No previous content</span>'
        highlighted_after = f'<span class="bg-success text-white p-1 rounded">{after_text[:1000]}...</span>' if len(after_text) > 1000 else f'<span class="bg-success text-white p-1 rounded">{after_text}</span>'
        return highlighted_before, highlighted_after
    
    if not after_text:
        # Everything in before was removed
        highlighted_before = f'<span class="bg-danger text-white p-1 rounded">{before_text[:1000]}...</span>' if len(before_text) > 1000 else f'<span class="bg-danger text-white p-1 rounded">{before_text}</span>'
        highlighted_after = '<span class="text-muted">Content removed</span>'
        return highlighted_before, highlighted_after
    
    # Split into words for better diff
    before_words = before_text.split()
    after_words = after_text.split()
    
    # Use difflib to find differences
    matcher = difflib.SequenceMatcher(None, before_words, after_words)
    
    highlighted_before_parts = []
    highlighted_after_parts = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            # Same content
            equal_text = ' '.join(before_words[i1:i2])
            highlighted_before_parts.append(equal_text)
            highlighted_after_parts.append(equal_text)
        elif tag == 'delete':
            # Removed from before
            removed_text = ' '.join(before_words[i1:i2])
            highlighted_before_parts.append(f'<span class="bg-danger text-white p-1 rounded" title="Removed">{removed_text}</span>')
        elif tag == 'insert':
            # Added to after
            added_text = ' '.join(after_words[j1:j2])
            highlighted_after_parts.append(f'<span class="bg-success text-white p-1 rounded" title="Added">{added_text}</span>')
        elif tag == 'replace':
            # Changed content
            old_text = ' '.join(before_words[i1:i2])
            new_text = ' '.join(after_words[j1:j2])
            highlighted_before_parts.append(f'<span class="bg-warning text-dark p-1 rounded" title="Changed from this">{old_text}</span>')
            highlighted_after_parts.append(f'<span class="bg-info text-white p-1 rounded" title="Changed to this">{new_text}</span>')
    
    # Join parts and truncate if too long
    before_html = ' '.join(highlighted_before_parts)
    after_html = ' '.join(highlighted_after_parts)
    
    # Truncate if too long (but preserve HTML tags)
    if len(before_html) > 2000:
        before_html = before_html[:2000] + '... <span class="text-muted">[truncated]</span>'
    if len(after_html) > 2000:
        after_html = after_html[:2000] + '... <span class="text-muted">[truncated]</span>'
    
    return before_html, after_html

@app.route('/scan_now')
def scan_now():
    def monitor_all_advisors():
        try:
            with db_lock:
                conn = get_db_connection_sqlite()
                c = conn.cursor()
                # Get ALL advisors, not just for a specific user
                c.execute('SELECT id, name FROM advisors WHERE website_url IS NOT NULL AND website_url != ""')
                advisors = c.fetchall()
                conn.close()
            
            # Process in batches of 5
            batch_size = 5
            for i in range(0, len(advisors), batch_size):
                batch = advisors[i:i + batch_size]
                threads = []
                
                # Start 5 threads simultaneously
                for advisor_id, advisor_name in batch:
                    thread = threading.Thread(target=monitor_advisor, args=(advisor_id,))
                    threads.append(thread)
                    thread.start()
                
                # Wait for all 5 threads in this batch to complete
                for thread in threads:
                    thread.join()
                
                # Optional: Small delay between batches
                if i + batch_size < len(advisors):  # Don't sleep after the last batch
                    time.sleep(2)
                    
        except Exception as e:
            logger.error(f"Error in monitor_all_advisors: {str(e)}")
    
    threading.Thread(target=monitor_all_advisors, daemon=True).start()
    print(f'Website scan started for all advisors in batches of 5!')
    return redirect(url_for('dashboard'))


    
@app.route('/run_compliance_check_page', methods=['POST'])
def run_compliance_check_page():
    """Run compliance check for a single page or specific post/tweet"""
    from datetime import datetime
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data received'}), 400
            
        advisor_id = data.get('advisor_id')
        page_url = data.get('page_url')
        tweet_id = data.get('tweet_id')
        post_id = data.get('post_id')
        
        logger.info(f"Compliance check request - Advisor ID: {advisor_id}, Page: {page_url}, Tweet ID: {tweet_id}, Post ID: {post_id}")
        
        if not advisor_id or not page_url:
            return jsonify({'success': False, 'error': 'Missing advisor_id or page_url'})
        
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Check if advisor exists
            c.execute('SELECT name FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            if not advisor:
                logger.error(f"Advisor {advisor_id} not found in database")
                conn.close()
                return jsonify({'success': False, 'error': f'Advisor {advisor_id} not found'})
            
            # Get page content
            c.execute('SELECT content_text FROM website_snapshots WHERE advisor_id = ? AND page_url = ?', 
                     (advisor_id, page_url))
            page_data = c.fetchone()
            
            if not page_data:
                logger.error(f"Page {page_url} not found for advisor {advisor_id}")
                conn.close()
                return jsonify({'success': False, 'error': f'Page {page_url} not found'})
            
            content_text = page_data[0]
            conn.close()  # Close connection here after getting data
        
        # Determine what content to check
        text_to_check = content_text  # Default to full content
        
        # Handle specific LinkedIn posts
        if post_id and 'linkedin.com' in page_url.lower():
            post_sections = content_text.split('Post ')
            specific_post_content = None
            
            for section in post_sections:
                if section.strip().startswith(post_id):
                    lines = section.strip().split('\n')
                    post_text = []
                    for line in lines[1:]:
                        if line.strip():
                            post_text.append(line.strip())
                    specific_post_content = '\n'.join(post_text)
                    break
            
            if specific_post_content:
                text_to_check = specific_post_content
                logger.info(f"Running compliance check on specific LinkedIn post {post_id}: {len(text_to_check)} characters")
            else:
                return jsonify({'success': False, 'error': f'LinkedIn post {post_id} not found'})
        
        # Handle specific Twitter posts
        elif tweet_id and ('x.com' in page_url.lower() or 'twitter.com' in page_url.lower()):
            tweet_sections = content_text.split('Tweet ')
            specific_tweet_content = None
            
            for section in tweet_sections:
                if section.strip().startswith(tweet_id):
                    lines = section.strip().split('\n')
                    tweet_text = []
                    for line in lines[1:]:
                        if line.strip():
                            tweet_text.append(line.strip())
                    specific_tweet_content = '\n'.join(tweet_text)
                    break
            
            if specific_tweet_content:
                text_to_check = specific_tweet_content
                logger.info(f"Running compliance check on specific tweet {tweet_id}: {len(text_to_check)} characters")
            else:
                return jsonify({'success': False, 'error': f'Tweet {tweet_id} not found'})
        
        # Handle specific Facebook posts
        elif post_id and ('facebook.com' in page_url.lower() or 'fb.com' in page_url.lower()):
            post_sections = content_text.split('Post ')
            specific_post_content = None
            
            for section in post_sections:
                if section.strip().startswith(post_id):
                    lines = section.strip().split('\n')
                    post_text = []
                    for line in lines[1:]:
                        if line.strip():
                            post_text.append(line.strip())
                    specific_post_content = '\n'.join(post_text)
                    break
            
            if specific_post_content:
                text_to_check = specific_post_content
                logger.info(f"Running compliance check on specific Facebook post {post_id}: {len(text_to_check)} characters")
            else:
                return jsonify({'success': False, 'error': f'Facebook post {post_id} not found'})
        
        logger.info(f"Running compliance check on {len(text_to_check)} characters of content")
        
        # NOW run compliance check with the extracted content
        if tweet_id:
            compliance_result = perform_whole_text_compliance_check(text_to_check)
        elif post_id:
            compliance_result = perform_compliance_check(text_to_check, page_num=1)
        else:
            compliance_result = perform_compliance_check(text_to_check, page_num=1)
        
        logger.info(f"Compliance check result: compliant={compliance_result.get('compliant', 'unknown')}")
        
        # Store compliance issues in database
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()

            # Store compliance check record AFTER getting the result
            result_status = 'compliant' if compliance_result.get("compliant") else 'non-compliant'
            post_type = 'tweet' if tweet_id else 'facebook_post' if 'facebook.com' in page_url.lower() else 'linkedin_post' if 'linkedin.com' in page_url.lower() else 'unknown'
            check_post_id = tweet_id or post_id
            current_local_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # ADD THIS LINE

            if check_post_id and post_type != 'unknown':
                c.execute('''INSERT INTO compliance_checks 
                            (advisor_id, page_url, post_type, post_id, checked_at, result)
                            VALUES (?, ?, ?, ?, ?, ?)''',
                         (advisor_id, page_url, post_type, check_post_id, current_local_time, result_status))

            # Clear existing issues - only for the specific tweet/post being checked
            if tweet_id:
                c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?', 
                         (advisor_id, page_url, f'%Tweet {tweet_id}%'))
                logger.info(f"Cleared existing issues for tweet {tweet_id}")
            elif post_id and 'facebook.com' in page_url.lower():
                c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?', 
                         (advisor_id, page_url, f'%Post {post_id}%'))
                logger.info(f"Cleared existing issues for Facebook post {post_id}")
            elif post_id and 'linkedin.com' in page_url.lower():
                c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?', 
                         (advisor_id, page_url, f'%LinkedIn Post {post_id}%'))
                logger.info(f"Cleared existing issues for LinkedIn post {post_id}")
            else:
                c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ?', 
                         (advisor_id, page_url))
                logger.info(f"Cleared all existing issues for page {page_url}")

            issues_added = 0
            
            # Store new compliance issues if any
            if not compliance_result.get("compliant"):
                flagged_instances = compliance_result.get("flagged_instances", [])
                logger.info(f"Found {len(flagged_instances)} flagged instances")
                
                for instance in flagged_instances:
                    flagged_text = instance.get('flagged_instance', '')
                    
                    if tweet_id:
                        flagged_text = f"Tweet {tweet_id}: {flagged_text}"
                    elif post_id and 'facebook.com' in page_url.lower():
                        flagged_text = f"Facebook Post {post_id}: {flagged_text}"
                    elif post_id and 'linkedin.com' in page_url.lower():
                        flagged_text = f"LinkedIn Post {post_id}: {flagged_text}"
                    
                    c.execute('''INSERT INTO compliance_issues 
                                (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                VALUES (?, ?, ?, ?, ?, ?)''',
                             (advisor_id, page_url, 
                              flagged_text,
                              instance.get('specific_compliant_alternative', ''), 
                              str(instance.get('confidence', '')), 
                              instance.get('rationale', '')))
                    issues_added += 1
                
                logger.info(f"üö® Added {issues_added} compliance issues for {page_url}")
            else:
                logger.info(f"‚úÖ No compliance issues found on {page_url}")
            
            
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            c.execute('''UPDATE website_snapshots 
                        SET compliance_checked_at = ? 
                        WHERE advisor_id = ? AND page_url = ?''',
                     (current_time, advisor_id, page_url))
            
            conn.commit()
            conn.close()
        
        # Return success message
        message = f"Compliance check completed. Found {issues_added} issues." if issues_added > 0 else "Compliance check completed. No issues found."
        
        return jsonify({
            'success': True, 
            'message': message, 
            'issues_count': issues_added,
            'reload_page': True
        })
        
    except Exception as e:
        logger.error(f"Error in run_compliance_check_page: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': f'An error occurred: {str(e)}'})
    
@app.route('/linkedin_post_compliance_issues/<int:advisor_id>')
def linkedin_post_compliance_issues(advisor_id):
    """Display all compliance issues for a specific LinkedIn post"""
    page_url = request.args.get('page_url')
    post_id = request.args.get('post_id')

    print(f"DEBUG: advisor_id={advisor_id}, page_url={page_url}, post_id={post_id}")
    
    if not page_url or not post_id:
        print(f"DEBUG: Missing parameters - page_url={page_url}, post_id={post_id}")
        flash('Invalid LinkedIn post parameters', 'error')
        return redirect(url_for('advisor_details', advisor_id=advisor_id))
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        
        # Get advisor info
        c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
        advisor = c.fetchone()
        
        if not advisor:
            flash('Advisor not found', 'error')
            return redirect(url_for('dashboard'))
        
        # Get all compliance issues for this specific LinkedIn post
        c.execute('''SELECT * FROM compliance_issues 
                    WHERE advisor_id = ? AND page_url = ? AND flagged_text LIKE ?
                    ORDER BY detected_at DESC''', 
                 (advisor_id, page_url, f'%LinkedIn Post {post_id}%'))
        issues = c.fetchall()
        
        conn.close()
    
    page_title = f"LinkedIn Post {post_id}"
    
    return render_template('linkedin_post_compliance_issues.html', 
                         advisor=advisor, 
                         issues=issues, 
                         page_url=page_url,
                         post_id=post_id,
                         page_title=page_title)

@app.route('/debug_advisor/<int:advisor_id>')
def debug_advisor(advisor_id):
    """Debug what's in the database for a specific advisor"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get advisor info
            c.execute('SELECT * FROM advisors WHERE id = ?', (advisor_id,))
            advisor = c.fetchone()
            
            # Get website snapshots
            c.execute('SELECT page_url, page_title, last_checked FROM website_snapshots WHERE advisor_id = ?', (advisor_id,))
            pages = c.fetchall()
            
            # Get compliance issues
            c.execute('SELECT * FROM compliance_issues WHERE advisor_id = ?', (advisor_id,))
            issues = c.fetchall()
            
            conn.close()
        
        return f"""
        <h3>Advisor {advisor_id} Debug Info:</h3>
        <p><strong>Advisor:</strong> {advisor}</p>
        <p><strong>Pages ({len(pages)}):</strong> {pages}</p>
        <p><strong>Compliance Issues ({len(issues)}):</strong> {issues}</p>
        <br><a href="/advisor-dashboard">Back to Dashboard</a>
        """
        
    except Exception as e:
        return f"Debug error: {str(e)}"
    


@app.route('/run_compliance_check/<int:advisor_id>')
def run_compliance_check(advisor_id):
    # Remove login check
    # Get user ID
    user_email = session.get('user_email', 'default_user@example.com')
    user_id = hash(user_email)
    
    with db_lock:
        conn = get_db_connection_sqlite()
        c = conn.cursor()
        c.execute('SELECT name FROM advisors WHERE id = ? AND bd_id = ?', (advisor_id, user_id))
        advisor = c.fetchone()
        conn.close()
    
    if not advisor:
        flash('Advisor not found', 'error')
        return redirect(url_for('dashboard'))
    
    def run_compliance_check_bg():
        try:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            c.execute('SELECT page_url, content_text FROM website_snapshots WHERE advisor_id = ?', (advisor_id,))
            pages = c.fetchall()
            
            total_issues = 0
            
            for page_url, content_text in pages:
                # Fix: Use your existing compliance check function properly
                compliance_data = perform_compliance_check(content_text)  # Remove page_num parameter
                
                if not compliance_data.get("compliant"):
                    flagged_instances = compliance_data.get("flagged_instances", [])
                    
                    # Clear existing issues for this page first
                    c.execute('DELETE FROM compliance_issues WHERE advisor_id = ? AND page_url = ?', 
                             (advisor_id, page_url))
                    
                    for instance in flagged_instances:
                        c.execute('''INSERT INTO compliance_issues 
                                    (advisor_id, page_url, flagged_text, compliant_alternative, confidence, rationale)
                                    VALUES (?, ?, ?, ?, ?, ?)''',
                                 (advisor_id, page_url, 
                                  instance.get('flagged_instance', ''), 
                                  instance.get('specific_compliant_alternative', ''), 
                                  instance.get('confidence', ''), 
                                  instance.get('rationale', '')))
                        total_issues += 1
            
            c.execute('UPDATE advisors SET compliance_checked = TRUE WHERE id = ?', (advisor_id,))
            conn.commit()
            conn.close()
            
            logger.info(f"Compliance check completed for {advisor[0]}: {total_issues} issues found")
            
        except Exception as e:
            logger.error(f"Error in compliance check: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")  # Add full traceback
    
    threading.Thread(target=run_compliance_check_bg, daemon=True).start()
    
    print(f'Compliance check started for {advisor[0]}! Results will appear shortly.')  # Use print instead of flash
    return redirect(url_for('advisor_details', advisor_id=advisor_id))

@app.route('/debug_linkedin_refresh/<int:advisor_id>/<domain>')
def debug_linkedin_refresh(advisor_id, domain):
    """Debug LinkedIn refresh status"""
    try:
        with db_lock:
            conn = get_db_connection_sqlite()
            c = conn.cursor()
            
            # Get LinkedIn URL
            c.execute('''SELECT page_url, content_text, last_checked FROM website_snapshots 
                        WHERE advisor_id = ? 
                        AND page_url LIKE '%linkedin.com%' ''', (advisor_id,))
            linkedin_result = c.fetchone()
            
            conn.close()
            
            if linkedin_result:
                linkedin_url, content_text, last_checked = linkedin_result
                
                # Parse existing posts
                existing_posts = []
                if content_text and "LINKEDIN PROFILE:" in content_text:
                    post_sections = content_text.split('Post ')
                    for section in post_sections:
                        if section.strip() and section.strip() != '':
                            lines = section.strip().split('\n')
                            if len(lines) > 1:
                                post_id = lines[0].strip()
                                existing_posts.append(post_id)
                
                debug_info = f"""
                <h3>LinkedIn Debug for Advisor {advisor_id}</h3>
                <p><strong>URL:</strong> {linkedin_url}</p>
                <p><strong>Last Checked:</strong> {last_checked}</p>
                <p><strong>Content Length:</strong> {len(content_text)} characters</p>
                <p><strong>Existing Posts Found:</strong> {len(existing_posts)}</p>
                <h4>Existing Post IDs:</h4>
                <ul>
                """
                
                for post_id in existing_posts[:10]:  # Show first 10
                    debug_info += f"<li>{post_id}</li>"
                
                debug_info += """
                </ul>
                <h4>Recent Content Preview:</h4>
                <pre style="background:#f5f5f5;padding:10px;max-height:300px;overflow:auto;">
                """
                debug_info += content_text[:1000] + "..."
                debug_info += """
                </pre>
                <p><a href="/refresh_linkedin/""" + str(advisor_id) + "/" + domain + """">Try Refresh Again</a></p>
                <p><a href="/domain_details/""" + str(advisor_id) + """?domain=""" + domain + """">Back to Domain</a></p>
                """
                
                return debug_info
            else:
                return f"No LinkedIn profile found for advisor {advisor_id}"
                
    except Exception as e:
        return f"Debug error: {str(e)}"

@app.route('/active-text')
def active_text():
    return render_template('active.html')

@app.route('/sentence-compliance-check', methods=['POST'])
def sentence_compliance_check():
    try:
        data = request.json
        sentence = data.get('sentence', '').strip()
        
        if not sentence:
            return jsonify({'error': 'No sentence provided'})
        
        # Step 1: Use your existing BERT endpoint
        try:
            bert_response = requests.post(
                f'http://localhost:{request.environ.get("SERVER_PORT", "5000")}/bert_compliance_check',
                json={'text': sentence},
                headers={'Content-Type': 'application/json'}
            )
            
            if bert_response.status_code != 200:
                return jsonify({'error': 'BERT classification failed'})
            
            bert_result = bert_response.json()
            
        except Exception as e:
            return jsonify({'error': f'BERT model check failed: {str(e)}'})
        
        # If BERT says it's compliant, return clean result
        if not bert_result.get('is_non_compliant', False):
            return jsonify({
                'is_compliant': True,
                'sentence': sentence,
                'bert_check': True
            })
        
        # Step 2: If BERT flags it, verify with DeepSeek
        deepseek_payload = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": """You are a FINRA compliance expert. Analyze the provided sentence for regulatory violations.

If the sentence is NON-COMPLIANT, respond in this exact JSON format:
{
    "is_compliant": false,
    "violations": [
        {
            "text": "exact problematic phrase",
            "alternative": "short compliant alternative",
            "reason": "brief violation explanation"
        }
    ]
}

If the sentence is COMPLIANT, respond:
{
    "is_compliant": true
}

Focus on: guarantees, promises of returns, misleading statements, unsubstantiated claims."""
                },
                {
                    "role": "user",
                    "content": f"Analyze this sentence: '{sentence}'"
                }
            ],
            "temperature": 0.1
        }
        
        deepseek_response = requests.post(
            'https://api.deepseek.com/chat/completions',
            headers={
                'Authorization': f'Bearer {DEEPSEEK_API_KEY}',
                'Content-Type': 'application/json'
            },
            json=deepseek_payload
        )
        
        if deepseek_response.status_code != 200:
            return jsonify({'error': 'DeepSeek verification failed'})
        
        deepseek_content = deepseek_response.json()['choices'][0]['message']['content']
        
        try:
            import json
            import re
            json_match = re.search(r'\{.*\}', deepseek_content, re.DOTALL)
            if json_match:
                deepseek_result = json.loads(json_match.group())
            else:
                deepseek_result = json.loads(deepseek_content)
        except:
            return jsonify({'error': 'Failed to parse DeepSeek response'})
        
        return jsonify({
            'is_compliant': deepseek_result.get('is_compliant', True),
            'sentence': sentence,
            'violations': deepseek_result.get('violations', []),
            'bert_flagged': True,
            'deepseek_verified': True
        })
        
    except Exception as e:
        return jsonify({'error': str(e)})
    
    
@app.route('/bert_gpt_test', methods=['POST'])
def bert_gpt_test():
    """Test endpoint that uses BERT + GPT verification identical to main function"""
    request_id = register_request()
    logger.info(f"Starting request {request_id}: bert_gpt_test")
    
    data = request.get_json()
    custom_text = data.get("text", "").strip()
    
    if not custom_text:
        mark_request_complete(request_id)
        return jsonify({"error": "No text provided.", "request_id": request_id}), 400
    
    try:
        # Use EXACTLY the same function as your main compliance checking
        compliance_data = perform_compliance_check(custom_text)
        
        # Return the raw results from perform_compliance_check
        response_data = {
            "compliant": compliance_data.get("compliant", True),
            "flagged_instances": compliance_data.get("flagged_instances", []),
            "request_id": request_id
        }
        
        mark_request_complete(request_id)
        return jsonify(response_data), 200
        
    except Exception as e:
        logger.error(f"Error in BERT+GPT test: {e}")
        mark_request_complete(request_id)
        return jsonify({"error": str(e), "request_id": request_id}), 500

@app.route('/api/usage-analytics', methods=['GET'])
def get_usage_analytics():
    """Get detailed usage analytics for the current user"""
    if 'user_email' not in session:
        return jsonify({'error': 'Not logged in'}), 401
    
    user_email = session['user_email']
    users = load_users()
    user = next((u for u in users if u['email'] == user_email), None)
    
    if not user:
        return jsonify({'error': 'User not found'}), 404
    
    return jsonify({
        'user_email': user_email,
        'total_cost': user.get('User Cost', 0.0),
        'session_stats': session_token_usage if 'session_token_usage' in globals() else {}
    })

@app.route('/api/process/<filename>', methods=['GET'])
def api_process_file(filename):
    logger.info(f"API PROCESS FILE ENDPOINT CALLED FOR: {filename}")
    """API version of process_file that returns JSON for Locust testing"""
    # Generate a request ID
    request_id = register_request()
    logger.info(f"Starting API request {request_id}: api_process_file for {filename}")
    
    global processed_files
    logger.info(f"Processing file API: {filename}")
    
    # Check if file exists
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    if not os.path.isfile(file_path):
        mark_request_complete(request_id)
        return jsonify({
            "success": False, 
            "error": "File not found",
            "request_id": request_id
        }), 404
    
    # Check if already processed (use cache)
    if filename in processed_files:
        logger.info(f"Returning cached API results for {filename}.")
        mark_request_complete(request_id)
        return jsonify({
            "success": True,
            "cached": True,
            "text_length": len(processed_files[filename]['text']),
            "sentence_count": processed_files[filename].get('results', 0),
            "flagged_instances": len(processed_files[filename]['finra_analysis']),
            "request_id": request_id
        })
    
    try:
        # Extract text by page based on file type
        if file_path.endswith('.pdf'):
            text_by_page = extract_text_from_pdf(file_path)
        elif file_path.endswith('.docx'):
            text_by_page = extract_text_from_docx(file_path)
        elif file_path.endswith('.mp4'):
            logger.info(f"Transcribing MP4 file: {file_path}")
            text_by_page = transcribe_mp4(file_path)
        else:
            logger.info(f"Unsupported file type: {file_path}")
            mark_request_complete(request_id)
            return jsonify({
                "success": False, 
                "error": "Unsupported file type",
                "request_id": request_id
            }), 400
        
        # Get all text
        all_text = " ".join([page["text"] for page in text_by_page])
        
        # Clean the text - remove bullet points and other special characters
        cleaned_text = re.sub(r'[‚Ä¢\u2022]', '', all_text)  # Remove bullet points
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()  # Clean up whitespace
        
        # Split into sentences
        sentences = split_into_sentences(cleaned_text)
        results = len(sentences)
        
        # Process for compliance
        disclosures = DISCLOSURE_WORDS
        finra_analysis = []
        
        for page in text_by_page:
            page_text = page["text"].replace('‚Ä¢', '').replace('\u2022', '')  # Remove bullet points
            page_num = page.get("page")
            
            # Pass the cleaned text string directly
            compliance_data = perform_compliance_check(page_text, page_num)

            if not compliance_data.get("compliant"):
                instances = compliance_data.get("flagged_instances", [])
                for instance in instances:
                    if isinstance(instance, dict) and instance.get("flagged_instance"):
                        # Only add non-empty, valid instances
                        if instance["flagged_instance"].strip() and \
                           instance["flagged_instance"] != "‚Ä¢" and \
                           len(instance["flagged_instance"]) > 1:
                            finra_analysis.append(instance)
        
        # Cache results
        processed_files[filename] = {
            'text': cleaned_text,
            'text_by_page': text_by_page,
            'revised_text': "",
            'disclosures': disclosures,
            'sliced_disclosures': list(disclosures.values())[:5],
            'finra_analysis': finra_analysis,
            'results': results
        }
        
        # Log token usage summary
        log_session_token_summary()
        
        # Set status flags
        final_check_status[filename] = True
        logger.info("API process complete!")
        
        # Mark request as complete
        mark_request_complete(request_id)
        
        # Return JSON response with results
        return jsonify({
            "success": True,
            "cached": False,
            "text_length": len(cleaned_text),
            "sentence_count": results,
            "flagged_instances": len(finra_analysis),
            "request_id": request_id
        })
        
    except Exception as e:
        logger.error(f"Error in API process_file: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Mark request as complete even on error
        mark_request_complete(request_id)
        
        return jsonify({
            "success": False,
            "error": str(e),
            "request_id": request_id
        }), 500

@app.route('/request_status/<request_id>', methods=['GET'])
def check_request_status(request_id):
    """Check if a specific request has completed processing"""
    is_complete = is_request_complete(request_id)
    return jsonify({"complete": is_complete})

@app.route('/logout', methods=['POST'])
def logout():
    # Clear session
    session.clear()
    
    # Create response for redirect
    response = make_response(jsonify({'status': 'success'}))
    
    # Set cookie expiration to past date
    response.set_cookie('session', '', expires=0)
    
    # Add cache-control headers to prevent caching
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    
    return response

@app.route('/disclaimer')
def disclaimer():
    return render_template('disclaimer.html')

@app.route('/documentation')
def documentation():
    return render_template('documentation.html')

@app.route('/blog')
def blog():
    return render_template('blog.html')

# Routes for Articles in Blog
@app.route('/blog/sec-marketing-rule')
def sec_marketing_rule():
    return render_template('blog_sec_marketing_rule.html')

@app.route('/blog/cryptocurrency-compliance')
def cryptocurrency_compliance():
    return render_template('blog_cryptocurrency_compliance.html')

@app.route('/blog/social-media-strategy')
def social_media_strategy():
    return render_template('blog_social_media_strategy.html')

@app.route('/blog/why')
def why():
    return render_template('why.html')

@app.route('/blog/advisor-marketing-rules')
def advisor_marketing_rules():
    return render_template('advisor_marketing_rules.html')

@app.route('/blog/compliance-case-studies')
def compliance_case_studies():
    return render_template('compliance_case_studies.html')





@app.route('/case-studies')
def case_studies():
    return render_template('case_studies.html')

@app.route('/bert_compliance_check', methods=['POST'])
def bert_compliance_check():
    # Generate a request ID
    request_id = register_request()
    logger.info(f"Starting request {request_id}: bert_compliance_check")
    
    torch.manual_seed(42)
    data = request.get_json()
    custom_text = data.get("text", "").strip()
    
    if not custom_text:
        logger.error("No text provided for BERT compliance check.")
        mark_request_complete(request_id)  # Mark complete on error
        return jsonify({"error": "No text provided.", "request_id": request_id}), 400
    
    try:
        # Use the centralized perform_compliance_check function 
        compliance_data = perform_compliance_check(custom_text)
        
        # Format the response to match the expected structure for the frontend
        is_compliant = compliance_data.get("compliant", True)
        flagged_instances = compliance_data.get("flagged_instances", [])
        
        if isinstance(flagged_instances, list) and len(flagged_instances) > 0:
            # Found non-compliant text
            first_instance = flagged_instances[0]
            
            # Get the original values
            flagged_instance = first_instance.get("flagged_instance", "")
            specific_alternative = first_instance.get("specific_compliant_alternative", "")
            
            # Check if the alternative is our error message
            if specific_alternative == "The server is unable to evaluate your request at this time due to unauthorized language usage or poor word selection. Please modify your submission and try again.":
                # Use the error message directly without additional formatting
                reformatted_rationale = specific_alternative
            else:
                # Use the normal format for regular alternatives
                reformatted_rationale = f"The statement \"{flagged_instance}\" contains language that may be considered absolute or promissory, presented without proper qualifiers. A more compliant version could be \"{specific_alternative}\"."

            response_data = {
                "is_non_compliant": not is_compliant,
                "verification_response": reformatted_rationale,  # Use the reformatted rationale
                "bert_prediction": {
                    "is_non_compliant": not is_compliant,
                    "confidence": first_instance.get("confidence", "0.00")
                },
                "sentence": custom_text,
                "detailed_results": {
                    "overall_non_compliant": not is_compliant,
                    "flagged_instances": flagged_instances
                },
                "request_id": request_id  # Add the request ID
            }
            
            # Mark the request as complete
            mark_request_complete(request_id)
            
            return jsonify(response_data), 200
        else:
            # No compliance issues found
            response_data = {
                "is_non_compliant": False,
                "verification_response": "No compliance issues found.",
                "bert_prediction": {
                    "is_non_compliant": False,
                    "confidence": "0.00"
                },
                "sentence": custom_text,
                "detailed_results": {
                    "overall_non_compliant": False,
                    "flagged_instances": []
                },
                "request_id": request_id  # Add the request ID
            }
            
            # Mark the request as complete
            mark_request_complete(request_id)
            
            return jsonify(response_data), 200
        
    except Exception as e:
        logger.error(f"Error during BERT compliance check: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Mark complete even on error
        mark_request_complete(request_id)
        
        return jsonify({
            "error": "An error occurred during compliance checking.",
            "message": str(e),
            "request_id": request_id  # Add the request ID
        }), 500    

    
def create_pdf_from_text(text, output_path):
    """Create a PDF file from transcribed text with better formatting"""
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    from reportlab.lib.units import inch
    from reportlab.lib.enums import TA_JUSTIFY
    
    # Create a custom style for the transcription text
    doc = SimpleDocTemplate(output_path, pagesize=letter, 
                           rightMargin=72, leftMargin=72,
                           topMargin=72, bottomMargin=72)
    
    styles = getSampleStyleSheet()
    styles.add(ParagraphStyle(name='Justify', 
                              fontName='Helvetica',
                              fontSize=12,
                              leading=14,
                              alignment=TA_JUSTIFY,
                              spaceAfter=10))
    
    story = []
    
    # Add a title
    story.append(Paragraph("Transcription", styles['Heading1']))
    story.append(Spacer(1, 0.25*inch))
    
    # Add date and time of transcription
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    story.append(Paragraph(f"Generated on: {timestamp}", styles['Italic']))
    story.append(Spacer(1, 0.25*inch))
    
    # Split text into paragraphs - respect both double newlines and sentence endings
    parts = re.split(r'\n\n|\r\n\r\n', text)
    
    for part in parts:
        if not part.strip():
            continue
            
        # Further split long paragraphs by sentences for better readability
        if len(part) > 300:  # If paragraph is very long
            sentences = re.split(r'(?<=[.!?])\s+', part)
            current_paragraph = ""
            
            for sentence in sentences:
                if len(current_paragraph) + len(sentence) > 300:
                    # Add the current paragraph to the story
                    if current_paragraph:
                        story.append(Paragraph(current_paragraph, styles['Justify']))
                    current_paragraph = sentence
                else:
                    if current_paragraph:
                        current_paragraph += " " + sentence
                    else:
                        current_paragraph = sentence
            
            # Add any remaining text
            if current_paragraph:
                story.append(Paragraph(current_paragraph, styles['Justify']))
        else:
            # Add regular paragraph
            story.append(Paragraph(part, styles['Justify']))
        
        # Add space after each paragraph
        story.append(Spacer(1, 0.1*inch))
    
    # Build the PDF
    doc.build(story)
    logger.info(f"Created formatted PDF from transcribed text at: {output_path}")


@app.route('/process_video', methods=['POST'])
def process_video():
    data = request.get_json()
    filename = data.get('filename')
    
    if not filename or not filename.endswith('.mp4'):
        return jsonify({'error': 'Invalid or missing MP4 filename'}), 400
    
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    if not os.path.isfile(file_path):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        # Transcribe the video file
        logger.info(f"Processing video file: {filename}")
        text_by_page = transcribe_mp4(file_path)
        all_text = " ".join([page["text"] for page in text_by_page])
        
        # Create a PDF from the transcription
        base_name = os.path.splitext(filename)[0]
        pdf_filename = f"{base_name}_transcription.pdf"
        pdf_path = os.path.join(app.config['UPLOAD_FOLDER'], pdf_filename)
        
        # Create PDF from transcribed text
        create_pdf_from_text(all_text, pdf_path)
        
        # Store the transcription results in memory for access as-is
        processed_files[filename] = {
            'text': all_text,
            'text_by_page': text_by_page,
            'revised_text': "",
            'disclosures': DISCLOSURE_WORDS,
            'sliced_disclosures': list(DISCLOSURE_WORDS.values())[:5] if DISCLOSURE_WORDS else [],
            'finra_analysis': [],
            'results': len(split_into_sentences(all_text))
        }
        
        # Mark processing as complete
        final_check_status[filename] = True
        logger.info(f"MP4 transcription completed for {filename}")
        
        # Now process the PDF through the existing pipeline by redirecting
        return jsonify({
            'success': True,
            'message': 'Video processed and converted to PDF for analysis',
            'redirect': f'/process/{pdf_filename}'  # Redirect to process the PDF instead
        })
    except Exception as e:
        logger.error(f"Error processing video: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': f"Error processing video: {str(e)}"
        }), 500

    

@app.route('/verification_only', methods=['POST'])
def verification_only():
    data = request.get_json()
    custom_text = data.get("text", "").strip()
    
    if not custom_text:
        logger.error("No text provided for quick verification check.")
        return jsonify({"error": "No text provided."}), 400
    
    try:
        # For quick verification, analyze the entire text as one unit
        # Don't split into sentences to avoid losing context
        logger.info(f"Verifying complete text: {custom_text}")
        
        # Then modify your verification prompt by adding this section:
        skepticism_words = load_skepticism_words()
        skepticism_instruction = f"""
        SPECIAL ATTENTION: Apply higher skepticism when analyzing text containing these words in financial/business contexts: {', '.join(skepticism_words)}

        These words often indicate promissory or absolute language when used in financial communications.
        """
                    
        # Use the original verification prompt for individual instance
        verification_prompt = f"""
        {skepticism_instruction}

Determine if this text violates FINRA's communication rules by being false, misleading, promissory, exaggerated, or contains profanity:

"{custom_text}"

Answer with ONLY "YES" or "NO", followed by a brief explanation.
"YES" means it IS non-compliant or contains profanity.
"NO" means it IS compliant.

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)



CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.

--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant


--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
        
        verification_response = call_deepseek_api(verification_prompt)
        logger.info(f"Deepseek verification response: {verification_response}")
        
        # Check if Deepseek confirms this is non-compliant
        is_non_compliant = False
        first_word = ""
        if verification_response:
            first_word = verification_response.strip().split()[0].upper() if verification_response.strip() else ""
            is_non_compliant = first_word == "YES"
        
        # If we found a violation, return immediately
        return jsonify({
            "verification_response": verification_response,
            "is_non_compliant": is_non_compliant
        }), 200
        
    except Exception as e:
        logger.error(f"Error during quick verification: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({
            "error": "An error occurred during quick verification check.",
            "message": str(e)
        }), 500


@app.route('/intro')

def intro_page():

    return render_template('intro.html')



@app.route('/api/remove-user-completely', methods=['POST'])
def permanently_remove_user_from_system():
    try:
        # Get current user info based on how your app is structured
        user_email = session.get('user_email')
        
        if not user_email:
            return jsonify({"success": False, "error": "Not logged in"}), 401
        
        # Load all users to find current user
        with open('users.json', 'r') as f:
            users = json.load(f)
        
        # Find current user to check admin status
        current_user = None
        for user in users:
            if user.get('email') == user_email:
                current_user = user
                break
        
        # Check if user is admin
        if not current_user or current_user.get("Administrator Access NEW") != "Yes":
            return jsonify({"success": False, "error": "Unauthorized access"}), 403
        
        # Get the user email from the request
        data = request.json
        email_to_remove = data.get('email')
        
        if not email_to_remove:
            return jsonify({"success": False, "error": "Email is required"}), 400
        
        # Don't allow removing yourself
        if email_to_remove == user_email:
            return jsonify({"success": False, "error": "You cannot remove your own account"}), 400
        
        # Find and remove the user
        user_found = False
        for i, user in enumerate(users):
            if user.get('email') == email_to_remove:
                users.pop(i)
                user_found = True
                break
        
        if not user_found:
            return jsonify({"success": False, "error": "User not found"}), 404
        
        # Save the updated user data
        with open('users.json', 'w') as f:
            json.dump(users, f, indent=4)
        
        return jsonify({"success": True, "message": "User permanently removed from system"})
    
    except Exception as e:
        app.logger.error(f"Error removing user: {str(e)}")
        return jsonify({"success": False, "error": f"An error occurred: {str(e)}"}), 500
    
def get_user_data(email):
    """Retrieve user data from users.json file by email"""
    try:
        with open('users.json', 'r') as f:
            users_data = json.load(f)
            
        for user in users_data:
            if user.get('email') == email:
                return user
                
        return None  # User not found
    except Exception as e:
        print(f"Error reading user data: {str(e)}")
        return None

@app.route('/admin-access')
def admin_access_page():
    # Check if the user is logged in and is an admin
    user_email = session.get('user_email')
    if not user_email:
        return redirect('/login')
    
    # Get the user's data
    user_data = get_user_data(user_email)
    if not user_data or user_data.get("Administrator Access NEW") != "Yes":
        # Redirect non-admins to the profile page
        return redirect('/profile')
    
    return render_template('admin-access.html')

@app.route('/api/all-users', methods=['GET'])
def get_all_users():
    # Check if the user is logged in and is an admin
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'error': 'User not logged in'}), 401
    
    user_data = get_user_data(user_email)
    if not user_data or user_data.get("Administrator Access NEW") != "Yes":
        return jsonify({'error': 'Unauthorized access'}), 403
    
    # Get all users from the JSON file
    users = []
    try:
        with open('users.json', 'r') as f:
            users_data = json.load(f)
            # Return a simplified version with only necessary fields
            users = [
                {
                    'fullName': user.get('fullName', ''),
                    'email': user.get('email', ''),
                    'Administrator Access NEW': user.get('Administrator Access NEW', 'No'),
                    'User Cost': user.get('User Cost', 0.00)  # Include User Cost field
                }
                for user in users_data
            ]
    except Exception as e:
        return jsonify({'error': f'Failed to load users: {str(e)}'}), 500
    
    return jsonify(users)


@app.route('/api/update-admin-access', methods=['POST'])
def update_admin_access():
    # Check if the user is logged in and is an admin
    user_email = session.get('user_email')
    if not user_email:
        return jsonify({'error': 'User not logged in'}), 401
    
    current_user = get_user_data(user_email)
    if not current_user or current_user.get("Administrator Access NEW") != "Yes":
        return jsonify({'error': 'Unauthorized access'}), 403
    
    # Get request data
    data = request.json
    target_email = data.get('email')
    admin_access = data.get('adminAccess')  # "Yes" or "No"
    
    if not target_email or admin_access not in ["Yes", "No"]:
        return jsonify({'error': 'Invalid request parameters'}), 400
    
    # Update the user's admin access in the users.json file
    try:
        with open('users.json', 'r') as f:
            users_data = json.load(f)
        
        # Find the user and update their admin access
        user_found = False
        for user in users_data:
            if user.get('email') == target_email:
                user["Administrator Access NEW"] = admin_access
                user_found = True
                break
        
        if not user_found:
            return jsonify({'error': 'User not found'}), 404
        
        # Save the updated users data
        with open('users.json', 'w') as f:
            json.dump(users_data, f, indent=4)
        
        return jsonify({'success': True})
    
    except Exception as e:
        return jsonify({'error': f'Failed to update admin access: {str(e)}'}), 500
    
# Store password reset tokens with expiration
password_reset_tokens = {}  # token -> {email, expiry}

@app.route('/reset-password', methods=['GET'])
def reset_password_page():
    return render_template('reset_password.html')

def generate_reset_token():
    """Generate a unique token for password reset"""
    return str(uuid.uuid4())

def send_password_reset_email(email, reset_link):
    """Send password reset email using SendGrid"""
    try:
        # Configure message
        message = Mail(
            from_email='riley.r.giauque@gmail.com',  # Must be verified in SendGrid
            to_emails=email,
            subject='Password Reset Request',
            html_content=f'''
                <div style="font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; padding: 20px; border: 1px solid #e0e0e0; border-radius: 5px;">
                    <h2 style="color: #004e98;">Password Reset Request</h2>
                    <p>You have requested to reset your password. Click the link below to set a new password:</p>
                    <p><a href="{reset_link}" style="display: inline-block; padding: 10px 20px; background-color: #00aaff; color: #ffffff; text-decoration: none; border-radius: 5px;">Reset Password</a></p>
                    <p>If you did not request this reset, please ignore this email.</p>
                    <p>This link will expire in 24 hours.</p>
                </div>
            '''
        )
        
        # Send email
        sg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))
        response = sg.send(message)
        
        logger.info(f"Reset email sent to {email}, status code: {response.status_code}")
        return True
    except Exception as e:
        logger.error(f"Error sending password reset email: {e}")
        return False
    
@app.route('/reset-password', methods=['POST'])
def reset_password_request():
    data = request.get_json()
    email = data.get('email')
    
    if not email:
        return jsonify({'error': 'Email is required'}), 400
    
    # Check if user exists
    users = load_users()
    user = next((u for u in users if u['email'] == email), None)
    
    if not user:
        # For security reasons, don't reveal that the email doesn't exist
        # Instead, pretend we sent an email
        return jsonify({'message': 'If your email exists in our system, you will receive a password reset link.'}), 200
    
    # Generate a token and set expiration time (24 hours from now)
    token = generate_reset_token()
    expiry = datetime.now() + timedelta(hours=24)
    
    # Store the token
    password_reset_tokens[token] = {
        'email': email,
        'expiry': expiry
    }
    
    # Create reset link
    reset_link = f"{request.host_url}reset?token={token}"
    
    # Send the reset email (simulation)
    success = send_password_reset_email(email, reset_link)
    
    if success:
        return jsonify({'message': 'Password reset link has been sent to your email.'}), 200
    else:
        return jsonify({'error': 'Failed to send reset email. Please try again later.'}), 500

@app.route('/reset', methods=['GET'])
def reset_password_form():
    token = request.args.get('token')
    
    if not token or token not in password_reset_tokens:
        return render_template('reset_error.html', error="Invalid or expired reset link"), 400
    
    token_data = password_reset_tokens[token]
    
    # Check if token is expired
    if datetime.now() > token_data['expiry']:
        del password_reset_tokens[token]  # Clean up expired token
        return render_template('reset_error.html', error="Reset link has expired"), 400
    
    return render_template('reset_password2.html', token=token)

@app.route('/reset', methods=['POST'])
def handle_password_reset():
    token = request.form.get('token')
    new_password = request.form.get('new_password')
    confirm_password = request.form.get('confirm_password')
    
    if not token or token not in password_reset_tokens:
        return render_template('reset_error.html', error="Invalid or expired reset link"), 400
    
    token_data = password_reset_tokens[token]
    
    # Check if token is expired
    if datetime.now() > token_data['expiry']:
        del password_reset_tokens[token]  # Clean up expired token
        return render_template('reset_error.html', error="Reset link has expired"), 400
    
    if not new_password or not confirm_password:
        return render_template('reset_password2.html', token=token, error="Please fill out all fields"), 400
    
    if new_password != confirm_password:
        return render_template('reset_password2.html', token=token, error="Passwords do not match"), 400
    
    # Update the user's password
    email = token_data['email']
    users = load_users()
    user = next((u for u in users if u['email'] == email), None)
    
    if user:
        user['newPassword'] = new_password  # Update password
        save_users(users)
        
        # Clean up the used token
        del password_reset_tokens[token]
        
        # Redirect to login page with success message
        return render_template('reset_password2.html', success=True)
    else:
        return render_template('reset_error.html', error="User not found"), 404    


@app.route('/process_saved_analysis', methods=['GET'])
def process_saved_analysis():
    # This will render a results page using data from the client
    # without needing the original file
    return render_template('results.html', 
                           load_from_client=True, 
                           finra_analysis=[])  # Empty placeholder

@app.route('/scan_file_for_disclosures', methods=['POST'])
def scan_file_for_disclosures():
    """Scan an uploaded file for keywords from disclosures.json and return matches by page"""
    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'No file uploaded'}), 400
        
    uploaded_file = request.files['file']
    if uploaded_file.filename == '':
        return jsonify({'success': False, 'error': 'No file selected'}), 400
    
    try:
        # Save the file temporarily
        filename = secure_filename(uploaded_file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        uploaded_file.save(file_path)
        
        # Extract text by page from the file
        if file_path.endswith('.pdf'):
            text_by_page = extract_text_from_pdf(file_path)
        elif file_path.endswith('.docx'):
            text_by_page = extract_text_from_docx(file_path)
        else:
            os.remove(file_path)  # Clean up
            return jsonify({'success': False, 'error': 'Unsupported file type'}), 400
        
        # Load disclosures from disclosures.json
        DISCLOSURES_FILE = os.path.join(os.path.dirname(__file__), 'disclosures.json')
        disclosures = {}
        
        if os.path.exists(DISCLOSURES_FILE):
            try:
                with open(DISCLOSURES_FILE, 'r') as f:
                    disclosures_data = json.load(f)
                    
                    # Handle different formats of disclosures.json
                    if isinstance(disclosures_data, list) and len(disclosures_data) > 0:
                        disclosures = disclosures_data[0] if isinstance(disclosures_data[0], dict) else {}
                    else:
                        disclosures = disclosures_data
                
                logger.info(f"Loaded {len(disclosures)} disclosures for file scanning")
            except Exception as e:
                logger.error(f"Error loading disclosures.json: {e}")
                # Fallback disclosures if loading fails
                disclosures = {
                    "mutual fund": "Mutual fund investing involves risk; principal loss is possible.",
                    "index fund": "Index funds are subject to market risk, including the possible loss of principal.",
                    "performance": "Past performance does not guarantee future results.",
                    "investing": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "investment": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "market": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "funds": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "returns": "Past performance is no guarantee of future results."
                }
        else:
            logger.info(f"Disclosures file not found for scanning")
            disclosures = {
                "mutual fund": "Mutual fund investing involves risk; principal loss is possible.",
                "index fund": "Index funds are subject to market risk, including the possible loss of principal.",
                "performance": "Past performance does not guarantee future results.",
                "investing": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "investment": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "market": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "funds": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "returns": "Past performance is no guarantee of future results."
            }
        
        # Map of page numbers to their specific disclosures
        page_specific_disclosures = {}
        
        # Process each page for specific disclosures
        for page_data in text_by_page:
            page_num = page_data.get('page', 0)
            page_text = page_data.get('text', '')
            
            if not page_text:
                continue
            
            logger.info(f"Scanning page {page_num} for disclosure keywords")
            
            # Convert to lowercase for case-insensitive matching
            page_text_lower = page_text.lower()
            page_matches = []
            
            # Check each keyword for a match in the page text
            for keyword, description in disclosures.items():
                keyword_lower = keyword.lower()
                
                # If the keyword is found in the page text
                if keyword_lower in page_text_lower:
                    # Add the match with both keyword and description
                    page_matches.append({
                        "keyword": keyword,
                        "description": description
                    })
                    logger.info(f"Page {page_num}: Matched keyword '{keyword}'")
            
            # Only add pages that have matches
            if page_matches:
                page_specific_disclosures[str(page_num)] = page_matches
        
        # Clean up the temporary file
        os.remove(file_path)
        
        return jsonify({
            'success': True,
            'page_specific_disclosures': page_specific_disclosures
        })
        
    except Exception as e:
        logger.error(f"Error scanning file for disclosures: {e}")
        # Try to clean up the file if it exists
        try:
            if 'file_path' in locals() and os.path.exists(file_path):
                os.remove(file_path)
        except:
            pass
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/training')
def training():
    return render_template('training.html')

# Add this new route to check if advisor type is selected
@app.route('/api/advisor-type-status', methods=['GET'])
def advisor_type_status():
    if 'user_email' not in session:
        return jsonify({'error': 'User not logged in'}), 401
    
    user_email = session['user_email']
    users = load_users()
    user = next((u for u in users if u['email'] == user_email), None)
    
    if user:
        has_advisor_type = 'advisorType' in user and user['advisorType']
        return jsonify({
            'hasAdvisorType': has_advisor_type,
            'signUpStatus': user.get('Sign Up Status', 'Not Yet')
        })
    
    return jsonify({'error': 'User not found'}), 404

@app.route('/get_page_specific_disclosures', methods=['POST'])
def get_page_specific_disclosures():
    """Get page-specific disclosures based on exact keyword matching for each page"""
    data = request.get_json()
    content_type = data.get('content_type', '')  # 'Book' or 'Presentation'
    text = data.get('text', '')  # Full document text
    original_text_by_page = data.get('original_text_by_page', [])  # Text split by pages
    
    logger.info(f"Processing page-specific disclosures for {content_type}")
    logger.info(f"Received {len(original_text_by_page)} pages to analyze")
    
    # Debug log the specific pages we received
    for i, page in enumerate(original_text_by_page):
        page_num = page.get('page', i+1)
        page_text = page.get('text', '')
        logger.info(f"Page {page_num}: {page_text[:100]}...")
    
    if not content_type or (not text and not original_text_by_page):
        logger.info(f"Missing required params: content_type={content_type}, pages={len(original_text_by_page)}")
        return jsonify({'error': 'Missing required parameters'}), 400
    
    try:
        # Load disclosures from disclosures.json
        DISCLOSURES_FILE = os.path.join(os.path.dirname(__file__), 'disclosures.json')
        disclosures = {}
        
        if os.path.exists(DISCLOSURES_FILE):
            try:
                with open(DISCLOSURES_FILE, 'r') as f:
                    disclosures_data = json.load(f)
                    
                    # Handle different formats of disclosures.json
                    if isinstance(disclosures_data, list) and len(disclosures_data) > 0:
                        disclosures = disclosures_data[0] if isinstance(disclosures_data[0], dict) else {}
                    else:
                        disclosures = disclosures_data
                
                logger.info(f"Loaded {len(disclosures)} disclosures from {DISCLOSURES_FILE}")
                
                # Log a sample of the keywords to help debug
                sample_keys = list(disclosures.keys())[:5]
                logger.info(f"Sample keywords: {sample_keys}")
            except Exception as e:
                logger.error(f"Error loading disclosures.json: {e}")
                # Add fallback disclosures if loading fails
                disclosures = {
                    "mutual fund": "Mutual fund investing involves risk; principal loss is possible.",
                    "index fund": "Index funds are subject to market risk, including the possible loss of principal.",
                    "performance": "Past performance does not guarantee future results.",
                    "investing": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "investment": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "market": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "funds": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                    "returns": "Past performance is no guarantee of future results."
                }
        else:
            logger.info(f"Disclosures file not found: {DISCLOSURES_FILE}")
            # Add fallback disclosures if file doesn't exist
            disclosures = {
                "mutual fund": "Mutual fund investing involves risk; principal loss is possible.",
                "index fund": "Index funds are subject to market risk, including the possible loss of principal.",
                "performance": "Past performance does not guarantee future results.",
                "investing": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "investment": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "market": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "funds": "Investing involves risk including loss of principal. No strategy assures success or protects against loss.",
                "returns": "Past performance is no guarantee of future results."
            }
        
        # Log all keywords for debugging
        logger.info(f"Available disclosure keywords: {list(disclosures.keys())}")
        
        # Map of page numbers to their specific disclosures
        page_disclosures = {}
        matched_keywords = {}  # Track which keywords matched on which pages
        
        # Process each page for specific disclosures
        for page_data in original_text_by_page:
            page_num = page_data.get('page', 0)
            page_text = page_data.get('text', '')
            
            if not page_text:
                logger.info(f"Skipping page {page_num}: No text content")
                continue
            
            logger.info(f"Processing page {page_num} with {len(page_text)} characters")
            logger.info(f"Page {page_num} sample: {page_text[:100]}...")
            
            # Convert to lowercase for case-insensitive matching
            page_text_lower = page_text.lower()
            current_page_disclosures = []
            matched_keywords[page_num] = []
            
            # Check each keyword for an exact match in the page text
            for keyword, disclosure in disclosures.items():
                keyword_lower = keyword.lower()
                
                # If the keyword is found in the page text
                if keyword_lower in page_text_lower:
                    # Add the disclosure if not already present
                    if disclosure not in current_page_disclosures:
                        current_page_disclosures.append(disclosure)
                        matched_keywords[page_num].append(keyword)
                        logger.info(f"Page {page_num}: Matched keyword '{keyword}'")
            
            # Special handling for investing-related terms
            investing_terms = ["invest", "investing", "investment", "mutual fund", "funds", "return", "returns"]
            has_investing_term = any(term in page_text_lower for term in investing_terms)
            
            if has_investing_term:
                investing_disclosure = "Investing involves risk including loss of principal. No strategy assures success or protects against loss."
                if investing_disclosure not in current_page_disclosures:
                    current_page_disclosures.append(investing_disclosure)
                    matched_keywords[page_num].append("investment terms")
                    logger.info(f"Page {page_num}: Added investing disclosure based on investment terms")
            
            # Always add standard disclosures for test pages to demonstrate functionality
            if "mutual fund" in page_text_lower:
                mutual_fund_disclosure = "Mutual fund investing involves risk; principal loss is possible."
                if mutual_fund_disclosure not in current_page_disclosures:
                    current_page_disclosures.append(mutual_fund_disclosure)
                    matched_keywords[page_num].append("mutual fund")
            
            if "index fund" in page_text_lower:
                index_fund_disclosure = "Index funds are subject to market risk, including the possible loss of principal."
                if index_fund_disclosure not in current_page_disclosures:
                    current_page_disclosures.append(index_fund_disclosure)
                    matched_keywords[page_num].append("index fund")
            
            if "cd" in page_text_lower.split():  # Match "CD" as a whole word
                cd_disclosure = "CDs are FDIC insured up to applicable limits and offer a fixed rate of return."
                if cd_disclosure not in current_page_disclosures:
                    current_page_disclosures.append(cd_disclosure)
                    matched_keywords[page_num].append("CD")
            
            # Only add pages that have disclosures
            if current_page_disclosures:
                # Store page number as string for JSON compatibility
                page_disclosures[str(page_num)] = current_page_disclosures
                logger.info(f"Page {page_num}: Added {len(current_page_disclosures)} disclosures")
        
        # If no page disclosures were found, return an empty object
        if not page_disclosures:
            logger.info("No page-specific disclosures found")
        else:
            logger.info(f"Found page-specific disclosures for pages: {list(page_disclosures.keys())}")
        
        # Add a general disclosure to ensure there's always something to display
        general_disclosures = ["Content in this material is for general information only and not intended to provide specific advice or recommendations for any individual."]
        
        return jsonify({
            'page_disclosures': page_disclosures,
            'general_disclosures': general_disclosures,
            'debug_info': {
                'matched_keywords': matched_keywords,
                'keywords_available': list(disclosures.keys())
            }
        })
        
    except Exception as e:
        logger.error(f"Error generating page-specific disclosures: {e}")
        return jsonify({'error': str(e)}), 500
    
@app.route('/get_file_pages', methods=['POST'])
def get_file_pages():
    """Get page-by-page text for a processed file"""
    data = request.get_json()
    filename = data.get('filename')
    
    if not filename:
        return jsonify({'success': False, 'error': 'Filename is required'}), 400
    
    try:
        # Check if the file has been processed
        if filename in processed_files and 'text_by_page' in processed_files[filename]:
            text_by_page = processed_files[filename]['text_by_page']
            
            # Log what we're returning
            logger.info(f"Returning {len(text_by_page)} pages for file: {filename}")
            
            return jsonify({
                'success': True,
                'text_by_page': text_by_page
            })
        else:
            logger.info(f"File {filename} not found in processed_files or missing text_by_page")
            return jsonify({'success': False, 'error': 'File not found or processing incomplete'}), 404
    
    except Exception as e:
        logger.error(f"Error retrieving file pages: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    
@app.route('/get_user_scenario', methods=['GET'])
def get_user_scenario():
    """Get the assigned scenario for the current user."""
    try:
        # Get the current user from session
        if 'user_email' not in session:
            return jsonify({"success": False, "error": "User not logged in"}), 401
        
        user_email = session['user_email']
        
        # Load users from JSON
        users = load_users()
        
        # Find the user
        user = next((u for u in users if u['email'] == user_email), None)
        if not user:
            return jsonify({"success": False, "error": "User not found"}), 404
        
        # Check if user has an assigned scenario
        if 'assignedScenarios' in user and user['assignedScenarios'] and len(user['assignedScenarios']) > 0:
            scenario_title = user['assignedScenarios'][0]
            
            # Load scenarios to get the full text
            scenarios = {}
            if os.path.exists(JSON_FILE_PATH):
                with open(JSON_FILE_PATH, 'r') as file:
                    scenarios = json.load(file)
            
            # Get the scenario text
            scenario_text = scenarios.get(scenario_title, scenario_title)
            
            return jsonify({
                "success": True, 
                "hasScenario": True,
                "scenarioTitle": scenario_title,
                "scenarioText": scenario_text
            })
        
        # No scenario assigned
        return jsonify({"success": True, "hasScenario": False})
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    
# Remove scenario from user
@app.route('/remove_user_scenario', methods=['POST'])
def remove_user_scenario():
    """Remove assigned scenario from a user."""
    try:
        data = request.get_json()
        user_email = data.get('email')
        
        # Log the incoming request for debugging
        print(f"Removal request for email: {user_email}")
        
        if not user_email:
            return jsonify({"success": False, "error": "User email is required"}), 400

        # Load users from the JSON file
        users = load_users()

        # Find the user by email
        user = next((u for u in users if u['email'] == user_email), None)
        if user:
            # Remove the assigned scenarios
            if 'assignedScenarios' in user:
                user['assignedScenarios'] = []
            
            # Save the updated user data
            save_users(users)
            return jsonify({"success": True, "message": f"Scenario removed from user '{user_email}'."}), 200
        else:
            return jsonify({"success": False, "error": f"User '{user_email}' not found."}), 404
    except Exception as e:
        print(f"Error in remove_user_scenario: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500
    
@app.route('/assign-disclosures')
def assign_disclosures_page():
    return render_template('assign_disclosures.html')

@app.route("/get_relevant_disclosures", methods=["POST"])
def get_relevant_disclosures():
    data = request.get_json()
    revised_text = data.get("revised_text", "")
    
    logger.info(f"Generating disclosures for revised text: '{revised_text[:50]}...'")
    
    # Get the relevant disclosures based on keywords in the revised text
    matched_disclosures = get_matching_disclosures(revised_text)
    
    # Format the disclosures into a single paragraph
    disclosure_paragraph = " ".join(matched_disclosures)
    
    return jsonify({
        "disclosure_paragraph": disclosure_paragraph
    })

@app.route('/get_disclosures', methods=['GET'])
def get_disclosures():
    """Get standard disclosures from disclosures.json"""
    try:
        # File path to disclosures.json
        DISCLOSURES_FILE = os.path.join(os.path.dirname(__file__), 'disclosures.json')
        
        if os.path.exists(DISCLOSURES_FILE):
            with open(DISCLOSURES_FILE, 'r') as f:
                disclosures_data = json.load(f)
            
            # Convert the data to a regular dictionary for the frontend
            if isinstance(disclosures_data, list) and len(disclosures_data) > 0:
                # In this case, it's a list with a single object containing all disclosures
                disclosures = disclosures_data[0] if isinstance(disclosures_data[0], dict) else {}
            else:
                # If it's already a dictionary, use it as is
                disclosures = disclosures_data
                
            return jsonify({"success": True, "disclosures": disclosures}), 200
        else:
            return jsonify({"success": False, "error": "Disclosures file not found", "disclosures": {}}), 404
    except Exception as e:
        logger.error(f"Error reading disclosures.json: {e}")
        return jsonify({"success": False, "error": str(e), "disclosures": {}}), 500

@app.route('/update_disclosure', methods=['POST'])
def update_disclosure():
    """Update disclosures in disclosures.json"""
    try:
        # File path to disclosures.json
        DISCLOSURES_FILE = os.path.join(os.path.dirname(__file__), 'disclosures.json')
        
        # Get request data
        data = request.json
        action = data.get('action')
        topic = data.get('topic')
        
        if not topic or not action:
            return jsonify({
                "success": False, 
                "error": "Missing required parameters"
            }), 400
        
        # Load existing disclosures
        if os.path.exists(DISCLOSURES_FILE):
            with open(DISCLOSURES_FILE, 'r') as f:
                disclosures_data = json.load(f)
        else:
            # Create a new structure that matches what we observed
            disclosures_data = [{}]
        
        # Handle the specific JSON structure (list with single object)
        if isinstance(disclosures_data, list) and len(disclosures_data) > 0:
            # Work with the first item in the list
            disclosures = disclosures_data[0]
            
            if action == 'add':
                disclosure_text = data.get('disclosure')
                if not disclosure_text:
                    return jsonify({
                        "success": False, 
                        "error": "Missing disclosure text"
                    }), 400
                
                # Add or update the disclosure
                disclosures[topic] = disclosure_text
                
            elif action == 'delete':
                # Remove the disclosure if it exists
                if topic in disclosures:
                    del disclosures[topic]
                else:
                    return jsonify({
                        "success": False, 
                        "error": f"Disclosure topic '{topic}' not found"
                    }), 404
            else:
                return jsonify({
                    "success": False, 
                    "error": f"Invalid action: {action}"
                }), 400
        else:
            # If the structure is different, initialize it
            disclosures_data = [{}]
            if action == 'add':
                disclosure_text = data.get('disclosure')
                if not disclosure_text:
                    return jsonify({
                        "success": False, 
                        "error": "Missing disclosure text"
                    }), 400
                
                # Add the disclosure to the new structure
                disclosures_data[0][topic] = disclosure_text
            else:
                return jsonify({
                    "success": False, 
                    "error": "No disclosures to delete"
                }), 404
        
        # Save the updated disclosures
        with open(DISCLOSURES_FILE, 'w') as f:
            json.dump(disclosures_data, f, indent=4)
        
        return jsonify({"success": True}), 200
        
    except Exception as e:
        logger.error(f"Error updating disclosures.json: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


# First, keep the endpoint for API calls
@app.route('/get_matching_disclosures', methods=['POST'])
def get_matching_disclosures_api():
    """API endpoint to get matching disclosures for given text"""
    data = request.get_json()
    text = data.get('text', '')
    
    if not text:
        return jsonify({'error': 'Text is required'}), 400
        
    matched_disclosures = get_matching_disclosures(text)
    return jsonify({'disclosures': matched_disclosures})

# Then, define the helper function separately
def get_matching_disclosures(text):
    """
    Analyze text for keywords from disclosures.json and return only matching disclosures.
    """
    # Load disclosures from disclosures.json
    DISCLOSURES_FILE = os.path.join(os.path.dirname(__file__), 'disclosures.json')
    disclosures = {}
    
    try:
        if os.path.exists(DISCLOSURES_FILE):
            with open(DISCLOSURES_FILE, 'r') as f:
                disclosures_data = json.load(f)
                
                # Handle different formats of disclosures.json
                if isinstance(disclosures_data, list) and len(disclosures_data) > 0:
                    disclosures = disclosures_data[0] if isinstance(disclosures_data[0], dict) else {}
                else:
                    disclosures = disclosures_data
                    
            logger.info(f"Loaded {len(disclosures)} disclosures from {DISCLOSURES_FILE}")
    except Exception as e:
        logger.error(f"Error loading disclosures: {e}")
        return ["Error loading disclosures."]
    
    # Convert to lowercase for case-insensitive matching
    text_lower = text.lower()
    
    # List to store matched disclosures
    matched_disclosures = []
    matched_keywords = []
    
    # Add standard general disclosure
    general_disclosure = "Content in this material is for general information only and not intended to provide specific advice or recommendations for any individual."
    matched_disclosures.append(general_disclosure)
    
    # Look for specific keywords in the text
    for keyword, disclosure in disclosures.items():
        keyword_lower = keyword.lower()
        
        # Check if the keyword is in the text
        if keyword_lower in text_lower:
            matched_keywords.append(keyword)
            
            # Only add the disclosure if we haven't added it yet
            if disclosure not in matched_disclosures:
                matched_disclosures.append(disclosure)
                logger.info(f"Added disclosure for keyword: {keyword}")
    
    # Always include investing disclosure if investing-related terms are found
    investing_terms = ["invest", "investing", "investment", "mutual fund", "funds"]
    if any(term in text_lower for term in investing_terms):
        investing_disclosure = "Investing involves risk including loss of principal. No strategy assures success or protects against loss."
        if investing_disclosure not in matched_disclosures:
            matched_disclosures.append(investing_disclosure)
    
    logger.info(f"Found {len(matched_keywords)} matching keywords: {matched_keywords}")
    logger.info(f"Returning {len(matched_disclosures)} matched disclosures")
    
    return matched_disclosures

@app.route('/manage-disclosures')
def manage_disclosures():
    return render_template('manage_disclosures.html')


def load_training_data():
    """Load and preprocess training data from PostgreSQL database."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Query the fdc_data table
        cursor.execute("SELECT non_compliant, compliant FROM fdc_data;")
        data = cursor.fetchall()
        
        texts = []
        labels = []
        
        for item in data:
            non_compliant = item[0]
            compliant = item[1]
            
            texts.append(non_compliant)
            texts.append(compliant)
            labels.append(1)  # Non-compliant
            labels.append(0)  # Compliant
        
        cursor.close()
        
        logger.debug(f"Loaded training data: {len(texts)} examples")
        return texts, labels
    except Exception as e:
        logger.error(f"Error loading training data from database: {e}")
        return [], []
    finally:
        if conn:
            release_db_connection(conn)
            

def save_model(model, tokenizer, model_path='finra_compliance_model.pth', tokenizer_path='finra_tokenizer'):
    """Save the trained model and tokenizer."""
    torch.save(model.state_dict(), model_path)
    tokenizer.save_pretrained(tokenizer_path)


# BERT Functionality

def predict_compliance_bert(text, model, tokenizer):
    # Prepare input
    inputs = tokenizer(text, 
                      return_tensors="pt",
                      truncation=True,
                      max_length=512,
                      padding=True)
    
    # Make prediction
    with torch.no_grad():
        outputs = model(**inputs)
        probabilities = softmax(outputs.logits, dim=1)
        prediction = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][prediction].item()
    
    return prediction, confidence







# Sync Subscription Stautus with User in Stripe 
def sync_subscription_status(email):
    users = load_users()
    user = next((u for u in users if u['email'] == email), None)
    if not user:
        return {"error": "User not found in local database."}

    # 1) Query Stripe for the customer's active subscription
    customers = stripe.Customer.list(email=email).data
    if not customers:
        user['Subscribed'] = "No"
        user['Unsubscribed'] = "Yes"
        user['AccessEndDate'] = "Unknown"
        user['Sign Up Status'] = "Not Yet"  # No Stripe customer => certainly "Not Yet"
    else:
        customer = customers[0]
        subs = stripe.Subscription.list(customer=customer.id, status="active").data
        if subs:
            subscription = subs[0]

            # ‚îÄ‚îÄ‚îÄ [ NEW CODE ] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Expand this subscription to get its latest invoice
            full_sub = stripe.Subscription.retrieve(
                subscription.id, expand=["latest_invoice"]
            )
            # If there's an invoice and it's "paid," user has had at least 1 successful billing
            if full_sub.latest_invoice and full_sub.latest_invoice.status == "paid":
                user["Sign Up Status"] = "Signed Up"
            else:
                user["Sign Up Status"] = "Not Yet"
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            
            # Here's the new logic:
            if subscription.cancel_at_period_end:
                # They have scheduled a cancel; we consider them "unsubscribed" now
                user['Subscribed'] = "No"
                user['Unsubscribed'] = "Yes"
            else:
                # They do NOT have a pending cancel
                user['Subscribed'] = "Yes"
                user['Unsubscribed'] = "No"

            # Always set AccessEndDate from current_period_end
            next_billing_date = subscription.current_period_end
            user['AccessEndDate'] = datetime.utcfromtimestamp(next_billing_date).strftime('%Y-%m-%d')
        else:
            # No active sub
            user['Subscribed'] = "No"
            user['Unsubscribed'] = "Yes"
            user['AccessEndDate'] = "Unknown"
            user['Sign Up Status'] = "Not Yet"

    save_users(users)
    return user




# Function for checking subscription status in Profile HTML
@app.route('/api/subscription-status', methods=['GET'])
def subscription_status():
    email = request.args.get('email')
    if not email:
        return jsonify({'error': 'Email parameter is required'}), 400
    users = load_users()
    user = next((u for u in users if u['email'] == email), None)
    if user:
        return jsonify({'subscribed': user.get('Subscribed') == "Yes"})
    return jsonify({'error': 'User not found'}), 404

# Example Flask route to increment login count
@app.route('/track_login', methods=['POST'])
def track_login():
    user_email = request.json.get('email')
    if not user_email:
        return jsonify({'error': 'Email required'}), 400

    # Fetch user from the database
    user = db.session.query(User).filter_by(email=user_email).first()
    if user:
        user.login_count = (user.login_count or 0) + 1
        db.session.commit()
        return jsonify({'login_count': user.login_count})
    else:
        return jsonify({'error': 'User not found'}), 404


# Load users.json data
def loads_users():
    with open("users.json", "r") as file:
        return json.load(file)

# Identifying Current User in Profile HTML for updating the Advisor Type
@app.route('/api/current-user', methods=['GET'])
@limiter.limit("30 per minute")  # Limit this endpoint to 30 calls per minute per IP
def get_current_user():
    if 'user_email' in session:
        user_email = session['user_email']
        
        # Check if we already have user data cached in the session
        if 'cached_user_data' in session and session['cached_user_data'].get('email') == user_email:
            logger.debug(f"Using cached user data for {user_email}")
            return jsonify(session['cached_user_data'])
        
        logger.debug(f"Session user_email: {user_email}")  # Changed from info to debug
        users = load_users()
        user = next((u for u in users if u['email'] == user_email), None)
        
        if user:
            # Cache frequently accessed user data in session
            session['cached_user_data'] = user
            
            # Ensure `usage` and `usage_denominator` are present
            user['usage'] = user.get('usage', 0.0001)
            user['usage_denominator'] = user.get('usage_denominator', 100)

            # Only log AccessEndDate if it changes
            if 'last_access_end_date' not in session or session['last_access_end_date'] != user.get('AccessEndDate'):
                session['last_access_end_date'] = user.get('AccessEndDate')
                logger.info(f"AccessEndDate for {user_email}: {user.get('AccessEndDate')}")
            
            return jsonify(user)
    
    return jsonify({'error': 'User not found or not logged in'}), 401

def invalidate_user_cache(user_email):
    """Clear cached user data when user is updated"""
    if 'cached_user_data' in session and session.get('user_email') == user_email:
        session.pop('cached_user_data', None)
        logger.debug(f"Invalidated cache for user {user_email}")

@app.route('/update-profile', methods=['POST'])
def update_profile():
    data = request.json
    if 'user_email' not in session:
        return jsonify({"success": False, "error": "Not logged in"}), 401
    
    current_email = session['user_email']
    users = load_users()
    
    # Find the user to update
    for user in users:
        if user.get('email') == current_email:
            # Update basic profile information
            first_name = data.get('firstName', '')
            last_name = data.get('lastName', '')
            user['fullName'] = f"{first_name} {last_name}"
            
            # Update advisor type if provided
            if 'advisorType' in data:
                user['advisorType'] = data['advisorType']
            
            # Save users back to file
            save_users(users)
            logger.info(f"Updated profile for {current_email}, including advisorType: {data.get('advisorType', 'not provided')}")
            
            return jsonify({"success": True})
    
    return jsonify({"success": False, "error": "User not found"}), 404




# PROHIBITED WORDS FUNCTIONALITY

# Function for alligning prohibited words Control Panel with Results
@app.route('/get_prohibited_words', methods=['GET'])
def get_prohibited_words():

    try:
        conn = psycopg2.connect(
            dbname="postgresql_instance_free",
            user="postgresql_instance_free_user",
            password="bz3SdnKi6g6TRdM4j1AtE2Ash8VNgiQO",
            host="dpg-cts4psa3esus73dn1cn0-a.oregon-postgres.render.com",
            port="5432"
        )
        cursor = conn.cursor()
        cursor.execute("SELECT word, alternative FROM prohibited_words;")
        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        # Make a dict: { "word1": "alternative1", "word2": "alternative2", ... }
        prohibited_words_dict = {row[0]: row[1] for row in rows}
        return jsonify(prohibited_words_dict), 200
    except Exception as e:
        logger.info(f"Error fetching prohibited words from DB: {e}")
        return jsonify({"error": str(e)}), 500

# File to store prohibited words
STORAGE_FILE = 'prohibited_words.json'

# Function to load data from JSON file
def load_prohibited_words():
    try:
        with open(STORAGE_FILE, 'r') as file:
            return json.load(file)
    except FileNotFoundError:
        return {}

# Function to save data to JSON file
def save_prohibited_words(data):
    with open(STORAGE_FILE, 'w') as file:
        json.dump(data, file, indent=4)

# Function to delete prohibited word in control panel
@app.route('/delete_prohibited_word', methods=['POST'])
def delete_prohibited_word():
    data = request.get_json()
    word = data.get('word')
    if not word:
        return jsonify({'success': False, 'error': 'Word not provided'}), 400

    # Connect to DB
    try:
        conn = psycopg2.connect(
            dbname="postgresql_instance_free",
            user="postgresql_instance_free_user",
            password="bz3SdnKi6g6TRdM4j1AtE2Ash8VNgiQO",
            host="dpg-cts4psa3esus73dn1cn0-a.oregon-postgres.render.com",
            port="5432"
        )
        cursor = conn.cursor()

        # Delete the row where 'word' matches
        cursor.execute(
            "DELETE FROM prohibited_words WHERE word = %s",
            (word.lower(),)
        )
        rows_deleted = cursor.rowcount  # how many rows were actually deleted
        conn.commit()
        cursor.close()
        conn.close()

        if rows_deleted > 0:
            return jsonify({
                'success': True,
                'message': f'Prohibited word "{word}" deleted successfully from DB.'
            }), 200
        else:
            return jsonify({
                'success': False,
                'error': f'Word "{word}" not found in DB.'
            }), 404
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Error deleting word from DB: {str(e)}'
        }), 500

# Function to add prohibited word
@app.route('/add_prohibited_word', methods=['POST'])
def add_prohibited_word():
    data = request.get_json()
    word = data.get('word', '').strip()
    alternative = data.get('alternative', '').strip()

    if not word:
        return jsonify({'success': False, 'error': 'Word must be provided.'}), 400


    # 1) Keep your JSON functionality
    # Load the current prohibited words from your JSON
    prohibited_words = load_prohibited_words()  

    # Update the dictionary in memory
    prohibited_words[word] = alternative if alternative else "No alternative provided"

    # Save the updated dictionary back to JSON
    save_prohibited_words(prohibited_words)     

    # 2) Also insert into PostgreSQL
    try:
        conn = psycopg2.connect(
            dbname="postgresql_instance_free",
            user="postgresql_instance_free_user",
            password="bz3SdnKi6g6TRdM4j1AtE2Ash8VNgiQO",
            host="dpg-cts4psa3esus73dn1cn0-a.oregon-postgres.render.com",
            port="5432"
        )
        cursor = conn.cursor()

        # Insert the new word into the prohibited_words table
        sql = """INSERT INTO prohibited_words (word, alternative) VALUES (%s, %s);"""
        cursor.execute(sql, (word.lower(), alternative or "No alternative provided"))
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:

        # If the DB insertion fails, you might still consider the JSON save successful
        return jsonify({
            'success': False,
            'error': f'Could not save to DB: {str(e)}. JSON save succeeded.'
        }), 500


    # 3) Return success
    return jsonify({
        'success': True,
        'message': f'Prohibited word "{word}" added to JSON and DB successfully.'
    }), 200

# Flagging Prohibited Words
@app.route('/flag_prohibited_words', methods=['POST'])
def flag_prohibited_words():
    try:
        data = request.get_json()
        text = data.get('text', '')
        if not text:
            return jsonify({"success": False, "error": "Text to scan is required."}), 400
        flagged_instances = []

        # Scan text for prohibited words
        for entry in prohibited_words:
            word = entry["word"]
            if word.lower() in text.lower():
                flagged_instances.append({
                    "word": word,
                    "alternative": entry.get("alternative", "No alternative provided"),
                    "context": get_context(text, word)
                })
        return jsonify({"success": True, "flagged": flagged_instances})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


def get_context(text, word, window=30):
    """Return a snippet of text around the flagged word for context."""
    idx = text.lower().find(word.lower())
    if idx == -1:
        return None
    start = max(idx - window, 0)
    end = min(idx + len(word) + window, len(text))
    return text[start:end]








# ASSIGNING USERS SCENARIOS for disclosure

# Endpoint to delete a scenario
@app.route('/delete_scenario', methods=['POST'])
def delete_scenario():
    """Delete a disclosure scenario."""
    try:
        data = request.json
        title = data.get('title')
        if not title:
            return jsonify({"success": False, "error": "Scenario title is required."}), 400

        # Load existing scenarios
        if os.path.exists(JSON_FILE_PATH):
            with open(JSON_FILE_PATH, 'r') as file:
                scenarios = json.load(file)
                
                # Ensure the file contains a dictionary
                if not isinstance(scenarios, dict):
                    raise ValueError("JSON file does not contain a valid dictionary.")
        else:
            return jsonify({"success": False, "error": "No scenarios found."}), 404

        # Check if the title exists in the scenarios
        if title not in scenarios:
            return jsonify({"success": False, "error": "Scenario not found."}), 404

        # Remove the scenario
        del scenarios[title]

        # Save the updated scenarios back to the file
        with open(JSON_FILE_PATH, 'w') as file:
            json.dump(scenarios, file, indent=4)
        return jsonify({"success": True, "message": f"Scenario '{title}' deleted successfully."})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


# Path to the JSON file
JSON_FILE_PATH = "scenarios.json"

@app.route('/save_advisor_type', methods=['POST'])
def save_advisor_type():
    try:
        data = request.json
        email = data.get('email')
        advisor_type = data.get('advisorType')
        
        if not email:
            return jsonify({"success": False, "error": "Email is required"}), 400
        
        # Load users from the JSON file
        users = load_users()
        
        # Find the user by email
        user_found = False
        for user in users:
            if user['email'] == email:
                user['advisorType'] = advisor_type  # Add/update advisorType
                user_found = True
                break
        
        if not user_found:
            return jsonify({"success": False, "error": "User not found"}), 404
        
        # Save users back to file
        save_users(users)
        
        return jsonify({"success": True, "message": "Advisor type saved successfully"})
    except Exception as e:
        logger.error(f"Error saving advisor type: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

# Endpoint to update user scenario
@app.route('/update_user_scenario', methods=['POST'])
def update_user_scenario():
    try:
        data = request.json
        user = data.get('user')
        scenario = data.get('scenario')
        if not user or not scenario:
            return jsonify({"success": False, "error": "User or scenario missing."}), 400

        # Load existing data
        if os.path.exists(JSON_FILE_PATH):
            with open(JSON_FILE_PATH, 'r') as file:
                try:
                    scenarios = json.load(file)
                    
                    # Ensure the file contains a dictionary
                    if not isinstance(scenarios, dict):
                        raise ValueError("JSON file does not contain a valid dictionary.")
                except (json.JSONDecodeError, ValueError) as e:

                    # If the JSON is invalid or not a dictionary, reset it
                    scenarios = {}
        else:
            scenarios = {}

        # Update the JSON file with the new scenario
        scenarios[user] = scenario.get('disclosure', '')
        with open(JSON_FILE_PATH, 'w') as file:
            json.dump(scenarios, file, indent=4)
        return jsonify({"success": True, "message": "Scenario successfully updated."})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

# Endpoint to add a new scenario
@app.route('/add_scenario', methods=['POST'])
def add_scenario():
    """Add a new disclosure scenario."""
    try:
        data = request.json
        title = data.get('title')
        disclosure = data.get('disclosure')

        if not title or not disclosure:
            return jsonify({"success": False, "error": "Scenario title or disclosure is missing."}), 400

        # Load existing scenarios
        if os.path.exists(JSON_FILE_PATH):
            with open(JSON_FILE_PATH, 'r') as file:
                try:
                    scenarios = json.load(file)

                    # Ensure the file contains a dictionary
                    if not isinstance(scenarios, dict):
                        raise ValueError("JSON file does not contain a valid dictionary.")
                except (json.JSONDecodeError, ValueError):
                    scenarios = {}
        else:
            scenarios = {}

        # Add the new scenario
        if title in scenarios:
            return jsonify({"success": False, "error": "Scenario title already exists."}), 400
        scenarios[title] = disclosure

        # Save the updated scenarios back to the file
        with open(JSON_FILE_PATH, 'w') as file:
            json.dump(scenarios, file, indent=4)
        return jsonify({"success": True, "message": "Scenario added successfully."})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


# Endpoint to fetch all scenarios
@app.route('/get_scenarios', methods=['GET'])
def get_scenarios():
    """Get all disclosure scenarios."""
    try:
        # Check if the JSON file exists
        if os.path.exists(JSON_FILE_PATH):
            with open(JSON_FILE_PATH, 'r') as file:
                scenarios = json.load(file)
                # Ensure the file contains a dictionary
                if not isinstance(scenarios, dict):
                    raise ValueError("JSON file does not contain a valid dictionary.")
        else:
            scenarios = {}

        # Convert scenarios into a list of titles and disclosures for the response
        scenario_list = [{"title": title, "disclosure": disclosure} for title, disclosure in scenarios.items()]
        return jsonify({"success": True, "scenarios": scenario_list})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


# Load assignments from JSON
def load_assignments():
    with open('assignments.json', 'r') as f:
        return json.load(f)

@app.route('/get_assigned_disclosure', methods=['POST'])
def get_assigned_disclosure():
    try:
        data = request.json
        user = data.get('user')  # Get the logged-in user from the request

        # Load assignments from the JSON file
        assignments = load_assignments()

        # Check if the user has an assigned scenario
        if user in assignments:
            return jsonify(success=True, disclosure=assignments[user])
        else:
            return jsonify(success=True, disclosure=None)  # No scenario assigned
    except Exception as e:
        return jsonify(success=False, error=str(e))


SCENARIOS_FILE = 'scenarios.json'


# Load scenarios from a file
def load_scenarios():
    """Load scenarios from PostgreSQL database."""
    scenarios = {}
    conn = None
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Query the scenarios table
        cursor.execute("SELECT scenario_key, scenario_value FROM scenarios;")
        rows = cursor.fetchall()
        
        for row in rows:
            scenario_key = row[0]
            scenario_value = row[1]
            scenarios[scenario_key] = scenario_value
        
        cursor.close()
        
        logger.debug(f"Loaded {len(scenarios)} scenarios from database")
        return scenarios
    except Exception as e:
        logger.error(f"Error loading scenarios from database: {e}")
        return {}
    finally:
        if conn:
            release_db_connection(conn)
            
# Save scenarios to a file
def save_scenarios(scenarios):
    """Save scenarios to PostgreSQL database."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # First, clear existing scenarios (optional - depends on your approach)
        cursor.execute("DELETE FROM scenarios;")
        
        # Insert all scenarios
        for key, value in scenarios.items():
            cursor.execute(
                "INSERT INTO scenarios (scenario_key, scenario_value) VALUES (%s, %s);",
                (key, value)
            )
        
        conn.commit()
        cursor.close()
        
        logger.debug(f"Saved {len(scenarios)} scenarios to database")
    except Exception as e:
        logger.error(f"Error saving scenarios to database: {e}")
    finally:
        if conn:
            release_db_connection(conn)

            
# Assigns Scenario
@app.route('/assign_scenario', methods=['POST'])
def assign_scenario():
    """Assign a scenario to a user."""
    try:
        data = request.get_json()
        user_email = data.get('email') or data.get('user')  # Accept either 'email' or 'user' parameter
        scenario_title = data.get('scenario')
        if not user_email or not scenario_title:
            return jsonify({"success": False, "error": "User and scenario are required"}), 400

        # Load users from the JSON file
        users = load_users()

        # Find the user by email
        user = next((u for u in users if u['email'] == user_email), None)
        if user:
            
            # Add the scenario to the user's assigned scenarios
            if 'assignedScenarios' not in user:
                user['assignedScenarios'] = []
            
            # Replace existing scenarios with the new one
            user['assignedScenarios'] = [scenario_title]
            
            # Save the updated user data
            save_users(users)
            return jsonify({"success": True, "message": f"Scenario '{scenario_title}' assigned to user '{user_email}'."}), 200
        else:
            return jsonify({"success": False, "error": f"User '{user_email}' not found."}), 404
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    

#Search for existing user
@app.route('/search_user', methods=['POST'])
def search_user():
    """Search for a user account by username or email."""
    data = request.get_json()
    query = data.get('query', '').strip().lower()
    if not query:
        return jsonify({"success": False, "message": "Search query is required"}), 400
    
    # Load users from the JSON file
    users = load_users()  # Ensure this function loads the `users.json` file correctly

    # Find matching users
    matching_users = [
        user for user in users
        if query in user.get('email', '').strip().lower() or query in user.get('fullName', '').strip().lower()
    ]

    if matching_users:
        return jsonify({"success": True, "users": matching_users}), 200
    else:
        return jsonify({"success": False, "message": "No matching users found."}), 404





# CUSTOM DISCLOSURES (may be redundant)


# Function to create custom disclosures
@app.route('/get_custom_disclosures', methods=['GET'])
def get_custom_disclosures():
    
    # Load custom disclosures from the JSON file
    try:
        with open(CUSTOM_DISCLOSURES_FILE, 'r') as f:
            custom_disclosures = json.load(f)
        return jsonify({"disclosures": custom_disclosures}), 200
    except FileNotFoundError:
        return jsonify({"disclosures": {}}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# Function to delete Custom Disclosures 
@app.route('/delete_custom_disclosure', methods=['POST'])
def delete_custom_disclosure():
    data = request.get_json()
    word_to_delete = data.get('word', '').strip()
    if not word_to_delete:
        return jsonify({"success": False, "error": "No word specified"}), 400
    try:
        with open(CUSTOM_DISCLOSURES_FILE, 'r') as f:
            custom_disclosures = json.load(f)
        if word_to_delete in custom_disclosures:
            del custom_disclosures[word_to_delete]

            # Save the updated disclosures back to the file
            with open(CUSTOM_DISCLOSURES_FILE, 'w') as f:
                json.dump(custom_disclosures, f, indent=4)
                
            # Also update in-memory DISCLOSURE_WORDS dictionary
            if word_to_delete in DISCLOSURE_WORDS:
                del DISCLOSURE_WORDS[word_to_delete]
            return jsonify({"success": True}), 200
        else:
            return jsonify({"success": False, "error": "Word not found"}), 404
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500



# Control Panel
@app.route('/control-panel')
def control_panel():
    return redirect(url_for('manage_disclosures'))

# Set the secret key for sessions
app.secret_key = 's3cr3t_k3y_@1234'

# File path for users.json
USER_FILE = os.path.join(os.path.dirname(__file__), 'users.json')

# Ensure users.json exists
if not os.path.exists(USER_FILE):
    with open(USER_FILE, 'w') as file:
        json.dump([], file)  # Initialize with an empty list
    logger.info(f"Created {USER_FILE} with an empty list.")


# Updates Users Usage
def update_user_usage(user_email, sentence_count=None, usage_increment=0.0):
    """Updates the user's usage based on sentence count or a fixed usage increment."""
    users = load_users()
    user = next((u for u in users if u['email'] == user_email), None)

    if user:
        if sentence_count is not None:
            cost_per_sentence = 0.0175
            additional_cost = sentence_count * cost_per_sentence
            user['usage'] += additional_cost
            logger.info(f"Updated usage for {user_email}: +${additional_cost:.2f}, Total: ${user['usage']:.2f}")

        if usage_increment > 0:
            user['usage'] += usage_increment
            logger.info(f"Updated usage for {user_email}: +${usage_increment:.2f}, Total: ${user['usage']:.2f}")

        save_users(users)
    else:
        logger.info(f"User with email {user_email} not found.")


# Checking Subscription
@app.route('/api/check_subscription', methods=['GET'])
def check_subscription():
    email = request.args.get('email')
    if not email:
        return jsonify({"error": "Email parameter is required"}), 400

    # Sync subscription status with Stripe
    user = sync_subscription_status(email)
    if "error" in user:
        return jsonify(user), 404

    if user.get('Subscribed') == "No":
        return jsonify({"showPaymentPopup": True})
    return jsonify({"showPaymentPopup": False})



# BACKEND HTML FUNCTIONALITY [Users Control (Add/Delete/Assign Admin)]
USERS_FILE = 'users.json'


# API to fetch all users BACKEND HTML
@app.route('/api/users', methods=['GET'])
def get_users():
    
    try:
        with open(USERS_FILE, 'r') as file:
            users = json.load(file)
        return jsonify(users), 200
    except FileNotFoundError:
        return jsonify({"error": "Users file not found"}), 404
    except json.JSONDecodeError:
        return jsonify({"error": "Error decoding JSON file"}), 500

# Render the backend.html page on browser BACKEND HTML
@app.route('/users')
def users_list_page():
    return render_template('backend.html')  # Replace 'users_list.html' with the filename

# Delete a user in the BACKEND HTML
@app.route('/api/delete-user', methods=['POST'])
def delete_user():
    data = request.json
    email = data.get('email')
    
    if not email:
        return jsonify({'error': 'Email is required'}), 400

    users = load_users()
    updated_users = [user for user in users if user['email'] != email]
    if len(users) == len(updated_users):
        return jsonify({'error': 'User not found'}), 404
    save_users(updated_users)
    return jsonify({'message': f'User with email {email} deleted successfully'}), 200


# Function to read the Users.json for Admin "Yes" or Admin "No" BACKEND HTML

@app.route('/api/add-admin', methods=['POST'])
def add_admin():
    data = request.json
    email = data.get('email')
    if not email:
        return jsonify({'error': 'Email is required'}), 400
    
    try:
        # Load users
        with open('users.json', 'r') as f:
            users = json.load(f)

        # Find and update the user
        user_found = False
        for user in users:
            if user['email'] == email:
                user['Administrator'] = 'Yes'
                user_found = True
                break

        if not user_found:
            return jsonify({'error': 'User not found'}), 404

        # Save changes back to users.json
        with open('users.json', 'w') as f:
            json.dump(users, f, indent=4)
        return jsonify({'message': f'User {email} is now an Administrator'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Function to accurately update the Subscription shown in User Profile PROFILE HTML

@app.route('/update-subscription', methods=['POST'])
def update_subscription():
    data = request.json
    email = data.get('email')
    new_plan = data.get('subscriptionPlan')

    if not email or not new_plan:
        return jsonify({'error': 'Email and subscription plan are required'}), 400
    users = load_users()
    user_found = False
    for user in users:
        if user['email'] == email:  # Locate the user by email
            user['subscriptionPlan'] = new_plan  # Update the subscriptionPlan
            user_found = True
            break
    if not user_found:
        return jsonify({'error': 'User not found'}), 404
    save_users(users)  # Save updated users list back to JSON
    return jsonify({'message': 'Subscription plan updated successfully'}), 200

# Function to cancel subscription in PROFILE HTMl@app.route('/api/cancel-subscription', methods=['POST'])
@app.route('/api/cancel-subscription', methods=['POST'])
def cancel_subscription():
    data = request.json
    email = data.get('email')

    if not email:
        return jsonify({'error': 'Email is required.'}), 400

    try:
        # Retrieve the Stripe customer associated with the email
        customers = stripe.Customer.list(email=email).data
        if not customers:
            return jsonify({'error': 'Customer not found.'}), 404
        customer = customers[0]

        # Retrieve active subscriptions for the customer
        subscriptions = stripe.Subscription.list(customer=customer.id, status="active").data
        if not subscriptions:
            return jsonify({'error': 'No active subscription found for user.'}), 404

        # Cancel the active subscription
        subscription = subscriptions[0]
        stripe.Subscription.modify(
            subscription.id,              # <-- pass the actual subscription ID
            cancel_at_period_end=True
        )


        # Sync subscription status locally
        user = sync_subscription_status(email)
        if "error" in user:
            return jsonify(user), 404

        return jsonify({'message': 'Subscription canceled successfully.'})
    except Exception as e:
        logger.info(f"Error canceling subscription: {str(e)}")
        return jsonify({'error': 'An error occurred while canceling the subscription.'}), 500


# Function to track users who have unsubscribed
@app.route('/update-unsubscribed', methods=['POST'])
def update_unsubscribed():
    data = request.json
    email = data.get('email')

    if not email:
        return jsonify({'error': 'Email is required'}), 400

    # Sync subscription status locally
    user = sync_subscription_status(email)
    if "error" in user:
        return jsonify(user), 404

    return jsonify({'message': 'User unsubscribed successfully.'})

# Helper functions
def load_users():
    """Load users from PostgreSQL database with JSON fallback."""
    users = []
    conn = None
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Query the users table
        cursor.execute("""
            SELECT 
                id, full_name, email, password, api_key, usage, usage_denominator, 
                flat_rate_paid, subscribed, unsubscribed, sign_up_status, 
                administrator, access_end_date, subscription_plan, advisor_type, 
                administrator_access_new, assigned_scenarios, user_cost
            FROM users;
        """)
        rows = cursor.fetchall()
        
        for row in rows:
            user = {
                "id": row[0],
                "fullName": row[1],
                "email": row[2],
                "newPassword": row[3],
                "api_key": row[4],
                "usage": row[5],
                "usage_denominator": row[6],
                "flat_rate_paid": row[7],
                "Subscribed": row[8],
                "Unsubscribed": row[9],
                "Sign Up Status": row[10],
                "Administrator": row[11],
                "AccessEndDate": row[12],
                "subscriptionPlan": row[13],
                "advisorType": row[14],
                "Administrator Access NEW": row[15],
                "assignedScenarios": row[16],  # This will be an array in PostgreSQL
                "User Cost": row[17]
            }
            users.append(user)
        
        cursor.close()
        
        logger.debug(f"Loaded {len(users)} users from database")
        return users
    except Exception as e:
        logger.error(f"Error loading users from database: {e}")
        
        # FALLBACK: Try to load from JSON file
        try:
            logger.warning("Database connection failed, falling back to local JSON file")
            with open('users.json', 'r') as file:
                users = json.load(file)
            logger.info(f"Successfully loaded {len(users)} users from local JSON file")
            return users
        except Exception as json_error:
            logger.error(f"Error loading users from JSON fallback: {json_error}")
            return []
    finally:
        if conn:
            release_db_connection(conn)  # Return connection to the pool
            
    
def save_users(users):
    """Save users to PostgreSQL database."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # For each user in the users list
        for user in users:
            user_email = user.get('email')
            # Check if the user already exists
            cursor.execute("SELECT id FROM users WHERE email = %s;", (user_email,))
            existing_user = cursor.fetchone()
            
            if existing_user:
                # Update existing user
                cursor.execute("""
                    UPDATE users SET
                        full_name = %s,
                        password = %s,
                        api_key = %s,
                        usage = %s,
                        usage_denominator = %s,
                        flat_rate_paid = %s,
                        subscribed = %s,
                        unsubscribed = %s,
                        sign_up_status = %s,
                        administrator = %s,
                        access_end_date = %s,
                        subscription_plan = %s,
                        advisor_type = %s,
                        administrator_access_new = %s,
                        assigned_scenarios = %s,
                        user_cost = %s
                    WHERE email = %s;
                """, (
                    user.get('fullName'),
                    user.get('newPassword'),
                    user.get('api_key'),
                    user.get('usage', 0.0),
                    user.get('usage_denominator', 5),
                    user.get('flat_rate_paid', False),
                    user.get('Subscribed', 'No'),
                    user.get('Unsubscribed', 'No'),
                    user.get('Sign Up Status', 'Not Yet'),
                    user.get('Administrator', 'No'),
                    user.get('AccessEndDate', 'Unknown'),
                    user.get('subscriptionPlan', ''),
                    user.get('advisorType', ''),
                    user.get('Administrator Access NEW', 'No'),
                    user.get('assignedScenarios', []),
                    user.get('User Cost', 0.0),
                    user_email
                ))
                
                # Invalidate user cache if this is the current user
                if 'user_email' in session and session['user_email'] == user_email:
                    invalidate_user_cache(user_email)
            else:
                # Insert new user
                cursor.execute("""
                    INSERT INTO users (
                        full_name, email, password, api_key, usage, usage_denominator,
                        flat_rate_paid, subscribed, unsubscribed, sign_up_status,
                        administrator, access_end_date, subscription_plan, advisor_type,
                        administrator_access_new, assigned_scenarios, user_cost
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
                """, (
                    user.get('fullName'),
                    user_email,
                    user.get('newPassword'),
                    user.get('api_key'),
                    user.get('usage', 0.0),
                    user.get('usage_denominator', 5),
                    user.get('flat_rate_paid', False),
                    user.get('Subscribed', 'No'),
                    user.get('Unsubscribed', 'No'),
                    user.get('Sign Up Status', 'Not Yet'),
                    user.get('Administrator', 'No'),
                    user.get('AccessEndDate', 'Unknown'),
                    user.get('subscriptionPlan', ''),
                    user.get('advisorType', ''),
                    user.get('Administrator Access NEW', 'No'),
                    user.get('assignedScenarios', []),
                    user.get('User Cost', 0.0)
                ))
        
        conn.commit()
        cursor.close()
        
        logger.debug(f"Saved {len(users)} users to database")  # Changed from info to debug
    except Exception as e:
        logger.error(f"Error saving users to database: {e}")
    finally:
        if conn:
            release_db_connection(conn)  # Return connection to the pool


# USERS.JSON

def generate_api_key():
    return secrets.token_hex(32)  # Generates a 64-character hex string

@app.route('/register', methods=['POST'])
def register():
    data = request.json
    if not data:
        return jsonify({'error': 'No data received'}), 400
    users = load_users()
    if any(user['email'] == data['email'] for user in users):
        return jsonify({'error': 'Email is already registered.'}), 400
    api_key = generate_api_key()
    new_user = {
        "fullName": data['fullName'],
        "email": data['email'],
        "newPassword": data['newPassword'],
        "api_key": api_key,
        "usage": 0.0,
        "usage_denominator": 5, #Example denominator
        "flat_rate_paid": False,
        "Subscribed": "No",  # Default value
        "Unsubscribed": "No",  # Default value
        "Sign Up Status":"Not Yet",  # Default value
        "Administrator": "No",  # Default value
        "AccessEndDate": "2025-01-31",
        "subscriptionPlan": data.get('subscriptionPlan', "")

    }
    users.append(new_user)
    save_users(users)

    # Store email in session
    session['user_email'] = data['email']
    logger.info(f"New user registered and logged in: {data['email']}")  # Debugging
    return jsonify({
        'message': 'Registration successful!',
        'redirect_url': url_for('profile')
    }), 200








# Authenticate API key decorator
def authenticate_api_key(f):
    @wraps(f)
    def wrapper(*args, **kwargs):
        api_key = request.headers.get('Authorization')
        if not api_key:
            return jsonify({"error": "API key required"}), 401

        # Find user by API key
        users = load_users()
        user = next((u for u in users if u['api_key'] == api_key), None)
        if not user:
            return jsonify({"error": "Invalid API key"}), 403

        # Pass user data to the endpoint
        return f(user, users, *args, **kwargs)
    return wrapper


# Example API endpoint with usage tracking
@app.route('/api/resource', methods=['GET'])
@authenticate_api_key
def api_resource():
    if 'user_email' not in session:
        return jsonify({"error": "User not logged in"}), 403

    # Load user from session
    user_email = session['user_email']
    users = load_users()
    user = next((u for u in users if u['email'] == user_email), None)

    if not user:
        return jsonify({"error": "User not found"}), 404
    
    # Increment usage
    cost_per_call = 0.10
    user['usage'] += cost_per_call
    save_users(users)
    return jsonify({

        "message": "API request successful.",
        "usage": f"${user['usage']:.2f}",
        "note": "You are charged $0.10 per request."
    }), 200





@app.route('/login', methods=['GET'])
def serve_login_page():
    reset_success = request.args.get('reset_success')
    if reset_success:
        return render_template('login.html', message="Your password has been reset successfully. Please log in with your new password.")
    return render_template('login.html')

# Route to handle login functionality
@app.route('/login', methods=['POST'])
def handle_login():
    data = request.json
    username = data.get('username')  # Email as username
    password = data.get('password')
    
    # Load users
    users = load_users()
    user = next((u for u in users if u['email'] == username), None)
    if user and user['newPassword'] == password:
        session.permanent = True  # Make this session permanent
        session['user_email'] = user['email']  # Store email in session
        return jsonify({'message': 'Login successful!', 'redirect_url': url_for('handle_upload')}), 200
    else:
        return jsonify({'error': 'Invalid email or password'}), 401





# STRIPE FUNCTIONALITY


# Live Stripe Key
#stripe.api_key = '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'

# Remove "Live Stripe Key" and "Test Stripe Key" above and below when moving to web
#stripe.api_key = os.environ.get('STRIPE_API_KEY', '')

# Test Stripe Key
stripe.api_key = 'sk_test_51GbFkxE4vPtJSn6ecxnMRVMsiWwgJrxOWxsQ7rSuA1s5NtTYEyRKyNr7Bi575PlHwTqng7gcI92coYA5UpBwu0xT00f24K6saO'


@app.route('/profile')
def profile():
    return render_template('profile.html')

@app.route('/create-subscription', methods=['POST'])
def create_subscription():
    try:
        data = request.get_json()
        payment_method_id = data.get('paymentMethodId')
        price_id = data.get('priceId')
        email = data.get('email')  # Get the email sent from the frontend

        # Get billing address details
        billing_details = {
            'name': data.get('billingName'),
            'email': email,  # Use the email from the frontend
            'address': {
                'line1': data.get('billingAddress'),
                'city': data.get('billingCity'),
                'state': data.get('billingState'),
                'postal_code': data.get('billingZip'),
                'country': data.get('billingCountry'),
            },
        }
        if not email:
            return jsonify({'error': 'Email is required.'}), 400

        # Create the customer
        customer = stripe.Customer.create(
            email=email,
        )

        # Attach the PaymentMethod to the customer
        stripe.PaymentMethod.attach(
            payment_method_id,
            customer=customer.id,
        )

        # Update the billing details on the PaymentMethod
        stripe.PaymentMethod.modify(
            payment_method_id,
            billing_details=billing_details,
        )

        # Set the default payment method for the customer
        stripe.Customer.modify(
            customer.id,
            invoice_settings={'default_payment_method': payment_method_id},
        )
        
        # Create the subscription
        subscription = stripe.Subscription.create(
            customer=customer.id,
            items=[{'price': price_id}],
            expand=['latest_invoice.payment_intent'],
        )

        # Get the PaymentIntent if it exists
        payment_intent = subscription.latest_invoice.payment_intent
        client_secret = payment_intent.client_secret if payment_intent else None

        

        # Update the user's subscription status in users.json
        users = load_users()
        user = next((u for u in users if u['email'] == email), None)
        if user:
            user['Subscribed'] = "Yes"
            user['Unsubscribed'] = "No"  # Mark as subscribed

            if payment_intent and payment_intent.status == 'succeeded':
                user["Sign Up Status"] = "Signed Up"
            
            save_users(users)  # Save the updated users back to the file

        return jsonify({
            'subscriptionId': subscription.id,
            'clientSecret': client_secret,
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/create-payment-intent', methods=['POST'])
def create_payment_intent():
    try:
        # Get payment amount from the client-side
        data = request.get_json()
        amount = data['amount']  # Amount in cents

        # Create a payment intent
        intent = stripe.PaymentIntent.create(
            amount=amount,
            currency='usd',
            automatic_payment_methods={'enabled': True},
        )
        return jsonify({'clientSecret': intent['client_secret']})
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/charge-card', methods=['POST'])
def charge_card():
    try:
        data = request.get_json()
        price_id = data.get('priceId')
        email = data.get('email')  # Email passed from the frontend
        if not price_id or not email:
            return jsonify({'error': 'Invalid price ID or email'}), 400

        # Map price IDs to their respective amounts in cents
        price_mapping = {
            'price_1Qf8LCE4vPtJSn6exafXQTA8': 2500,   # $25 in cents
            'price_1Qf8O4E4vPtJSn6enzkQ08eC': 5000,   # $50 in cents
            'price_1Qf8QCE4vPtJSn6e6eTX5kTE': 10000,  # $100 in cents
        }
        amount = price_mapping.get(price_id)
        if not amount:
            return jsonify({'error': 'Invalid price ID'}), 400

        # Retrieve the customer from Stripe using email
        customers = stripe.Customer.list(email=email).data
        if not customers:
            return jsonify({'error': 'No customer found with this email'}), 404
        customer = customers[0]  # Assume the first customer is correct

        # Retrieve the customer's default payment method
        payment_methods = stripe.PaymentMethod.list(
            customer=customer.id,
            type="card",
        )

        if not payment_methods.data:
            return jsonify({'error': 'No saved payment method found for this customer'}), 404

        # Use the first payment method
        default_payment_method = payment_methods.data[0]
        
        # Create a payment intent
        payment_intent = stripe.PaymentIntent.create(
            amount=amount,
            currency='usd',
            customer=customer.id,
            payment_method=default_payment_method.id,
            confirm=True,
            description=f'Purchase of usage credits ({price_id})',
            automatic_payment_methods={
                "enabled": True,
                "allow_redirects": "never"
            },
        )

        # If payment succeeded, update the user's usage denominator
        users = load_users()
        user = next((u for u in users if u['email'] == email), None)
        if user:

            # Initialize usage_denominator if not present
            if 'usage_denominator' not in user:
                user['usage_denominator'] = 5.0

            # 1. Add the number purchased to usage_denominator
            if price_id == 'price_1Qf8LCE4vPtJSn6exafXQTA8':    # $25
                user['usage_denominator'] += 25
            elif price_id == 'price_1Qf8O4E4vPtJSn6enzkQ08eC':  # $50
                user['usage_denominator'] += 50
            elif price_id == 'price_1Qf8QCE4vPtJSn6e6eTX5kTE':  # $100
                user['usage_denominator'] += 100

            # 2. Subtract the current usage from the new denominator
            user['usage_denominator'] -= user['usage']

            # 3. Ensure usage remains unchanged
            user['usage'] = user.get('usage', 0.0)
            save_users(users)

        return jsonify({'clientSecret': payment_intent.client_secret})
    except stripe.error.CardError as e:
        return jsonify({'error': f"Card error: {e.user_message}"}), 400
    except Exception as e:
        return jsonify({'error': str(e)}), 400


# Fetches the next billing date the user can expect from STRIPE
@app.route('/api/next-billing-date', methods=['GET'])
def get_next_billing_date():
    email = request.args.get('email')
    if not email:
        return jsonify({'error': 'Email parameter is required'}), 400

    # 1. Fetch the Stripe customer by email
    customer = stripe.Customer.list(email=email).data
    if not customer:
        return jsonify({'error': 'Customer not found'}), 404
    customer_id = customer[0].id

    # 2. Check for an ACTIVE subscription first
    active_subscriptions = stripe.Subscription.list(
        customer=customer_id,
        status='active'
    ).data

    if active_subscriptions:
        # The user still has an active subscription (including "cancel_at_period_end" but not ended yet)
        subscription = active_subscriptions[0]
        next_billing_date = subscription.current_period_end

        # Convert timestamp to YYYY-MM-DD
        formatted_date = datetime.utcfromtimestamp(next_billing_date).strftime('%Y-%m-%d')
        logger.info(f"Next Billing Date (raw): {next_billing_date}")

        # Update AccessEndDate in users.json
        users = load_users()
        user = next((u for u in users if u['email'] == email), None)
        if user:
            user['AccessEndDate'] = formatted_date
            save_users(users)
            logger.info(f"Updated AccessEndDate for {email}: {formatted_date}")

        # Return the next billing date as you normally do
        return jsonify({'nextBillingDate': next_billing_date})

    # ----------------------------------------------------------------------
    # 3. If there's NO active subscription, check for a CANCELLED subscription
    canceled_subscriptions = stripe.Subscription.list(
        customer=customer_id,
        status='canceled',
        limit=1  # grab the most recent canceled subscription
    ).data

    if canceled_subscriptions:
        subscription = canceled_subscriptions[0]

        # If canceled_at_period_end was used, once it's truly ended,
        # ended_at will be set to the date/time it ended.
        ended_at = subscription.ended_at
        cancel_at_period_end = subscription.cancel_at_period_end
        current_period_end = subscription.current_period_end

        # Decide which date to show:
        # - If ended_at exists, use it (the subscription is truly ended).
        # - If ended_at is None but cancel_at_period_end is True, it normally means
        #   the subscription is still active until current_period_end. But if Stripe
        #   lists it here under "canceled," it's presumably ended, so ended_at should be set.
        if ended_at:
            # ended_at is a Unix timestamp for final cancellation
            date_to_use = ended_at
        else:
            # Fallback to current_period_end if ended_at is somehow missing
            date_to_use = current_period_end

        canceled_date_str = datetime.utcfromtimestamp(date_to_use).strftime('%Y-%m-%d')

        # Update AccessEndDate to reflect the final day they actually had/have access
        users = load_users()
        user = next((u for u in users if u['email'] == email), None)
        if user:
            user['AccessEndDate'] = canceled_date_str
            save_users(users)
            logger.info(f"Updated AccessEndDate for {email} to canceled date: {canceled_date_str}")

        return jsonify({
            'message': 'User subscription is canceled',
            'endedAt': date_to_use
        })

    # 4. If we have no active sub and no canceled sub in Stripe, return an error
    return jsonify({'error': 'No subscription found for this user.'}), 404
    
# Counts the Number of Sentences
@app.route('/number_of_sentences')

def index():
    logger.info("Route '/number_of_sentences' accessed.")  # Log that the route was accessed
    results = 2 + 3  # Compute 2 + 3
    logger.info(f"Result of 2 + 2: {results}")  # Log the result of the calculation
    return render_template('upload.html', results=results)  # Render the HTML template and pass `results`






# SHOWS THE LOGO

from flask import send_from_directory
@app.route('/logo.png')

def serve_logo():
    return send_from_directory('.', 'logo.png')  # Serve it from the current directory


# Configure upload folder and file size

UPLOAD_FOLDER = 'uploads/'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500MB


# Disclosure words dictionary

DISCLOSURE_WORDS = {}

# Then add this function to load all disclosures from the custom file
def load_all_disclosures():
    """Load all disclosures from the custom disclosures file."""
    global DISCLOSURE_WORDS
    try:
        if os.path.exists(CUSTOM_DISCLOSURES_FILE):
            with open(CUSTOM_DISCLOSURES_FILE, 'r') as f:
                DISCLOSURE_WORDS = json.load(f)
            logger.info(f"Loaded {len(DISCLOSURE_WORDS)} disclosures from {CUSTOM_DISCLOSURES_FILE}")
        else:
            logger.error(f"Custom disclosures file not found: {CUSTOM_DISCLOSURES_FILE}")
            # Initialize with empty dict if file doesn't exist
            DISCLOSURE_WORDS = {}
    except Exception as e:
        logger.error(f"Error loading disclosures: {e}")
        DISCLOSURE_WORDS = {}


# File path for storing custom disclosures persistently
CUSTOM_DISCLOSURES_FILE = os.path.join(app.config['UPLOAD_FOLDER'], 'custom_disclosures.json')

# Ensure the file exists
if not os.path.exists(CUSTOM_DISCLOSURES_FILE):
    with open(CUSTOM_DISCLOSURES_FILE, 'w') as f:
        json.dump({}, f)  # Start with an empty dictionary

@app.route('/add_custom_disclosure', methods=['POST'])
def add_custom_disclosure():
    try:
        data = request.get_json()
        logger.info("Received Data:", data)  # Debugging log
        word = data.get('word', '').strip()
        disclosure_text = data.get('disclosure', '').strip()
        if not word or not disclosure_text:
            return jsonify({"success": False, "error": "Both 'word' and 'disclosure' are required."}), 400
        with open(CUSTOM_DISCLOSURES_FILE, 'r') as f:
            custom_disclosures = json.load(f)
        if word in custom_disclosures:
            return jsonify({"success": False, "error": "Word already exists in custom disclosures."}), 400

        custom_disclosures[word] = disclosure_text
        with open(CUSTOM_DISCLOSURES_FILE, 'w') as f:
            json.dump(custom_disclosures, f, indent=4)

        DISCLOSURE_WORDS[word] = disclosure_text
        return jsonify({"success": True, "message": f"Custom disclosure for '{word}' added successfully."}), 200
    except Exception as e:
        logger.info(f"Error in add_custom_disclosure: {e}")  # Log the error
        return jsonify({"success": False, "error": str(e)}), 500








#Add logging to monitor requests to help debug duplicate requests
    
@app.before_request
def log_request_info():
    logger.info(f"Incoming Request: Path={request.path}, Method={request.method}")
    
#Upload file route
    
@app.route('/', methods=['GET', 'POST'])
def handle_upload():
    # Check if user is logged in
    if 'user_email' not in session:
        # User is not logged in, redirect to intro page
        return redirect(url_for('intro_page'))
    
    # Log the currently logged-in user
    logged_in_user = session.get('user_email')
    logger.info(f"Logged-in user: {session.get('user_email')}")
    
    if request.method == 'POST':
        # Handle file upload
        distribution_method = request.form.get('distribution_method')
        logger.info(f"Selected distribution method: {distribution_method}")

        uploaded_file = request.files.get('file')
        if not uploaded_file:
            logger.info("No file part in request")  # Debugging message
            return 'No file part', 400
        elif uploaded_file.filename == '':
            logger.info("No selected file")  # Debugging message
            return 'No selected file', 400
        else:
            filename = secure_filename(uploaded_file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            uploaded_file.save(file_path)
            logger.info(f"File saved at: {file_path}")  # Debugging message

            # Count sentences in the uploaded file
            if file_path.endswith('.pdf'):
                text_by_page = extract_text_from_pdf(file_path)
            elif file_path.endswith('.docx'):
                text_by_page = extract_text_from_docx(file_path)
            else:
                return "Unsupported file type", 400

            all_text = " ".join([page["text"] for page in text_by_page])
            sentence_count = len(split_into_sentences(all_text))
            update_user_usage(session.get('user_email'), sentence_count)
            logger.info(f"SENTENCE COUNT #: {sentence_count}")  # Debugging message

            # Redirect to process the file after saving
            return redirect(url_for('process_file', filename=filename))

    # Render the upload page and pass the logged-in user to the template
    return render_template('upload.html', user_email=logged_in_user)


# DOES NOT WORK YET
# Function to delete archived text in JSON file for archives

from datetime import datetime

@app.route('/delete_archived_text', methods=['POST'])
def delete_archived_text():
    try:
        data = request.get_json()
        date = data.get('date')  # Use the `date` field to identify the row
        logger.info(f"Date received for deletion: {date}")  # Debugging
        if not date:
            return jsonify({'error': 'Missing date field'}), 400

        # Load existing archives
        if os.path.exists(ARCHIVES_FILE):
            with open(ARCHIVES_FILE, 'r') as f:
                archives = json.load(f)
            logger.info(f"Archives before deletion: {archives}")  # Debugging

            # Normalize date formats for comparison
            try:
                received_date = datetime.strptime(date, "%m/%d/%Y").strftime('%Y-%m-%d')
            except ValueError as e:
                logger.info(f"Error parsing date: {e}")
                return jsonify({'error': 'Invalid date format received'}), 400

            # Filter out the entry to delete
            updated_archives = [
                entry for entry in archives
                if not entry.get('date', '').startswith(received_date)  # Check by matching the start of the date
            ]
            logger.info(f"Archives after deletion: {updated_archives}")  # Debugging

            # Save the updated archives
            with open(ARCHIVES_FILE, 'w') as f:
                json.dump(updated_archives, f, indent=4)
            return jsonify({'success': True, 'message': 'Row deleted successfully!'})
        else:
            return jsonify({'error': 'Archives file not found'}), 404
    except Exception as e:
        logger.info(f"Error in delete_archived_text: {e}")
        return jsonify({'error': str(e)}), 500





# Function to capitalize the first letter of each sentence for display purposes

def capitalize_sentences(text):

    # Capitalizes the first letter of each sentence
    sentence_endings = re.compile(r'([.!?]\s*)')
    sentences = sentence_endings.split(text)
    sentences = [s.capitalize() for s in sentences]
    return ''.join(sentences)


def process_page(page_content):
    page_num = page_content.get("page")
    text = page_content.get("text", "")
    
    # Split text into sentences
    sentences = split_into_sentences(text)
    
    # Log the number of sentences and sample sentences
    logger.info(f"Page {page_num}: Split into {len(sentences)} sentences.")
    for idx, sentence in enumerate(sentences, start=1):
        logger.info(f"Page {page_num} Sentence {idx}: {sentence}")
    
    # Perform compliance check on the entire text
    compliance_data = perform_compliance_check(text, page_num=page_num)
    
    flagged_instances = compliance_data.get("flagged_instances", [])
    
    return flagged_instances




@app.route('/process/<filename>', methods=['GET'])
def process_file(filename):
    
    # Generate a request ID
    request_id = register_request()
    logger.info(f"Starting request {request_id}: process_file for {filename}")
    
    global processed_files
    logger.info(f"Processing file: {filename}")
    logger.info(f"File size: {os.path.getsize(os.path.join(app.config['UPLOAD_FOLDER'], filename))} bytes")

    logger.info(f"[{datetime.utcnow()}] Incoming request: /process/{filename}")

    # Check if user is admin (default to False if not logged in)
    is_admin = False
    if 'user_email' in session:
        user_email = session['user_email']
        users = load_users()
        user = next((u for u in users if u['email'] == user_email), None)
        if user:
            is_admin = user.get('Administrator Access NEW') == 'Yes'

    disclosures = DISCLOSURE_WORDS

    # Initialize processing status
    final_check_status[filename] = False

    if filename in processed_files:
        logger.info(f"Returning cached results for {filename}.")
        final_check_status[filename] = True
        logger.info("CHECKED!")
        time.sleep(1)
        
        # Mark request as complete when returning cached results
        mark_request_complete(request_id)
        
        return render_template(
            'results.html',
            text=Markup(processed_files[filename]['text']),
            revised_text=processed_files[filename].get('revised_text', ''),
            disclosures=processed_files[filename]['disclosures'],
            sliced_disclosures=processed_files[filename].get('sliced_disclosures', []),
            finra_analysis=processed_files[filename]['finra_analysis'],
            results=processed_files[filename].get('results', 0),
            is_admin=is_admin,  # Pass is_admin to the template
            filename=filename,
            request_id=request_id  # Add request_id to template context
        )

    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    if not os.path.isfile(file_path):
        logger.info(f"File {filename} not found.")
        final_check_status[filename] = False
        
        # Mark request as complete on error
        mark_request_complete(request_id)
        
        return render_template('results.html', 
                              text="Error: File not found", 
                              revised_text="", 
                              disclosures=[], 
                              sliced_disclosures=[], 
                              finra_analysis=[], 
                              results=0, 
                              is_admin=is_admin,
                              request_id=request_id)

    try:
        # Extract text by page based on file type
        if file_path.endswith('.pdf'):
            text_by_page = extract_text_from_pdf(file_path)
        elif file_path.endswith('.docx'):
            text_by_page = extract_text_from_docx(file_path)
        elif file_path.endswith('.mp4'):
            # Handle video files
            logger.info(f"Transcribing MP4 file: {file_path}")
            text_by_page = transcribe_mp4(file_path)
        else:
            logger.info(f"Unsupported file type: {file_path}")
            final_check_status[filename] = False
            
            # Mark request as complete on error
            mark_request_complete(request_id)
            
            return render_template('results.html', 
                                  text="Unsupported file type", 
                                  revised_text="", 
                                  disclosures=[], 
                                  sliced_disclosures=[], 
                                  finra_analysis=[], 
                                  results=0, 
                                  is_admin=is_admin,
                                  request_id=request_id)

        # FILTER OUT TITLE CONTENT AND PREFIX
        filtered_text_by_page = []
        for page_data in text_by_page:
            page_text = page_data.get("text", "")
    
            # Remove entire TITLE section (both prefix and content)
            if "TITLE:" in page_text and "CONTENT:" in page_text:
                # Find where "CONTENT:" starts and only keep that part onwards
                content_start = page_text.find("CONTENT:")
                if content_start != -1:
                    page_text = page_text[content_start + 8:].strip()  # Remove "CONTENT:" and strip whitespace
            elif page_text.startswith("TITLE:"):
                # If it's only title without content section, remove everything until first newline
                lines = page_text.split('\n', 1)  # Split on first newline only
                if len(lines) > 1:
                    page_text = lines[1].strip()  # Take everything after first line
                else:
                    page_text = ""  # If only title line, make it empty
            
            # Also handle cases where content might have "CONTENT:" prefix without title
            if page_text.startswith("CONTENT:"):
                page_text = page_text[8:].strip()  # Remove "CONTENT:" prefix
    
            # Update the page data
            filtered_page_data = page_data.copy()
            filtered_page_data["text"] = page_text
            filtered_text_by_page.append(filtered_page_data)

        # Use the filtered text instead of the original
        text_by_page = filtered_text_by_page

        # Store the original text by page before processing
        original_text_by_page = text_by_page.copy()
        
        # Handle cross-page sentences
        processed_text_by_page = handle_cross_page_sentences(text_by_page)
        logger.info(f"Processed {len(text_by_page)} pages, handling cross-page sentences")
        
        # Join the processed text
        all_text = " ".join([page["text"] for page in processed_text_by_page])
        logger.info(f"Total text length: {len(all_text)} characters")
        logger.info(f"First 200 characters: {all_text[:200]}")
        
        # Clean the text - remove bullet points and other special characters
        cleaned_text = re.sub(r'[‚Ä¢\u2022]', '', all_text)  # Remove bullet points
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()  # Clean up whitespace
        
        sentences = split_into_sentences(cleaned_text)
        results = len(sentences)

        finra_analysis = []
        for page in processed_text_by_page:
            page_text = page["text"].replace('‚Ä¢', '').replace('\u2022', '')  # Remove bullet points
            page_num = page.get("page")
            
            # Pass the cleaned text string directly
            compliance_data = perform_compliance_check(page_text, page_num)

            if not compliance_data.get("compliant"):
                instances = compliance_data.get("flagged_instances", [])
                for instance in instances:
                    if isinstance(instance, dict) and instance.get("flagged_instance"):
                        # Only add non-empty, valid instances
                        if instance["flagged_instance"].strip() and \
                           instance["flagged_instance"] != "‚Ä¢" and \
                           len(instance["flagged_instance"]) > 1:
                            # Add highlighting to this instance
                            instance['highlighted_flagged_instance'] = extract_problematic_words_from_rationale(
                                instance['flagged_instance'], 
                                instance.get('rationale', '')
                            )
                            finra_analysis.append(instance)

        # Cache results - now includes both original and processed text by page
        processed_files[filename] = {
            'text': cleaned_text,
            'original_text_by_page': original_text_by_page,  # Store the original text by page
            'processed_text_by_page': processed_text_by_page,  # Store the processed text by page
            'revised_text': "",
            'disclosures': disclosures,
            'sliced_disclosures': list(disclosures.values())[:5],
            'finra_analysis': finra_analysis,
            'results': results
        }

        # Add this at the end, before returning the template
        log_session_token_summary()

        final_check_status[filename] = True
        logger.info("CHECKED!")
        time.sleep(1)

        # Mark the request as complete before returning the response
        mark_request_complete(request_id)

        # Add filename to localStorage for page-specific disclosures
        return render_template(
            'results.html',
            text=Markup(cleaned_text),
            revised_text="",
            disclosures=disclosures,
            sliced_disclosures=list(disclosures.values())[:5],
            finra_analysis=finra_analysis,
            results=results,
            filename=filename,  # Pass filename to template
            is_admin=is_admin,  # Pass is_admin to the template
            request_id=request_id  # Add request_id to template context
        )

    except Exception as e:
        logger.error(f"Error processing file: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        final_check_status[filename] = False

        # Queue pregeneration of alternatives for flagged instances
        if filename in processed_files and 'finra_analysis' in processed_files[filename] and processed_files[filename]['finra_analysis']:
            queue_alternatives_pregeneration(processed_files[filename]['finra_analysis'])

        
        # Mark the request as complete even on error
        mark_request_complete(request_id)
        
        return render_template('results.html', 
                              text="Error processing file", 
                              revised_text="", 
                              disclosures=[], 
                              sliced_disclosures=[], 
                              finra_analysis=[], 
                              results=0, 
                              is_admin=is_admin,
                              request_id=request_id)
    
# Function to remove repeated phrases more effectively.   
def remove_repeated_phrases_v2(text):
    words = text.split()
    cleaned_text = []
    buffer = []
    prev_chunk = []

    for word in words:
        buffer.append(word)
        if len(buffer) >= 5:  # Check for repeated sequences every 5 words
            current_chunk = ' '.join(buffer)
            if prev_chunk and current_chunk == prev_chunk:

                # If repeated, skip appending the duplicate chunk
                buffer = []
            else:

                # Append the chunk and reset the buffer
                cleaned_text.extend(buffer)
                prev_chunk = current_chunk
                buffer = []

    # Append any remaining words in the buffer
    if buffer:
        cleaned_text.extend(buffer)
    return ' '.join(cleaned_text)



# Load the spaCy model for English (you need to run 'python -m spacy download en_core_web_sm' once)
nlp = spacy.load("en_core_web_sm")

# Add the sentencizer to improve sentence splitting
if "sentencizer" not in nlp.pipe_names:
    nlp.add_pipe("sentencizer")
    

# Function to check contextual similarity using NLP
USE_SIMILARITY_CHECK = False  # Set to True to enable the similarity check

def is_contextually_similar(text, keyword):
    if not USE_SIMILARITY_CHECK:

        # Placeholder logic or always return False if similarity check is disabled
        return False

    # Use spacy to compare similarity between the document and the keyword
    doc = nlp(text)
    keyword_doc = nlp(keyword)
    similarity = doc.similarity(keyword_doc)

    # Define a similarity threshold; adjust as needed for sensitivity
    threshold = 0.7  # 70% similar
    return similarity >= threshold

# Function to check similarity ratio using SequenceMatcher
def is_similar_ratio(text, keyword):
    return SequenceMatcher(None, text, keyword).ratio() >= 0.7  # Adjust threshold if needed

def normalize_disclosure(text):
    return ' '.join(text.split()).lower()  # Normalize spacing and case




# Function to analyze text for trigger and disclosure words with contextual recognition
def analyze_text(text):
    """
    Analyze text for trigger and disclosure words with contextual recognition.
    Returns only disclosures that match keywords in the text.
    """
    logger.info(f"analyze_text called with: '{text[:50]}...'")
    
    # Load standard disclosures from disclosures.json
    standard_disclosures = {}
    DISCLOSURES_FILE = os.path.join(os.path.dirname(__file__), 'disclosures.json')
    
    try:
        if os.path.exists(DISCLOSURES_FILE):
            with open(DISCLOSURES_FILE, 'r') as f:
                disclosures_data = json.load(f)
                
                # Log the structure of the loaded data
                logger.info(f"Loaded disclosures data type: {type(disclosures_data)}")
                if isinstance(disclosures_data, list):
                    logger.info(f"List length: {len(disclosures_data)}")
                    if len(disclosures_data) > 0:
                        logger.info(f"First item type: {type(disclosures_data[0])}")
                
                # Handle different formats of disclosures.json
                if isinstance(disclosures_data, list) and len(disclosures_data) > 0:
                    standard_disclosures = disclosures_data[0] if isinstance(disclosures_data[0], dict) else {}
                else:
                    standard_disclosures = disclosures_data
                
                logger.info(f"Loaded {len(standard_disclosures)} standard disclosures")
    except Exception as e:
        logger.error(f"Error loading standard disclosures: {e}")
    
    # Load custom disclosures
    custom_disclosures = {}
    CUSTOM_DISCLOSURES_FILE = os.path.join(app.config['UPLOAD_FOLDER'], 'custom_disclosures.json')
    
    try:
        if os.path.exists(CUSTOM_DISCLOSURES_FILE):
            with open(CUSTOM_DISCLOSURES_FILE, 'r') as f:
                custom_disclosures = json.load(f)
                logger.info(f"Loaded {len(custom_disclosures)} custom disclosures")
    except Exception as e:
        logger.error(f"Error loading custom disclosures: {e}")
    
    # Merge disclosures, with custom taking precedence
    all_disclosures = {**standard_disclosures, **custom_disclosures}
    logger.info(f"Combined disclosure dictionary has {len(all_disclosures)} items")
    
    # Convert to lowercase for case-insensitive matching
    text_lower = text.lower()
    
    # Track matched disclosures and keywords
    matched_disclosures = []
    matched_keywords = []
    
    # Start with general disclosure
    general_disclosure = "Content in this material is for general information only and not intended to provide specific advice or recommendations for any individual."
    matched_disclosures.append(general_disclosure)
    logger.info(f"Added general disclosure by default")
    
    # Check each keyword for a match in the text
    for keyword, disclosure in all_disclosures.items():
        keyword_lower = keyword.lower()
        
        # If the keyword is found in the text
        if keyword_lower in text_lower:
            matched_keywords.append(keyword)
            
            # Add the disclosure if not already present
            if disclosure not in matched_disclosures:
                matched_disclosures.append(disclosure)
                logger.info(f"Matched keyword '{keyword}' to disclosure: {disclosure[:30]}...")
    
    # Handle special case for 'investing'
    if "invest" in text_lower or "investing" in text_lower:
        investing_disclosure = "Investing involves risk including loss of principal. No strategy assures success or protects against loss."
        if investing_disclosure not in matched_disclosures:
            matched_disclosures.append(investing_disclosure)
            logger.info(f"Added investing disclosure based on 'invest/investing' keyword")
    
    # Handle special case for 'mutual funds'
    if "mutual fund" in text_lower or "funds" in text_lower:
        mutual_fund_disclosure = all_disclosures.get("Mutual Fund", "")
        if mutual_fund_disclosure and mutual_fund_disclosure not in matched_disclosures:
            matched_disclosures.append(mutual_fund_disclosure)
            logger.info(f"Added mutual fund disclosure based on 'funds' keyword")
    
    logger.info(f"Found {len(matched_keywords)} matching keywords: {matched_keywords}")
    logger.info(f"Returning {len(matched_disclosures)} matched disclosures")
    
    return text, matched_disclosures

# AI-based function to detect references to economic forecasts
#def ai_detects_economic_forecast(text):
    # Prompt the AI to detect economic forecasts
    #prompt = f"Does the following content reference an economic forecast?\n\nContent: {text}"
    
    #try:
        #response = anthropic.messages.create(
            #model="claude-3-opus-20240229",
            #system=SYSTEM_MESSAGE,  # Using global system message
            #max_tokens=50,
            #messages=[
                #{
                    #"role": "user",
                    #"content": prompt
                #}
            #]
        #)
        #ai_response = response.content[0].text.strip().lower()
        #return 'yes' in ai_response or 'economic forecast' in ai_response or 'prediction' in ai_response
    #except Exception as e:
        #logger.error(f"Error detecting economic forecast: {e}")
        #return False

from flask import request, jsonify, send_file
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from io import BytesIO

@app.route('/convert_text_to_pdf', methods=['POST'])
def convert_text_to_pdf():
    try:
        text = request.json.get('text', '')
        
        # Create a PDF in memory
        buffer = BytesIO()
        p = canvas.Canvas(buffer, pagesize=letter)
        
        # Write the text to the PDF
        text_object = p.beginText(40, 750)
        text_object.setFont("Helvetica", 12)
        
        # Split text into lines and add to PDF
        for line in text.split('\n'):
            text_object.textLine(line)
        
        p.drawText(text_object)
        p.save()
        
        # Move to the beginning of the BytesIO buffer
        buffer.seek(0)
        
        # Return the PDF
        return send_file(
            buffer,
            as_attachment=True,
            download_name='pasted_text.pdf',
            mimetype='application/pdf'
        )
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# Route for turning text into pdf on upload page
#from fpdf import FPDF  # Add this import at the top of app.py if not already present
#from flask import jsonify, url_for

#@app.route("/convert_text_to_pdf", methods=["POST"])
#def convert_text_to_pdf():
    #data = request.get_json()
    #text_content = data.get("text_content", "")
    #if text_content:

        # Save PDF in the uploads folder in the Project directory
        #pdf_path = os.path.join(app.config['UPLOAD_FOLDER'], "converted_text.pdf")
        #pdf = FPDF()
        #pdf.add_page()
        #pdf.set_auto_page_break(auto=True, margin=15)
        #pdf.set_font("Arial", size=12)

        # Add the text content to the PDF file
        #pdf.multi_cell(0, 10, text_content)
        #pdf.output(pdf_path)

        # Return URL pointing to the newly created PDF
        #pdf_url = url_for('serve_file', filename="converted_text.pdf", _external=True)
        #return jsonify({"pdf_url": pdf_url})
    #return "Error converting text to PDF", 400


@app.route('/uploads/<filename>')
def serve_file(filename):
    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)

@app.route("/analyze_file_content", methods=["POST"])
def analyze_file_content():

    # Check if the request has a file part
    if 'file' not in request.files:
        return jsonify({"error": "No file part in the request"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No file selected"}), 400

    # Save file temporarily for processing, or use in-memory processing
    temp_path = os.path.join(app.config['UPLOAD_FOLDER'], "temp_analysis.pdf")
    file.save(temp_path)
    
    # Example: perform your analysis function on the file (process text, run compliance check, etc.)

    # This is where you call your actual file processing logic
    results = perform_file_analysis(temp_path)  # Replace with your actual analysis function

    # Clean up by removing the temp file, if saved
    os.remove(temp_path)

    # Return the analysis results as JSON
    return jsonify(results)









# Progress Loading Bar
@app.route('/progress')
def progress():

    def generate():
        for i in range(0, 101, 10):  # Simulating progress
            yield f"data: {i}\n\n"
            time.sleep(0.5)  # Simulated delay
    return Response(stream_with_context(generate()), mimetype='text/event-stream')








# Recycle Button Function
import json
import random
import re
from difflib import SequenceMatcher

# Load the compliance examples from your JSON file
def load_compliance_examples():
    try:
        with open('fcd.json', 'r') as f:
            data = json.load(f)
            
            # If data is already a list, use it directly
            if isinstance(data, list):
                examples = data
            # If data is a dictionary with numbered keys (like {"0": {...}, "1": {...}, ...})
            elif isinstance(data, dict):
                examples = list(data.values())
            else:
                logger.error(f"Unexpected JSON format in fcd.json")
                return []
                
            # Filter to ensure we only use entries with both non_compliant and compliant text
            valid_examples = [
                example for example in examples 
                if "non_compliant" in example and "compliant" in example
            ]
            
            logger.info(f"Loaded {len(valid_examples)} valid compliance examples from fcd.json")
            return valid_examples
            
    except Exception as e:
        logger.error(f"Error loading compliance examples: {e}")
        return []

    
# Function to calculate text similarity
def similarity_score(a, b):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

# Add this at the top of your file with other imports/globals
alternative_cache = {}  # Dict to store multiple alternatives for each flagged text
alternative_index = {}  # Track which alternative is currently being shown

@app.route('/generate_new_alternative', methods=['POST'])
def generate_new_alternative():
    data = request.get_json()
    flagged_text = data.get("flagged_instance", "").strip()
    
    if not flagged_text:
        return jsonify({"new_alternative": None, "feedback": "No flagged instance provided."}), 400

    try:
        # Create simplified version of text for better cache hits
        simplified_text = re.sub(r'\s+', ' ', flagged_text.lower()).strip()
        cache_key = hashlib.md5(f"alt_{simplified_text}".encode('utf-8')).hexdigest()
        
        # Debug log to show what we're searching for in the cache
        logger.info(f"Looking for alternatives for: {flagged_text}")
        logger.info(f"Simplified text: {simplified_text}")
        logger.info(f"Cache key: {cache_key}")
        
        # Check if we have pre-generated alternatives in cache
        with cache_lock:
            # Debug: log what's in the cache
            logger.info(f"Current cache contains keys: {list(alternative_cache.keys())}")
            
            if cache_key in alternative_cache and len(alternative_cache[cache_key]) > 0:
                # Log what alternatives we found
                logger.info(f"Found {len(alternative_cache[cache_key])} pre-generated alternatives for {cache_key}")
                
                # We have pre-generated alternatives - get the next one in sequence
                if cache_key not in alternative_index:
                    alternative_index[cache_key] = 0
                else:
                    # Move to next alternative
                    alternative_index[cache_key] = (alternative_index[cache_key] + 1) % len(alternative_cache[cache_key])
                
                current_index = alternative_index[cache_key]
                alternative = alternative_cache[cache_key][current_index]
                
                logger.info(f"Using pre-generated alternative #{current_index+1}/{len(alternative_cache[cache_key])} for {cache_key[:8]}")
                
                # Generate rationale based on the alternative content
                if "risk" in alternative.lower():
                    rationale = "Modified to acknowledge investment risks while maintaining intent."
                elif any(word in alternative.lower() for word in ["may", "might", "could"]):
                    rationale = "Added qualifying language to avoid guaranteed outcomes."
                elif any(word in alternative.lower() for word in ["aim", "strive", "seek"]):
                    rationale = "Replaced guarantee with aspiration language to comply with regulations."
                else:
                    rationale = "Modified to use more balanced language and avoid promissory statements."
                
                # Update user cost with minimal amount for using cached alternative
                if 'user_email' in session:
                    user_email = session['user_email']
                    update_user_cost(user_email, 0.005)  # Minimal cost for pre-generated alternative
                
                return jsonify({"new_alternative": alternative, "rationale": rationale})
        
        # If we get here, no cached alternatives were found - generate on demand
        logger.info(f"No pre-generated alternatives found for full sentence: {flagged_text}, generating on demand...")
        
        # Queue this text for future pregeneration after we generate it on-demand
        queue_alternatives_pregeneration([{"flagged_instance": flagged_text}])
        
        # Generate alternatives - optimized prompt for faster processing
        prompt = f"""
Generate 3 FINRA-compliant alternatives for this non-compliant financial text and write to sound more natural and human while being FINRA-compliant:
"{flagged_text}"

FOCUS MAINLY ON adjusting only the specific part that makes the instance non-compliant.

Make it:
- Conversational and natural
- Free of robotic/legal language
- Compliant but approachable
- Don't include disclaimers or risk warnings

Format as:
ALT1: [First alternative]
ALT2: [Second alternative]
ALT3: [Third alternative]

Only the alternatives, no other text.
"""
        # Make API call
        api_response = call_deepseek_api(prompt)
        logger.info(f"Received response from API")
        
        # Parse the structured response to extract alternatives
        alternatives = []
        
        # Look for ALT1, ALT2, ALT3 format with fast regex
        alt_pattern = re.compile(r'ALT\d+:\s*(.*?)(?=ALT\d+:|$)', re.DOTALL)
        matches = alt_pattern.findall(api_response)
        
        # Fast processing of matches
        alternatives = [match.strip() for match in matches if match.strip()]
        
        # If parsing failed, use simple newline splitting as fallback
        if not alternatives:
            lines = [line.strip() for line in api_response.strip().split('\n') 
                    if line.strip() and not line.strip().startswith(('#', 'Alternative'))]
            alternatives = [re.sub(r'^\d+\.\s*', '', line) for line in lines]  # Remove any numbering
        
        # Filter out any invalid alternatives and ensure we have at least one good one
        alternatives = [alt for alt in alternatives if len(alt) >= 10 and len(alt) <= len(flagged_text) * 2]
        
        # If we still don't have valid alternatives, create a default one
        if not alternatives:
            alternatives = ["We may help clients pursue their investment objectives while understanding that all investments involve risk."]
            logger.warning(f"Failed to parse alternatives from API response, using default")
        
        # Store the new alternatives in the cache for future use
        with cache_lock:
            alternative_cache[cache_key] = alternatives
            alternative_index[cache_key] = 0  # Reset index for new batch
            logger.info(f"Cached {len(alternatives)} alternatives with key {cache_key}")
        
        # Get the first alternative
        new_alternative = alternatives[0]
        
        # Generate rationale based on content
        if "risk" in new_alternative.lower():
            rationale = "Modified to acknowledge investment risks while maintaining intent."
        elif any(word in new_alternative.lower() for word in ["may", "might", "could"]):
            rationale = "Added qualifying language to avoid guaranteed outcomes."
        elif any(word in new_alternative.lower() for word in ["aim", "strive", "seek"]):
            rationale = "Replaced guarantee with aspiration language to comply with regulations."
        else:
            rationale = "Modified to use more balanced language and avoid promissory statements."
        
        # Update user usage for on-demand generation
        if 'user_email' in session:
            user_email = session['user_email']
            update_user_cost(user_email, 0.04)  # Standard cost for on-demand generation
            logger.info(f"Updated usage for {user_email} for on-demand alternative generation")
        
        return jsonify({
            "new_alternative": new_alternative,
            "rationale": rationale
        })
        
    except Exception as e:
        logger.error(f"Error generating alternatives: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Return a fallback alternative even on error
        return jsonify({
            "error": "An error occurred while generating alternatives. Please try again.",
            "new_alternative": "We may help clients pursue their investment objectives while understanding that all investments involve risk.",
            "rationale": "This is a default alternative due to an error in processing."
        }), 500
    
# Load prohibited words from JSON
def load_prohibited_words_custom():
    try:
        with open('prohibited_words.json', 'r') as file:
            return json.load(file)
    except Exception as e:
        logger.info(f"Error loading prohibited words: {e}")
        return []





def parse_compliance_response(response_text):
    # This variable tracks overall compliance status (top-level),
    # which you may or may not need depending on your use case.
    top_level_compliance_status = None
    
    message = ""
    flagged_instances = []
    current_instance = {}

    # Split the response into lines for processing
    lines = response_text.split('\n')
    
    for line in lines:
        line = line.strip()

        # Check if this is the top-level "Compliance Status: ..." line
        # (In some prompt formats, you might have a line like "Compliance Status: Compliant"
        #  at the very top. If you do not have a top-level compliance status, you can remove this.)
        if line.startswith("Compliance Status:"):
            top_level_compliance_status = line.split(":", 1)[1].strip().lower()

        elif line.startswith("Message:"):
            message = line.split(":", 1)[1].strip()

        elif line.startswith("Flagged Instances:"):
            # "Flagged Instances:" is just a header line, skip it
            continue

        elif re.match(r'^\d+\.\s+"(.+)"$', line):
            # We've reached a new flagged instance.
            # 1) If we have a previously built instance, append it (if it meets our filter).
            if current_instance:
                # Only append if "Mostly Compliant" or "Non-Compliant"
                status_val = current_instance.get("compliance_status", "").lower()
                if status_val in ["non-compliant"]:
                    flagged_instances.append(current_instance)
            
            # 2) Start building the new instance
            current_instance = {}
            match = re.match(r'^\d+\.\s+"(.+)"$', line)
            if match:
                current_instance["flagged_instance"] = match.group(1)

        elif line.startswith("- Compliance Status:"):
            # e.g. "- Compliance Status: Mostly Compliant."
            # remove trailing period if present and make it lowercase for consistency
            status_text = line.split(":", 1)[1].strip().rstrip(".").lower()
            current_instance["compliance_status"] = status_text

        elif line.startswith("- Specific Compliant Alternative:"):
            # e.g. "- Specific Compliant Alternative: "Invest in an IRA...""
            alt_text = line.split(":", 1)[1].strip().strip('"')
            current_instance["specific_compliant_alternative"] = alt_text

        elif line.startswith("- Rationale:"):
            # e.g. "- Rationale: "This is too promissory..."
            rationale_text = line.split(":", 1)[1].strip().strip('"')
            current_instance["rationale"] = rationale_text

        else:
            # Capture any unexpected lines for debugging
            if line and not line.startswith("---"):
                logger.warning(f"Unrecognized line format: {line}")

    # After the loop, we may have one last instance to append
    if current_instance:
        status_val = current_instance.get("compliance_status", "").lower()
        if status_val in ["non-compliant"]:
            flagged_instances.append(current_instance)

    # Decide how you want to handle overall compliance:
    # We can define top-level "compliant" as exactly "compliant",
    # or you might want to derive it from flagged instances instead.
    # Here we directly follow the old approach: 
    return {
        "compliant": (top_level_compliance_status == "compliant"),
        "message": message,
        "flagged_instances": flagged_instances
    }


@app.route('/check_custom_alternative', methods=['POST'])
def check_custom_alternative():
    # Generate a request ID
    request_id = register_request()
    logger.info(f"Starting request {request_id}: check_custom_alternative")
    
    data = request.get_json()
    custom_text = data.get("custom_text", "").strip()
    
    if not custom_text:
        logger.error("No text provided for compliance check.")
        mark_request_complete(request_id)  # Mark complete on error
        return jsonify({"error": "No text provided for compliance check.", "request_id": request_id}), 400
    
    try:

        # Check if this text was already flagged in the current session
        if 'processed_files' in globals():
            for filename, file_data in processed_files.items():
                if 'finra_analysis' in file_data:
                    for flagged_instance in file_data['finra_analysis']:
                        if flagged_instance.get('flagged_instance', '').strip() == custom_text:
                            # This text was already flagged - reformat for consistency
                            cached_instance = flagged_instance.copy()
                    
                            # Reformat the rationale to match the expected format
                            if "specific_compliant_alternative" in cached_instance:
                                alternative_text = cached_instance["specific_compliant_alternative"]
                                cached_instance["rationale"] = f"The statement \"{custom_text}\" contains language that may be considered absolute or promissory, presented without proper qualifiers. A more compliant version could be \"{alternative_text}\"."
                    
                            mark_request_complete(request_id)
                            return jsonify({
                                "compliant": False,
                                "message": f"Found 1 potential compliance issues.",
                                "flagged_instances": [cached_instance],
                                "request_id": request_id
                            }), 200
                        
        # Use the centralized perform_compliance_check function for the check
        compliance_data = perform_compliance_check(custom_text)
        
        # Format the response with consistent structure
        is_compliant = compliance_data.get("compliant", True)
        flagged_instances = compliance_data.get("flagged_instances", [])
        
        # Create message based on compliance status
        message = "No compliance issues found." if is_compliant else f"Found {len(flagged_instances)} potential compliance issues."
        
        # Define the violation types to match against
        violation_types = [
            "contains language that may be considered absolute or promissory, presented without proper qualifiers.",
            "contains language that may be considered inappropriate for audience.",
            "contains language that may be considered a prediction or projection of performance presented without proper qualifiers.",
            "contains language that may be considered unclear or does not provide a balanced treatment of risks and potential benefits."
        ]
        
        # If we have any flagged instances and need to categorize them
        if flagged_instances:
            # Build a single prompt to categorize all instances at once
            categorization_prompt = "Categorize each financial compliance issue into exactly one of these categories:\n"
            categorization_prompt += "1. Absolute/promissory statements\n"
            categorization_prompt += "2. Inappropriate language\n"
            categorization_prompt += "3. Performance projection\n"
            categorization_prompt += "4. Unbalanced risk/benefit treatment\n\n"
            
            for i, instance in enumerate(flagged_instances):
                if "flagged_instance" in instance:
                    categorization_prompt += f"Text {i+1}: \"{instance['flagged_instance']}\"\n"
            
            categorization_prompt += "\nRespond with ONLY the number for each text, in this format:\n"
            categorization_prompt += "Text 1: 2\nText 2: 1\nText 3: 4\n...etc."
            
            # Make a single API call to categorize all instances
            categorization_response = call_deepseek_api(categorization_prompt)
            logger.info(f"Categorization response: {categorization_response}")
            
            # Parse the response to extract categories
            categories = {}
            for line in categorization_response.strip().split('\n'):
                if line.startswith('Text '):
                    parts = line.split(':')
                    if len(parts) == 2:
                        try:
                            text_num = int(parts[0].replace('Text ', '').strip())
                            category_num = int(parts[1].strip())
                            if 1 <= category_num <= 4:  # Validate category number
                                categories[text_num] = category_num
                        except ValueError:
                            continue
            
            # Process the flagged instances with the categorized violation types
            for i, instance in enumerate(flagged_instances, 1):
                if "flagged_instance" in instance and "specific_compliant_alternative" in instance:
                    alternative_text = instance["specific_compliant_alternative"]
                    
                    # Get the category number (default to 1 if not found)
                    category_num = categories.get(i, 1)
                    
                    # Map category number to violation type
                    violation_type = violation_types[category_num-1]
                    
                    # Format the rationale with the appropriate violation type
                    instance["rationale"] = f"The statement \"{instance['flagged_instance']}\" {violation_type} A more compliant version could be \"{alternative_text}\"."
        
        response_data = {
            "compliant": is_compliant,
            "message": message,
            "flagged_instances": flagged_instances,
            "request_id": request_id
        }
        
        # Mark the request as complete
        mark_request_complete(request_id)
        
        return jsonify(response_data), 200
            
    except Exception as e:
        logger.error(f"Error during custom text compliance check: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Mark complete even on error
        mark_request_complete(request_id)
        
        return jsonify({
            "compliant": False,
            "error": "An error occurred during compliance checking.",
            "message": str(e),
            "flagged_instances": [],
            "request_id": request_id
        }), 500    
    
# Assessing Compliance Level
@app.route('/assess_compliance_level', methods=['POST'])
def assess_compliance_level():
    data = request.get_json()
    custom_text = data.get("custom_text")  # Get the custom text instead of feedback

    # Clear, concise prompt for AI to determine compliance level
    prompt = f"""

    Evaluate the following text for compliance with FINRA and SEC marketing rules:

    - Avoid implications of guarantees, guarantees of positive returns, or implications of risk-free investing.

    - Statements can be compliant even if they do not explicitly acknowledge risks, as long as they do not promise gains or make misleading claims.

    - If the text is compliant, respond with "compliant".

    - If the text is non-compliant, respond with "non-compliant".


    Text: "{custom_text}"

    Answer only with "compliant" or "non-compliant". Provide a single-sentence rationale.

    """
    compliance_level = call_deepseek_api(prompt).strip().lower()
    # Ensure we only get compliant/non-compliant
    if compliance_level.startswith("compliant") or compliance_level.startswith("non-compliant"):
        compliance_status = compliance_level.split()[0]  # Get just the first word
    else:
        compliance_status = "error"  # Or handle unexpected responses differently

    return jsonify({"compliance_level": compliance_status})




# WTF is this???

@app.route("/", methods=["POST"])

def process_form():
    
    # Simulate response for testing
    data = "Processing complete."
    response = Response(data, mimetype="text/plain")
    response.headers["Content-Length"] = str(len(data))
    return response




# GENERATING DISCLOSURES FUNCTION
@app.route("/generate_disclosures", methods=["POST"])
def generate_disclosures():
    data = request.get_json()
    revised_text = data.get("text", "")
    
    # Add detailed logging
    logger.info(f"generate_disclosures called with text: '{revised_text[:50]}...'")

    # Call your existing disclosure logic for the revised text
    analyzed_text, disclosures = analyze_text(revised_text)
    
    # Log what analyze_text returned
    logger.info(f"analyze_text returned {len(disclosures)} disclosures")
    for i, disc in enumerate(disclosures):
        logger.info(f"Disclosure {i+1}: {disc[:50]}...")

    # Load scenarios from scenarios.json
    with open("scenarios.json") as f:
        scenarios = json.load(f)

    # Determine the logged-in user
    logged_in_user = request.headers.get("Logged-In-User", "Unknown")
    logger.info(f"Logged-in user: {logged_in_user}")

    # Find the corresponding scenario for the user
    assigned_scenario = scenarios.get(logged_in_user, "No scenario assigned.")
    logger.info(f"Assigned scenario: {assigned_scenario[:50]}...")

    # Append specific key disclosures in a defined order
    investing_disclosure = "Investing involves risk including loss of principal. No strategy assures success or protects against loss."
    general_disclosure = "Content in this material is for general information only and not intended to provide specific advice or recommendations for any individual."
    
    # IMPORTANT: Only use the disclosures returned by analyze_text, don't add more
    # final_disclosures = [investing_disclosure, general_disclosure] + disclosures
    final_disclosures = disclosures  # Use ONLY what analyze_text returns
    final_disclosures = list(dict.fromkeys(final_disclosures))  # Remove duplicates if any
    
    logger.info(f"Returning {len(final_disclosures)} final disclosures")

    return jsonify({
        "investing_disclosure": investing_disclosure,
        "disclosures": final_disclosures,
        "assigned_scenario": assigned_scenario,
    })

# ARCHIVES PAGE FUNCTIONALITY

ARCHIVES_FILE = "archives.json"

# Route to save archived text
@app.route('/save_archived_text', methods=['POST'])
def save_archived_text():
    try:
        data = request.get_json()
        
        # Ensure required fields are present
        required_fields = ['text', 'user_email', 'link', 'disclosures', 'distribution_channel', 'distribution_method']
        for field in required_fields:
            if field not in data:
                return jsonify({'error': f'Missing field: {field}'}), 400

        # Add a timestamp for the date field
        data['date'] = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')

        # Load existing archives
        archives = []
        if os.path.exists(ARCHIVES_FILE):
            with open(ARCHIVES_FILE, 'r') as f:
                archives = json.load(f)

        # Add the new entry
        archives.append(data)

        # Save the updated archives
        with open(ARCHIVES_FILE, 'w') as f:
            json.dump(archives, f, indent=4)
        return jsonify({'success': True})
    except Exception as e:
        logger.info(f"Error in save_archived_text: {e}")
        return jsonify({'error': str(e)}), 500

    # Append the new entry with disclosures
    distribution_channels = data.get('distribution_channel', [])  # Correct key matching UPLOADS HTML
    logger.info("Received Distribution Channels:", distribution_channels)  # Debugging
    distribution_methods = data.get('distribution_method', [])  # Get selected distribution methods
    logger.info("Distribution Channels:", distribution_channels)  # Confirm this value
    logger.info("Distribution Methods:", distribution_methods)
    archives.append({

        "date": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
        "text": text,
        "link": file_link,
        "disclosures": disclosures,
        "distribution_channels": distribution_channels,  # Save distribution channels
        "distribution_methods": distribution_methods     # Save distribution methods
    })

    with open(ARCHIVES_FILE, 'w') as f:
        json.dump(archives, f, indent=4)
    return jsonify({"message": "Text archived successfully!"})


# Route to get archived texts

@app.route('/get_archived_texts', methods=['GET'])
def get_archived_texts():
    if os.path.exists(ARCHIVES_FILE):
        with open(ARCHIVES_FILE, 'r') as f:
            archives = json.load(f)
        logger.info("Archived Data (All):", archives)  # Debug
        if 'user_email' in session:
            user_email = session['user_email']
            logger.info(f"Logged-in User Email: {user_email}")  # Debug

            # Filter archives for the logged-in user
            filtered_archives = [entry for entry in archives if entry.get('user_email') == user_email]
            logger.info(f"Filtered Archives for {user_email}:", filtered_archives)  # Debug
            return jsonify(filtered_archives)
        else:

            # No user logged in: Return all for testing/admin purposes
            logger.info("No user logged in. Returning all archives.")  # Debug
            return jsonify(archives)
    else:
        logger.info("Archives file does not exist.")  # Debug
        return jsonify([])



@app.route('/archived', methods=['GET'])
def archived():
    return render_template('archived.html')










# Function to upload a file (to be archived or for processing... not sure?)

@app.route('/upload_file', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400

    # Save the file securely
    filename = secure_filename(file.filename)
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    file.save(file_path)
    logger.info(f"File uploaded and saved to: {file_path}")

    # Special handling for MP4 files
    if filename.lower().endswith('.mp4'):
        # Trigger the video processing endpoint
        return jsonify({
            'filename': filename,
            'is_mp4': True,
            'message': 'MP4 file uploaded, processing will begin'
        }), 200
    
    # Return the filename for progress tracking (for non-MP4 files)
    return jsonify({'filename': filename}), 200


# Second endpoint to get rationales based on check_id
@app.route('/get_rationales/<check_id>', methods=['POST'])
def get_rationales(check_id):
    data = request.get_json()
    custom_text = data.get("text", "").strip()
    
    # Rest of your existing check_quick_text logic to generate rationales
    # ...


# CHECKING QUICK TEXT

@app.route('/check_quick_text', methods=['POST'])
def check_quick_text():
    data = request.get_json()
    custom_text = data.get("text", "").strip()
    
    if not custom_text:
        logger.error("No text provided for quick compliance check.")
        return jsonify({"error": "No text provided."}), 400
    
    try:
        global BERT_MODEL, BERT_TOKENIZER
        if BERT_MODEL is None or BERT_TOKENIZER is None:
            if not initialize_bert():
                raise Exception("Failed to initialize BERT model")
        
        # Log the complete text analysis first (for debugging)
        logger.info(f"Analyzing complete text with BERT: {custom_text}")
        
        # First analyze the complete text with BERT (this is what you had before)
        inputs = BERT_TOKENIZER(custom_text, 
                          return_tensors="pt",
                          truncation=True,
                          max_length=512,
                          padding=True)
        
        # Log tokenization details
        logger.info(f"Tokenized full text: {len(inputs['input_ids'][0])} tokens")
    
        with torch.no_grad():
            outputs = BERT_MODEL(**inputs)
            probabilities = softmax(outputs.logits, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][prediction].item()
        
        logger.info(f"BERT prediction for complete text: {prediction} ('{'non-compliant' if prediction == 1 else 'compliant'}'), Confidence: {confidence:.2f}")
        
        # Now analyze sentence by sentence like your upload function
        # Split text into sentences for BERT analysis
        sentences = split_into_sentences(custom_text)
        flagged_instances = []
        bert_flagged_sentences = []
        
        logger.info(f"Split text into {len(sentences)} sentences for individual analysis")
        
        # First pass: Use BERT to identify potentially non-compliant sentences
        for i, sentence in enumerate(sentences):
            # Skip very short or empty sentences
            if not sentence.strip() or len(sentence.split()) < 3:
                continue
            
            # Log the sentence we're checking
            logger.info(f"Analyzing sentence #{i+1}: {sentence[:100]}{'...' if len(sentence) > 100 else ''}")
            
            # Prepare input for BERT - ensure proper tokenization
            encoded_input = BERT_TOKENIZER(
                sentence, 
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # Log token counts to verify proper tokenization
            token_count = len(encoded_input['input_ids'][0])
            logger.info(f"Sentence #{i+1} tokenized to {token_count} tokens")
            
            # Make prediction using BERT
            with torch.no_grad():
                outputs = BERT_MODEL(**encoded_input)
                probabilities = softmax(outputs.logits, dim=1)
                prediction = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][prediction].item()
            
            logger.info(f"BERT prediction for sentence #{i+1}: {prediction} ('{'non-compliant' if prediction == 1 else 'compliant'}'), Confidence: {confidence:.2f}")        

        # Now analyze sentence by sentence like your upload function
        # Split text into sentences for BERT analysis
        sentences = split_into_sentences(custom_text)
        flagged_instances = []
        bert_flagged_sentences = []
        
        logger.info(f"Split text into {len(sentences)} sentences for individual analysis")
        
        # First pass: Use BERT to identify potentially non-compliant sentences
        for i, sentence in enumerate(sentences):
            # Skip very short or empty sentences
            if not sentence.strip() or len(sentence.split()) < 3:
                continue
            
            # Prepare input for BERT
            inputs = BERT_TOKENIZER(sentence, 
                                  return_tensors="pt",
                                  truncation=True,
                                  max_length=512,
                                  padding=True)
        
            # Make prediction using BERT
            with torch.no_grad():
                outputs = BERT_MODEL(**inputs)
                probabilities = softmax(outputs.logits, dim=1)
                prediction = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][prediction].item()
            
            logger.info(f"BERT prediction for sentence #{i+1}: {prediction} ('{'non-compliant' if prediction == 1 else 'compliant'}'), Confidence: {confidence:.2f}")
            
            # If BERT predicts non-compliant (class 1)
            if prediction == 1 and len(sentence.strip()) > 5:
                words_in_sentence = len(sentence.split())
                
                if words_in_sentence > 2:
                    # Regular case: sentence is long enough to stand on its own
                    bert_flagged_sentences.append({
                        "sentence": sentence,
                        "confidence": confidence
                    })
                    logger.info(f"BERT flagged sentence (confidence: {confidence:.1%}): {sentence[:100]}...")
                else:
                    # Short sentence case: Check context by looking at neighboring sentences
                    logger.info(f"Found short non-compliant sentence: '{sentence}' - checking context...")
                    
                    # Find the current sentence index in the full sentences list
                    sentence_position = -1
                    for idx, s in enumerate(sentences):
                        if s.strip() == sentence.strip():
                            sentence_position = idx
                            break
                    
                    if sentence_position == -1:
                        logger.warning(f"Could not find short sentence '{sentence}' in the list of sentences")
                        continue
                    
                    # Get previous and next sentences if they exist
                    prev_sentence = sentences[sentence_position - 1] if sentence_position > 0 else ""
                    next_sentence = sentences[sentence_position + 1] if sentence_position < len(sentences) - 1 else ""
                    
                    # Create context by combining neighboring sentences
                    context = ""
                    if prev_sentence:
                        context += prev_sentence + " "
                    context += sentence
                    if next_sentence:
                        context += " " + next_sentence
                    
                    logger.info(f"Context for short sentence: '{context}'")
                    
                    # Send the context to Deepseek for evaluation
                    evaluation_prompt = f"""
Analyze this text which contains a short sentence that BERT flagged as potentially non-compliant:

Text with context: "{context}"
Short flagged sentence: "{sentence}"

First, determine if the short sentence is:
1. A standalone title or heading
2. Part of a Q&A or dialogue (like "Absolutely." as an answer to a question)
3. A fragment that should be interpreted with neighboring sentences

Then determine if the short sentence, when interpreted in context, contains any of these FINRA compliance issues:
- Guarantees of performance or returns
- Absolute statements without proper qualifiers
- Promissory language
- Exaggerated or misleading claims

Respond with ONLY:
- CONTEXT_TYPE: [Title/Heading, Answer/Response, Fragment]
- FLAG_DECISION: [Flag, Ignore]
- BEST_FLAGGED_TEXT: "[if FLAG_DECISION is Flag, provide the exact text that should be flagged - either just the short sentence or the context that gives it meaning]"

Example: 
CONTEXT_TYPE: Answer/Response
FLAG_DECISION: Flag
BEST_FLAGGED_TEXT: "Can we expect future technology to supplement trustees' discretionary decision-making processes? Absolutely."
"""
                    
                    context_evaluation = call_deepseek_api(evaluation_prompt)
                    logger.info(f"Deepseek context evaluation: {context_evaluation}")
                    
                    # Parse the evaluation response
                    context_type = "Unknown"
                    flag_decision = "Ignore"  # Default to ignore
                    best_flagged_text = sentence  # Default to just the short sentence
                    
                    for line in context_evaluation.strip().split('\n'):
                        if line.startswith("CONTEXT_TYPE:"):
                            context_type = line.replace("CONTEXT_TYPE:", "").strip()
                        elif line.startswith("FLAG_DECISION:"):
                            flag_decision = line.replace("FLAG_DECISION:", "").strip()
                        elif line.startswith("BEST_FLAGGED_TEXT:"):
                            best_flagged_text = line.replace("BEST_FLAGGED_TEXT:", "").strip().strip('"')
                    
                    # If Deepseek says to flag it, add to flagged instances with the appropriate text
                    if flag_decision.lower() == "flag":
                        bert_flagged_sentences.append({
                            "sentence": best_flagged_text,
                            "confidence": confidence,
                            "context_type": context_type  # Add this for reference
                        })
                        logger.info(f"BERT+Deepseek flagged contextual sentence (confidence: {confidence:.1%}): {best_flagged_text[:100]}...")
        
        # Second pass: For each BERT-flagged sentence, verify with Deepseek API
        for flagged in bert_flagged_sentences:
            sentence = flagged["sentence"]
            confidence = flagged["confidence"]
            
            # Then modify your verification prompt by adding this section:
            skepticism_words = load_skepticism_words()
            skepticism_instruction = f"""
            SPECIAL ATTENTION: Apply higher skepticism when analyzing text containing these words in financial/business contexts: {', '.join(skepticism_words)}

            These words often indicate promissory or absolute language when used in financial communications.
            """
                    
            # Use the original verification prompt for individual instance
            verification_prompt = f"""
            {skepticism_instruction}

Determine if this text violates FINRA's communication rules by being false, misleading, promissory, exaggerated, or contains profanity:

"{sentence}"

Answer with ONLY "YES" or "NO", followed by a brief explanation.
"YES" means it IS non-compliant or contains profanity.
"NO" means it IS compliant.

IMPORTANT DISTINCTION:
- Non-compliant: Statements that present specific financial benefits, tax advantages, or performance outcomes as definite facts, or possess profanity.
- Compliant: General statements, opinions or any indication of subjectivity, uncertainty, personal belief, interpretation.

CRITICAL: Statements presented as definitive facts without qualifying language are typically non-compliant when they involve:
- Tax benefits
- Investment outcomes
- Financial advantages
- Product features that don't universally apply

Examples of non-compliant statements:
- "A Traditional IRA is a place to put your money to save on taxes." (presents tax saving as definite)
- "Roth IRAs are a great vehicle for tax free investing" (presents absolute statement when additioal rules apply to receive benefits of investing in ROTH IRA)
- "IRAs are vehicles with tax advantages" (not necessarily true in all cases)
- "This fund outperforms the market" (absolute claim without qualification)
- "The strategy protects your assets during downturns" (unqualified protection claim)

Examples of compliant alternatives:
- "A Traditional IRA is a place to put your money to potentially save on taxes."
- "Roth IRAs may offer tax advantages for qualifying investors" or "Roth IRAs are a potential vehicle for tax advantaged investing"
- "IRAs are vehicles with potential tax advantages" (clarifies all advantages don't apply to everyone)
- "This fund is designed to seek competitive returns relative to its benchmark"
- "The strategy aims to help manage risk during market downturns"

CRITICAL DISTINCTION - OPINIONS vs. FACTUAL CLAIMS:
- Statements clearly identified as opinions (using phrases like "we believe," "we think," "in our opinion," "we don't think" or similar language) ARE COMPLIANT and should NOT be flagged.
- Focus on flagging statements presented as objective facts without proper qualification.

For example:
- COMPLIANT: "We don't think the market will decline" (clearly labeled as opinion)
- NON-COMPLIANT: "There are no signs of economic downturn" (absolute factual claim)
- NON-COMPLIANT: "This investment will provide consistent returns" (absolute promise)



CRITICAL DISTINCTION - PARTIAL NON-COMPLIANCE:
- When a statement contains BOTH opinion markers (like "we think" or "we believe") AND absolute factual claims, the statement is still non-compliant.
- The presence of opinion markers ONLY applies to the specific opinion being expressed, not to factual claims that follow.

For example:
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn." 
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)

IMPORTANT DISTINCTION - OPINION VS FACTUAL CLAIMS:
- Statements clearly labeled as opinions using phrases like "we believe," "we think," "in our opinion," "I don't think" ARE COMPLIANT when they qualify the entire claim.
- Statements that present market conditions, economic outlook, or investment outcomes as definitive facts without qualifying language are NON-COMPLIANT.

Example analysis:
- COMPLIANT: "I don't think the market will move lower from here."
  (This is entirely an opinion - "I don't think" qualifies the statement about market movement)
- COMPLIANT: "We believe this investment approach may offer potential benefits."
  (Opinion marker "we believe" plus appropriate qualifiers "may" and "potential")
- NON-COMPLIANT: "We think this fund will outperform because the economy is strong."
  (While "we think" qualifies the opinion about outperformance, "the economy is strong" is presented as a separate factual claim)
- NON-COMPLIANT: "I believe our strategy will work because inflation has peaked."
  (The opinion about the strategy is qualified, but "inflation has peaked" is stated as fact)

CRITICAL JUDGEMENT POINT:
When analyzing statements with both opinions and factual elements, determine if the factual elements are:
1. Part of the opinion being expressed (compliant)
2. Separate factual claims presented as objective truth (non-compliant)

For example:
- COMPLIANT: "We don't think the market will decline." 
  (The phrase "we don't think" is an opinion marker)
- NON-COMPLIANT: "We don't think the market will decline, as there are no signs of economic downturn."
  (The phrase "there are no signs of economic downturn" is an absolute factual claim even though "we don't think" is an opinion marker)
- COMPLIANT: "We don't think the market will decline, as we currently see few signs that might indicate a potential economic downturn."
  (Both parts are properly qualified)
  
- COMPLIANT: "We believe this fund will outperform." 
  ("We believe" qualifies the opinion about outperformance)
- NON-COMPLIANT: "We believe this fund will outperform because the economy is healthy." 
  (While "we believe" qualifies the opinion about outperformance, "the economy is healthy" is presented as fact)

When text contains both opinions and factual claims, be sure to correctly discern which part of the sentence is Non-compliant by using the instructions given in this section.

--
CRITICAL OPINION STRUCTURE CLARIFICATION: 
1. Opinion qualifiers (like "I don't think", "We believe", etc.) apply ONLY to the specific clause or statement they directly modify.
2. Each separate claim in a message must be evaluated independently.

Examples of properly qualified opinions:
- "I don't think the market will move lower from here." - COMPLIANT (the entire statement is qualified as an opinion)
- "We believe this strategy may work well." - COMPLIANT (qualified opinion with additional "may" modifier)

Examples with mixed content:
- "I don't think the market will decline, but GDP is going to be positive." - PARTIALLY NON-COMPLIANT 
  (The first part "I don't think the market will decline" is a properly qualified opinion, but "GDP is going to be positive" is an unqualified factual claim)
- "We believe inflation will ease because the Fed has already won." - PARTIALLY NON-COMPLIANT
  (The opinion about inflation is qualified, but "the Fed has already won" is presented as fact)

When analyzing a statement with "I don't think X, Y", only X is qualified as an opinion. Y needs its own qualification if it's a separate claim.

COMPLIANCE CRITICAL LANGUAGE RULES:
- "Perfect" is a prohibited term when discussing investments or recommendations, as it implies guaranteed outcomes or absolute suitability
- Language suggesting personalized recommendations like "I found the perfect stock for you" is non-compliant


--

Please do NOT refer to things as being "compliant" instead use language like "more compliant," "better positioned," "more professional," etc. since this is not an official determination.


Always answer "YES" (non-compliant) for statements that:
1. Present possible, implied or conditional benefits as definite outcomes
2. Make absolute claims about tax advantages
3. Lack qualifiers like "may," "potential," "designed to," or "aims to" when discussing benefits
4. State as fact something that doesn't apply in all circumstances
5. Use terms like "perfect," "ideal," or similar absolute language when describing investment recommendations

All financial benefits and advantages MUST be qualified with appropriate language.

After giving YES/NO answer, aviod directly referencing FINRA or stating that something violates rules.
Example: YES - The statement "I guarantee to make you money" is non-compliant because it makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, violating FINRA rules against guarantees and unwarranted statements.
Better wording: YES - The statement "I guarantee to make you money" makes an absolute, promissory claim about investment outcomes without any qualifiers or risk disclosures, potentially violating rules against guarantees and unwarranted statements.
"""
            
            verification_response = call_deepseek_api(verification_prompt)
            logger.info(f"Deepseek verification response for '{sentence[:50]}...': {verification_response}")
            
            # Check if Deepseek confirms this is non-compliant
            is_non_compliant = False
            if verification_response:
                first_word = verification_response.strip().split()[0].upper() if verification_response.strip() else ""
                is_non_compliant = first_word == "YES"
            
            if is_non_compliant:
                # Categorize the violation type
                categorization_prompt = f"""
                Categorize this non-compliant financial statement into exactly ONE category:
                "{sentence}"

                Categories:
                0. Absolute/promissory statements (contains guarantees, promises, or absolute language)
                1. Inappropriate language (contains unprofessional or unsuitable content)
                2. Performance projection (contains unsupported predictions about performance)
                3. Unbalanced risk/benefit treatment (fails to present balanced view of risks and benefits)

                Respond with ONLY the category number (0, 1, 2, or 3).
                """

                category_response = call_deepseek_api(categorization_prompt).strip()
                logger.info(f"Violation category response: {category_response}")

                # Parse the category number (default to 0 if parsing fails)
                try:
                    category_num = int(re.search(r'\d', category_response).group())
                    if category_num < 0 or category_num > 3:
                        category_num = 0
                except (ValueError, AttributeError):
                    category_num = 0

                # Map category to violation description (now properly aligned)
                violation_types = [
                    "contains language that may be considered absolute or promissory, presented without proper qualifiers.",
                    "contains language that may be considered inappropriate for audience.", 
                    "contains language that may be considered a prediction or projection of performance presented without proper qualifiers.",
                    "contains language that may be considered unclear or does not provide a balanced treatment of risks and potential benefits."
                ]
                violation_type = violation_types[category_num]
                logger.info(f"Selected violation type: {category_num} - {violation_type}")
                    
                # If confirmed non-compliant, get a compliant alternative
                alternative_prompt = f"""
            This text violates FINRA rules: "{sentence}"

            Rewrite it to be compliant by:
            1. Removing absolute statements
            2. Adding appropriate qualifiers
            3. Avoiding guarantees or promises
            4. Maintaining the original meaning

            Respond with ONLY the compliant alternative text and nothing else.
            """
                
                compliant_text = call_deepseek_api(alternative_prompt).strip()
                
                # Clean quotes if present
                if compliant_text.startswith('"') and compliant_text.endswith('"'):
                    compliant_text = compliant_text[1:-1]

                # If we got a bad alternative, use the custom error message
                if not compliant_text or len(compliant_text) < 5:
                    compliant_text = "The server is unable to evaluate your request at this time due to unauthorized language usage or poor word selection. Please modify your submission and try again."
                
                # Process the rationale to remove "Yes -" or "No -" prefix
                cleaned_rationale = verification_response
                if verification_response:
                    # Check for common patterns like "YES -", "Yes -", etc.
                    patterns = ["YES -", "Yes -", "YES- ", "Yes- "]
                    for pattern in patterns:
                        if verification_response.strip().startswith(pattern):
                            # Remove the pattern and strip leading whitespace
                            cleaned_rationale = verification_response.strip()[len(pattern):].strip()
                            break
                
                # Format the rationale with the violation type
                formatted_rationale = f"The statement \"{sentence}\" {violation_type}"
                logger.info(f"Formatted rationale: {formatted_rationale}")
                
                # Add to flagged instances
                instance = {
                    "flagged_instance": sentence,
                    "compliance_status": "non-compliant",
                    "specific_compliant_alternative": compliant_text,
                    "rationale": formatted_rationale,
                    "confidence": f"{confidence:.1%}"
                }
                
                flagged_instances.append(instance)
                logger.info(f"Added flagged instance: '{sentence[:50]}...'")
        
        # Determine overall compliance based on flagged instances
        is_compliant = len(flagged_instances) == 0
        
        # Create a specific message based on the flagged instances
        message = ""
        if is_compliant:
            message = "No compliance issues found. This text appears to be compliant with FINRA regulations."
        else:
            message = f"Found {len(flagged_instances)} potential compliance issues. Please review the suggested alternatives."
        
        # Log token usage summary
        log_session_token_summary()
        
        return jsonify({
            "compliant": is_compliant,
            "message": message,
            "flagged_instances": flagged_instances
        }), 200
        
    except Exception as e:
        logger.error(f"Error during quick text compliance check: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({
            "compliant": False,
            "error": "An error occurred during compliance checking.",
            "message": "The server is unable to evaluate your request at this time due to unauthorized language usage or poor word selection. Please modify your submission and try again."
        }), 500
    
# LOADING BAR FUNCTIONALITY AND RULES

# Dictionary to track final check status per file
final_check_status = {}

@app.route('/progress/<filename>', methods=['GET'])
def progress_stream(filename):
    def generate_progress():
        try:
            logger.info(f"Progress endpoint called with filename: {filename}")
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            if not os.path.isfile(file_path):
                yield f"data: Error: File not found\n\n"
                return

            # For MP4 files, handle transcription with progress updates
            if filename.endswith('.mp4'):
                # Start transcription in a separate thread to allow progress updates during processing
                import threading
                transcription_complete = threading.Event()
                transcription_result = [None]  # Use a list to store the result
                
                def do_transcription():
                    try:
                        result = transcribe_mp4(file_path)
                        transcription_result[0] = result
                        transcription_complete.set()
                    except Exception as e:
                        logger.error(f"Error in transcription thread: {e}")
                        transcription_complete.set()  # Signal completion even on error
                
                # Start transcription in background
                threading.Thread(target=do_transcription).start()
                
                # Show incremental progress during transcription (1% every 3 seconds up to 20%)
                current_progress = 0
                last_keepalive = time.time()
                
                while current_progress < 20 and not transcription_complete.is_set():
                    # Increment progress
                    current_progress += 1
                    yield f"data: {current_progress:.2f}\n\n"
                    
                    # Send keepalive messages every 15 seconds to prevent connection timeouts
                    for _ in range(3):  # Split wait time into smaller chunks
                        if transcription_complete.is_set():
                            break
                        
                        # Wait for a shorter period (1 second)
                        transcription_complete.wait(1)
                        
                        # Send keepalive message if needed
                        now = time.time()
                        if now - last_keepalive > 15:
                            yield f"data: {current_progress:.2f}\n\n"  # Send same progress as keepalive
                            last_keepalive = now
                
                # If we exit the loop before reaching 20%, jump to 20%
                if current_progress < 20:
                    current_progress = 20
                    yield f"data: {current_progress:.2f}\n\n"
                
                # Wait for transcription to complete if it hasn't already
                timeout_counter = 0
                max_timeout = 300  # 5 minutes max wait
                
                while not transcription_complete.is_set() and timeout_counter < max_timeout:
                    # Wait a bit but keep connection alive
                    time.sleep(2)
                    timeout_counter += 2
                    
                    # Send keepalive periodically
                    now = time.time()
                    if now - last_keepalive > 15:
                        yield f"data: {current_progress:.2f}\n\n"  # Send same progress as keepalive
                        last_keepalive = now
                
                # If we timed out waiting for transcription
                if timeout_counter >= max_timeout and not transcription_complete.is_set():
                    yield f"data: Error: Transcription is taking too long, please check server logs\n\n"
                    return
                
                # Get the transcription result
                text_by_page = transcription_result[0]
                if text_by_page is None:
                    yield f"data: Error: Transcription failed\n\n"
                    return
            else:
                # For other file types, proceed as normal
                if filename.endswith('.pdf'):
                    text_by_page = extract_text_from_pdf(file_path)
                elif filename.endswith('.docx'):
                    text_by_page = extract_text_from_docx(file_path)
                else:
                    yield f"data: Error: Unsupported file type\n\n"
                    return
                
                # Start with 0% for non-MP4 files
                current_progress = 0.0
                
            # The rest of the function continues as before
            all_text = " ".join([page["text"] for page in text_by_page])
            sentences = split_into_sentences(all_text)
            flagged_instances = []  # Define flagged instances if necessary
            total_sentences = len(sentences)

            if total_sentences == 0:
                yield f"data: Error: No sentences to process\n\n"
                return

            # Simulate progress from current_progress to 60%
            remaining_progress = 60 - current_progress
            increment = remaining_progress / total_sentences if total_sentences > 0 else 0
            last_keepalive = time.time()
            
            for i, sentence in enumerate(sentences, start=1):
                # Use a smaller sleep time for faster response
                time.sleep(1.85)  # Simulate processing time
                
                progress = current_progress + (i * increment)
                yield f"data: {progress:.2f}\n\n"
                
                # Send keepalive if needed
                now = time.time()
                if now - last_keepalive > 15:
                    yield f"data: {progress:.2f}\n\n"  # Resend as keepalive
                    last_keepalive = now

                # Simulate flagged instance detection
                if i <= len(flagged_instances):
                    yield f"data: Flagged Instance: {flagged_instances[i - 1]}\n\n"

                # Check if the final check is completed during sentence processing
                if final_check_status.get(filename, False):
                    yield "data: 100.00\n\n"
                    logger.info(f"Final check completed for {filename}. Progress set to 100% during sentence processing.")
                    break  # Stop further sentence processing as the final check is done

            # Wait for the processing to complete in /process/<filename>
            progress = 60  # Start at 60%
            last_keepalive = time.time()
            timeout_counter = 0
            max_wait_time = 300  # 5 minutes max wait
            
            while not final_check_status.get(filename, False) and timeout_counter < max_wait_time:
                progress += 2  # Increment progress by 2% each iteration
                if progress > 90:  # Cap progress at 90%
                    progress = 90
                
                logger.info(f"Waiting for final check completion for {filename}... Progress: {progress}%")
                yield f"data: {progress}\n\n"  # Send progress to the frontend
                
                # Wait but send keepalive messages during long waits
                for _ in range(5):  # Check every 0.5 seconds for 2.5 seconds total
                    time.sleep(0.5)
                    timeout_counter += 0.5
                    
                    # Send keepalive if we're still processing
                    now = time.time()
                    if now - last_keepalive > 15 and not final_check_status.get(filename, False):
                        yield f"data: {progress}\n\n"
                        last_keepalive = now
                        
                    if final_check_status.get(filename, False) or timeout_counter >= max_wait_time:
                        break
            
            # If we timed out waiting for completion
            if timeout_counter >= max_wait_time and not final_check_status.get(filename, False):
                yield f"data: Error: Processing is taking too long, please check server logs\n\n"
                return

            # Emit "Processing Complete!" once CHECKED! is logged
            logger.info(f"Final check completed for {filename}. Sending completion signal.")
            yield "data: 100.00\n\n"
            yield "data: Processing Complete!\n\n"

        except Exception as e:
            logger.error(f"Error in progress_stream: {e}")
            yield f"data: Error: {str(e)}\n\n"

    return Response(generate_progress(), content_type='text/event-stream')






# Terminal Logs
@app.route('/terminal_logs', methods=['GET'])
def terminal_logs():

    def generate_logs():
        try:
            with open("uploads/app.log", "r") as log_file:
                log_file.seek(0, os.SEEK_END)  # Start at the end of the file
                while True:
                    line = log_file.readline()
                    if line:
                        yield f"data: {line.strip()}\n\n"
                    time.sleep(0.1)
        except Exception as e:
            yield f"data: Error: {str(e)}\n\n"
    return Response(stream_with_context(generate_logs()), mimetype='text/event-stream')


def call_deepseek_api(prompt):
    """Call Deepseek API with accurate token-based cost tracking using actual Deepseek pricing"""
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": SYSTEM_MESSAGE},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 1000
        }
        
        # Add timeout and retry logic
        max_retries = 3
        timeout_seconds = 30
        
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                logger.info(f"Sending batch of 1 instances to Deepseek API (attempt {attempt + 1}/{max_retries})")
                
                response = requests.post(
                    DEEPSEEK_API_URL, 
                    headers=headers, 
                    json=payload,
                    timeout=timeout_seconds
                )
                
                response_time = time.time() - start_time
                response.raise_for_status()
                result = response.json()
                
                # Extract ACTUAL token usage from Deepseek API response
                usage = result.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
                
                # Determine current time and pricing tier
                from datetime import datetime, timezone
                current_utc = datetime.now(timezone.utc)
                current_hour = current_utc.hour + current_utc.minute / 60.0
                
                # Check if we're in discount period (16:30-00:30 UTC)
                is_discount_period = (current_hour >= 16.5) or (current_hour <= 0.5)
                
                if is_discount_period:
                    input_cost_per_token = 0.000000135  # $0.135 per 1M tokens
                    output_cost_per_token = 0.00000055   # $0.550 per 1M tokens
                    pricing_tier = "Discount (50% off)"
                else:
                    input_cost_per_token = 0.00000027    # $0.27 per 1M tokens
                    output_cost_per_token = 0.0000011    # $1.10 per 1M tokens
                    pricing_tier = "Standard"
                
                # Calculate actual cost
                prompt_cost = prompt_tokens * input_cost_per_token
                completion_cost = completion_tokens * output_cost_per_token
                total_cost = prompt_cost + completion_cost
                
                logger.info(f"Deepseek API call - Tokens: prompt={prompt_tokens}, completion={completion_tokens}, total={total_tokens}")
                logger.info(f"Response time: {response_time:.2f} seconds")
                logger.info(f"Estimated cost: ${total_cost:.6f} (Prompt: ${prompt_cost:.6f}, Completion: ${completion_cost:.6f})")
                
                # Update session tracking with actual data
                global session_token_usage
                if 'session_token_usage' not in globals():
                    session_token_usage = {
                        'prompt_tokens': 0,
                        'completion_tokens': 0,
                        'total_tokens': 0,
                        'total_cost': 0.0,
                        'api_calls': 0,
                        'standard_pricing_calls': 0,
                        'discount_pricing_calls': 0,
                        'start_time': time.time()
                    }
                
                session_token_usage['prompt_tokens'] += prompt_tokens
                session_token_usage['completion_tokens'] += completion_tokens
                session_token_usage['total_tokens'] += total_tokens
                session_token_usage['total_cost'] += total_cost
                session_token_usage['api_calls'] += 1
                
                # Track pricing tier usage
                if is_discount_period:
                    session_token_usage['discount_pricing_calls'] += 1
                else:
                    session_token_usage['standard_pricing_calls'] += 1
                
                # FIXED: Only update user cost if we have Flask context (not in background threads)
                try:
                    from flask import session, has_request_context
                    if has_request_context() and 'user_email' in session:
                        user_email = session['user_email']
                        update_user_cost(user_email, total_cost)
                except RuntimeError:
                    # We're in a background thread, skip user cost tracking
                    logger.debug("Skipping user cost update - running in background thread")
                
                # Log summary every 5 calls
                if session_token_usage['api_calls'] % 5 == 0:
                    logger.info(f"===== SESSION USAGE SUMMARY =====")
                    logger.info(f"Total API calls: {session_token_usage['api_calls']}")
                    logger.info(f"  - Standard pricing calls: {session_token_usage['standard_pricing_calls']}")
                    logger.info(f"  - Discount pricing calls: {session_token_usage['discount_pricing_calls']}")
                    logger.info(f"Total tokens: {session_token_usage['total_tokens']} (Input: {session_token_usage['prompt_tokens']}, Output: {session_token_usage['completion_tokens']})")
                    logger.info(f"Total cost: ${session_token_usage['total_cost']:.8f}")
                    logger.info(f"Average cost per call: ${session_token_usage['total_cost']/session_token_usage['api_calls']:.8f}")
                    logger.info(f"================================")
                
                return result.get('choices', [{}])[0].get('message', {}).get('content', '')
                
            except requests.exceptions.Timeout:
                logger.warning(f"Deepseek API timeout on attempt {attempt + 1}/{max_retries} (>{timeout_seconds}s)")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    logger.info(f"Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"All {max_retries} attempts failed due to timeout")
                    return "Error: API request timed out after multiple attempts."
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"Deepseek API request error on attempt {attempt + 1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    logger.info(f"Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"All {max_retries} attempts failed due to request errors")
                    return f"Error: API request failed after multiple attempts: {str(e)}"
        
        return "Error: Unexpected failure in API retry logic."
    
    except Exception as e:
        logger.error(f"Error calling Deepseek API: {e}")
        return None
    

# Add a new function to update user cost
def update_user_cost(user_email, cost_amount):
    """
    Update the user's accumulated API cost
    
    Args:
        user_email (str): The user's email
        cost_amount (float): The amount to add to the user's cost
    """
    try:
        # Load users from the JSON file
        users = load_users()
        user = next((u for u in users if u['email'] == user_email), None)
        
        if user:
            # Initialize User Cost field if it doesn't exist
            if 'User Cost' not in user:
                user['User Cost'] = 0.0
            
            # Add the new cost
            user['User Cost'] += cost_amount
            
            # Round to 6 decimal places for consistent display
            user['User Cost'] = round(user['User Cost'], 6)
            
            # Save the updated users back to the file
            save_users(users)
            
            logger.info(f"Updated cost for user {user_email}: +${cost_amount:.6f}, Total: ${user['User Cost']:.6f}")
        else:
            logger.warning(f"User with email {user_email} not found when updating cost")
    
    except Exception as e:
        logger.error(f"Error updating user cost: {e}")

# Update the log_session_token_summary function to include user-specific cost summary
def log_session_token_summary():
    """Log a complete summary of token usage and costs for the entire session."""
    if 'session_token_usage' in globals():
        # Calculate elapsed time
        elapsed_time = time.time() - session_token_usage.get('start_time', time.time())
        
        logger.info(f"\n")
        logger.info(f"============== COMPLETE SESSION USAGE SUMMARY ==============")
        logger.info(f"Total Deepseek API calls: {session_token_usage['api_calls']}")
        logger.info(f"  - Standard pricing calls: {session_token_usage.get('standard_pricing_calls', 0)}")
        logger.info(f"  - Discount pricing calls: {session_token_usage.get('discount_pricing_calls', 0)}")
        logger.info(f"Total tokens: {session_token_usage['total_tokens']}")
        logger.info(f"  - Input tokens: {session_token_usage['prompt_tokens']}")
        logger.info(f"  - Output tokens: {session_token_usage['completion_tokens']}")
        logger.info(f"Total accurate cost: ${session_token_usage['total_cost']:.8f}")
        logger.info(f"Average cost per call: ${session_token_usage['total_cost']/max(1, session_token_usage['api_calls']):.8f}")
        logger.info(f"Average tokens per call: {session_token_usage['total_tokens']/max(1, session_token_usage['api_calls']):.1f}")
        logger.info(f"Session duration: {elapsed_time:.2f} seconds")
        
        # Add user-specific cost summary if a user is logged in
        if 'user_email' in session:
            user_email = session['user_email']
            users = load_users()
            user = next((u for u in users if u['email'] == user_email), None)
            
            if user and 'User Cost' in user:
                logger.info(f"Current user: {user_email}")
                logger.info(f"Total accumulated cost for this user: ${user['User Cost']:.8f}")
        
        logger.info(f"============================================================\n")
        

def close_db_pool():
    """Close all connections in the pool when the app stops"""
    if connection_pool:
        connection_pool.closeall()
        logger.info("Database connection pool closed")

import atexit
atexit.register(close_db_pool)

@app.route('/bulk-upload', methods=['POST'])
def bulk_upload():
    try:
        # Check authentication
        if 'username' not in session:
            return jsonify({'success': False, 'error': 'Login required'})
        
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'No file uploaded'})
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'No file selected'})
        
        # Read Excel file
        import pandas as pd
        from io import BytesIO
        import sqlite3
        import json
        
        # Read the file into a pandas DataFrame
        df = pd.read_excel(BytesIO(file.read()))

        # DEBUG: Print what pandas actually read
        print("DEBUG: Excel column headers pandas found:")
        for i, col in enumerate(df.columns):
            print(f"  [{i}] '{col}'")

        print("DEBUG: All rows data:")
        for idx, row in df.iterrows():
            print(f"Row {idx + 2}:")  # +2 because row 1 is headers
            for col in df.columns:
                val = row[col]
                print(f"  {col}: '{val}' (type: {type(val)})")
            print("---")

        # Clean up the dataframe - replace NaN with empty strings
        df = df.fillna('')
        
        # Process each row
        processed_count = 0
        current_user = session['username']
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        for _, row in df.iterrows():
            # Helper function to clean values
            def clean_value(val):
                if pd.isna(val) or str(val).strip().lower() in ['nan', 'none', '', 'na']:
                    return ''
                return str(val).strip()
            
            # Extract and clean data from row
            advisor_name = clean_value(row.get('Advisor Name', ''))
            advisor_firm_type = clean_value(row.get('Advisor or Firm', ''))
            firm_name_column = clean_value(row.get('Firm Name', ''))
            
            # Skip entire row if advisor/firm type is empty
            if not advisor_firm_type:
                continue

            # For firms, advisor_name can be empty (we'll use firm name instead)
            # For advisors, advisor_name must have a value
            if advisor_firm_type.lower() == 'advisor' and not advisor_name:
                print(f"Skipping advisor row with no advisor name")
                continue

            # Determine if it's an advisor or firm
            is_firm = advisor_firm_type.lower() == 'firm'

            # Set up name and firm fields properly
            if is_firm:
                # It's a firm - use firm name for both fields
                final_name = firm_name_column
                final_firm = firm_name_column
            else:
                # It's an advisor - use advisor name and separate firm name
                final_name = advisor_name
                final_firm = firm_name_column or 'Unknown Firm'

            # Extract social media URLs
            website = clean_value(row.get('Website', ''))
            facebook_url = clean_value(row.get('Facebook URL', ''))
            linkedin_url = clean_value(row.get('Linkedin URL', ''))
            twitter_url = clean_value(row.get('Twitter/X URL', ''))
            instagram_url = clean_value(row.get('Instagram URL', ''))
            youtube_url = clean_value(row.get('Youtube URL', ''))

            print(f"DEBUG URLS for {final_name}:")
            print(f"  Website: '{website}'")
            print(f"  Facebook: '{facebook_url}'")
            print(f"  LinkedIn: '{linkedin_url}'")
            print(f"  Twitter: '{twitter_url}'")
            print(f"  Instagram: '{instagram_url}'")
            print(f"  YouTube: '{youtube_url}'")
            print(f"Processing: {final_name} (Type: {advisor_firm_type}, Firm: {final_firm})")

            # Create advisor entry in SQLite
            cursor.execute('''
                INSERT INTO advisors (name, firm, website_url, added_by_username)
                VALUES (%s, %s, %s, %s)
                RETURNING id
            ''', (final_name, final_firm, website, current_user))
            advisor_id = cursor.fetchone()[0]

            
            # Store social media profiles in a simple way for SQLite
            social_profiles = []
            
            if website:
                social_profiles.append({
                    'platform': 'website',
                    'url': website,
                    'name': 'Website'
                })
            
            if facebook_url:
                social_profiles.append({
                    'platform': 'facebook',
                    'url': facebook_url,
                    'name': 'Facebook'
                })
            
            if linkedin_url:
                social_profiles.append({
                    'platform': 'linkedin',
                    'url': linkedin_url,
                    'name': 'LinkedIn'
                })
            
            if twitter_url:
                social_profiles.append({
                    'platform': 'twitter',
                    'url': twitter_url,
                    'name': 'X/Twitter'
                })
            
            if instagram_url:
                social_profiles.append({
                    'platform': 'instagram',
                    'url': instagram_url,
                    'name': 'Instagram'
                })
            
            if youtube_url:
                social_profiles.append({
                    'platform': 'youtube',
                    'url': youtube_url,
                    'name': 'YouTube'
                })
            
            # Store the profiles data 
            if social_profiles:
                try:
                    # Try to update with profiles_data column
                    cursor.execute('''
                        INSERT INTO advisor_profiles (advisor_id, profiles_data)
                        VALUES (%s, %s)
                        ON CONFLICT (advisor_id)
                        DO UPDATE SET profiles_data = EXCLUDED.profiles_data
                    ''', (advisor_id, json.dumps(social_profiles)))

                except sqlite3.OperationalError as e:
                    if "no such column" in str(e):
                        # Add the column if it doesn't exist
                        cursor.execute('ALTER TABLE advisors ADD COLUMN profiles_data TEXT')
                        cursor.execute('''
                            UPDATE advisors SET profiles_data = ? WHERE id = ?
                        ''', (json.dumps(social_profiles), advisor_id))
                    else:
                        raise e
                    
            processed_count += 1
            print(f"Successfully processed: {final_name} with {len(social_profiles)} profiles")
        
        conn.commit()
        cursor.close()
        release_db_connection(conn)
        
        return jsonify({
            'success': True,
            'count': processed_count,
            'message': f'Successfully processed {processed_count} entries'
        })
        
    except Exception as e:
        print(f"Bulk upload error: {e}")
        return jsonify({'success': False, 'error': str(e)})
    
           
@app.route('/api/get-advisor-profiles/<int:advisor_id>')
def get_advisor_profiles(advisor_id):
    try:
        # Check authentication
        if 'username' not in session:
            return jsonify({'success': False, 'error': 'Login required'})
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Join advisors and advisor_profiles so we can return everything
        cursor.execute('''
            SELECT a.name, a.firm, ap.profiles_data
            FROM advisors a
            LEFT JOIN advisor_profiles ap ON a.id = ap.advisor_id
            WHERE a.id = %s AND a.added_by_username = %s
        ''', (advisor_id, session['username']))
        
        result = cursor.fetchone()
        cursor.close()
        release_db_connection(conn)
        
        if not result:
            return jsonify({'success': False, 'error': 'Advisor not found'})
        
        advisor_name, firm, profiles_data = result
        
        profiles = []
        if profiles_data:
            try:
                profiles = profiles_data if isinstance(profiles_data, list) else json.loads(profiles_data)
            except Exception:
                profiles = []
        
        return jsonify({
            'success': True,
            'advisor_name': advisor_name,
            'firm': firm,
            'profiles': profiles
        })
        
    except Exception as e:
        print(f"Error getting advisor profiles: {e}")
        return jsonify({'success': False, 'error': str(e)})

    
@app.route('/create-tables')
def create_tables():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # First, create the advisors table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS advisors (
                id SERIAL PRIMARY KEY,
                name VARCHAR(255) NOT NULL,
                firm VARCHAR(255),
                website_url TEXT,
                user_id INTEGER,
                crd_number VARCHAR(50),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Then create the advisor_profiles table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS advisor_profiles (
                id SERIAL PRIMARY KEY,
                advisor_id INTEGER NOT NULL,
                profiles_data JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                CONSTRAINT unique_advisor UNIQUE (advisor_id),
                CONSTRAINT fk_advisor FOREIGN KEY (advisor_id) REFERENCES advisors(id) ON DELETE CASCADE
            )
        """)
        
        conn.commit()
        cursor.close()
        release_db_connection(conn)
        
        return "Both tables created successfully!"
    except Exception as e:
        return f"Error creating tables: {e}"
    


@app.route('/debug_schema')
def debug_schema():
    """Check the schema of our working PostgreSQL database"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        result = ""

        # Check advisors table
        result += "üóÇÔ∏è advisors table:\n"
        cursor.execute("""
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = 'advisors'
            ORDER BY ordinal_position
        """)
        for col in cursor.fetchall():
            result += f"   üìÑ {col[0]} ({col[1]})\n"

        # Check advisor_profiles table
        result += "\nüóÇÔ∏è advisor_profiles table:\n"
        cursor.execute("""
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = 'advisor_profiles'
            ORDER BY ordinal_position
        """)
        for col in cursor.fetchall():
            result += f"   üìÑ {col[0]} ({col[1]})\n"

        # Optionally check sample data
        result += "\nüìã Sample advisors data:\n"
        cursor.execute("SELECT * FROM advisors LIMIT 3;")
        for i, sample in enumerate(cursor.fetchall()):
            result += f"   Row {i+1}: {str(sample)[:200]}...\n"

        cursor.close()
        release_db_connection(conn)

        return f"<pre>{result}</pre>"

    except Exception as e:
        return f"<pre>Error: {e}</pre>"

if __name__ == "__main__":
    import os
    import threading
    
    # Use PORT environment variable (required by Render)
    port = int(os.environ.get("PORT", 8000))
    debug_mode = os.environ.get("APP_ENV", "prod") != "prod"  # Default to production
    
    print("Starting application initialization...")
    
    try:
        # Load all disclosures from the custom file
        print("Loading disclosures...")
        load_all_disclosures()

        # Initialize skepticism words
        print("Initializing skepticism words...")
        initialize_skepticism_words()
        
        print("About to initialize BERT...")
        # Initialize BERT model
        if not initialize_bert():
            print("ERROR: Failed to initialize BERT model")
        else:
            print("BERT model initialized successfully")
        
        print("About to initialize advisor database...")
        # Initialize advisor monitoring database
        init_advisor_db()
        print("Advisor monitoring database initialized")

        # Carousel Monitoring
        print("Initializing carousel monitoring...")
        init_carousel_monitoring()
        print("Carousel monitoring database initialized")

        # Update database schema for existing tables
        print("Updating advisor database schema...")
        update_advisor_db_schema()
        
        print("About to start alternative pregeneration thread...")
        # Start the alternative pregeneration thread
        start_alternative_pregeneration_thread()
        print("Started alternative pregeneration thread")
        
        print("About to start scheduler thread...")
        # Start the scheduler in a background thread using the simple schedule library
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        print("Scheduler thread started - advisors will be monitored every 30 minutes")

        print("About to update compliance schema...")
        # Update compliance schema
        update_compliance_schema()
        
        print("About to update scan schema...")
        # Update scan schema
        update_scan_schema()
        
        print(f"About to start Flask app on port {port}...")
        # Use the PORT environment variable and disable debug in production
        app.run(debug=debug_mode, use_reloader=False, port=port, host='0.0.0.0')
        print("Flask app ended")
        
    except Exception as e:
        print(f"ERROR during application startup: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
